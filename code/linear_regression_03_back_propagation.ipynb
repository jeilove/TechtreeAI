{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 제일 간단한 수준의 linear regression의 코드를 리뷰해보자\n",
    "    2. back_propagation을 직접 해보자\n",
    "        tf의 optimizer를 사용하지 않고 gradient를 직접 구해서 weight/bias 값을 설정한다.\n",
    "    1. Tensorflow의 기본 뼈대 기능을 넣어보자\n",
    "        tensorboard\n",
    "        학습이 오래 걸리는 경우를 위해 저장/불러오기 (later)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = [1., 2., 3., 4., 6., 7., 8., 9., 10.]\n",
    "y_groundtruth = [1., 2., 3., 4., 6., 7., 8., 9., 10.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = tf.Variable([.3], dtype=tf.float32, name=\"W\")\n",
    "b = tf.Variable([-.3], dtype=tf.float32, name=\"b\")\n",
    "x = tf.placeholder(tf.float32, name=\"x\")\n",
    "y_hat = x * W + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(sess.run(y_hat, {x: x_train}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = tf.placeholder(tf.float32, name=\"y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(sess.run(loss, {x: x_train, y: y_groundtruth}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss\n",
    "loss = tf.reduce_sum(tf.square(y_hat - y)) # sum of the squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer\n",
    "#optimizer = tf.train.GradientDescentOptimizer(0.001)\n",
    "#train = optimizer.minimize(loss)\n",
    "\n",
    "# 자체 옵티마이저\n",
    "# Minimize: Gradient Descent using derivative: W -= learning_rate * derivative\n",
    "learning_rate = 0.001\n",
    "# W\n",
    "gradient_W = tf.reduce_sum((W * x + b - y) * x)\n",
    "descent_W = W - learning_rate * gradient_W\n",
    "update_W = W.assign(descent_W)\n",
    "# b\n",
    "gradient_b = tf.reduce_sum((W * x + b - y))\n",
    "descent_b = b - learning_rate * gradient_b\n",
    "update_b = b.assign(descent_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init) # reset values to incorrect defaults."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_hist = tf.summary.histogram(\"Weight\", W)\n",
    "b_hist = tf.summary.histogram(\"bias\", b)\n",
    "\n",
    "y_hat_hist = tf.summary.histogram(\"hypothesis\", y_hat)\n",
    "\n",
    "loss_scal = tf.summary.scalar(\"loss\", loss)\n",
    "\n",
    "# tensorboard --logdir=./logs/linear_regression_logs\n",
    "merged_summary = tf.summary.merge_all()\n",
    "writer = tf.summary.FileWriter(\"./logs/linear_regression_r0_03\")\n",
    "writer.add_graph(sess.graph)  # Show the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step:  0 loss:  198.21 W:  [ 0.56700003] b :  [-0.26230001]\n",
      "step:  100 loss:  0.0540211 W:  [ 1.02259552] b :  [-0.16177712]\n",
      "step:  200 loss:  0.0360777 W:  [ 1.01846552] b :  [-0.13220735]\n",
      "step:  300 loss:  0.0240944 W:  [ 1.01509047] b :  [-0.10804246]\n",
      "step:  400 loss:  0.0160915 W:  [ 1.0123322] b :  [-0.08829434]\n",
      "step:  500 loss:  0.0107466 W:  [ 1.01007807] b :  [-0.0721558]\n",
      "step:  600 loss:  0.00717706 W:  [ 1.00823605] b :  [-0.05896723]\n",
      "step:  700 loss:  0.00479325 W:  [ 1.00673068] b :  [-0.04818918]\n",
      "step:  800 loss:  0.00320112 W:  [ 1.00550044] b :  [-0.03938122]\n",
      "step:  900 loss:  0.00213789 W:  [ 1.00449514] b :  [-0.03218319]\n",
      "step:  1000 loss:  0.00142782 W:  [ 1.00367343] b :  [-0.02630085]\n",
      "step:  1100 loss:  0.000953561 W:  [ 1.00300205] b :  [-0.02149365]\n",
      "step:  1200 loss:  0.00063682 W:  [ 1.00245333] b :  [-0.0175651]\n",
      "step:  1300 loss:  0.00042532 W:  [ 1.00200498] b :  [-0.01435464]\n",
      "step:  1400 loss:  0.000284048 W:  [ 1.00163853] b :  [-0.01173104]\n",
      "step:  1500 loss:  0.000189715 W:  [ 1.00133896] b :  [-0.00958695]\n",
      "step:  1600 loss:  0.000126705 W:  [ 1.00109422] b :  [-0.00783473]\n",
      "step:  1700 loss:  8.46228e-05 W:  [ 1.00089431] b :  [-0.0064028]\n",
      "step:  1800 loss:  5.65182e-05 W:  [ 1.00073087] b :  [-0.00523262]\n",
      "step:  1900 loss:  3.77449e-05 W:  [ 1.00059736] b :  [-0.00427632]\n",
      "step:  2000 loss:  2.52121e-05 W:  [ 1.00048816] b :  [-0.00349483]\n",
      "step:  2100 loss:  1.68365e-05 W:  [ 1.00039887] b :  [-0.00285613]\n",
      "step:  2200 loss:  1.12463e-05 W:  [ 1.00032604] b :  [-0.0023341]\n",
      "step:  2300 loss:  7.51206e-06 W:  [ 1.00026643] b :  [-0.00190753]\n",
      "step:  2400 loss:  5.01731e-06 W:  [ 1.00021768] b :  [-0.00155899]\n",
      "step:  2500 loss:  3.35066e-06 W:  [ 1.00017798] b :  [-0.00127417]\n",
      "step:  2600 loss:  2.23814e-06 W:  [ 1.00014544] b :  [-0.00104117]\n",
      "step:  2700 loss:  1.49562e-06 W:  [ 1.00011897] b :  [-0.00085112]\n",
      "step:  2800 loss:  9.9789e-07 W:  [ 1.00009716] b :  [-0.00069543]\n",
      "step:  2900 loss:  6.67003e-07 W:  [ 1.00007939] b :  [-0.00056833]\n",
      "step:  3000 loss:  4.45707e-07 W:  [ 1.00006497] b :  [-0.00046472]\n",
      "step:  3100 loss:  2.97886e-07 W:  [ 1.00005293] b :  [-0.00037986]\n",
      "step:  3200 loss:  1.9866e-07 W:  [ 1.00004327] b :  [-0.00031016]\n",
      "step:  3300 loss:  1.3239e-07 W:  [ 1.00003541] b :  [-0.00025331]\n",
      "step:  3400 loss:  8.85923e-08 W:  [ 1.00002885] b :  [-0.00020694]\n",
      "step:  3500 loss:  5.90816e-08 W:  [ 1.0000236] b :  [-0.00016917]\n",
      "step:  3600 loss:  3.96028e-08 W:  [ 1.00001931] b :  [-0.00013836]\n",
      "step:  3700 loss:  2.64519e-08 W:  [ 1.00001585] b :  [-0.00011325]\n",
      "step:  3800 loss:  1.7823e-08 W:  [ 1.00001299] b :  [ -9.27924411e-05]\n",
      "step:  3900 loss:  1.19318e-08 W:  [ 1.00001073] b :  [ -7.61195406e-05]\n",
      "step:  4000 loss:  8.0088e-09 W:  [ 1.00000882] b :  [ -6.24912718e-05]\n",
      "step:  4100 loss:  5.44578e-09 W:  [ 1.00000727] b :  [ -5.14220992e-05]\n",
      "step:  4200 loss:  3.69332e-09 W:  [ 1.00000608] b :  [ -4.23952079e-05]\n",
      "step:  4300 loss:  2.50168e-09 W:  [ 1.00000501] b :  [ -3.50033952e-05]\n",
      "step:  4400 loss:  1.77335e-09 W:  [ 1.00000417] b :  [ -2.89953605e-05]\n",
      "step:  4500 loss:  1.2103e-09 W:  [ 1.00000346] b :  [ -2.41220187e-05]\n",
      "step:  4600 loss:  8.43926e-10 W:  [ 1.00000286] b :  [ -2.01279763e-05]\n",
      "step:  4700 loss:  5.86525e-10 W:  [ 1.0000025] b :  [ -1.68888801e-05]\n",
      "step:  4800 loss:  4.22663e-10 W:  [ 1.00000215] b :  [ -1.43386978e-05]\n",
      "step:  4900 loss:  3.08674e-10 W:  [ 1.00000179] b :  [ -1.21495441e-05]\n",
      "W: [ 1.00000155] b: [ -1.03355987e-05] loss: 2.18211e-10\n"
     ]
    }
   ],
   "source": [
    "for step in range(5000):\n",
    "    summary, _, _, W_value, b_value, loss_value = sess.run([merged_summary, update_W, update_b, W, b, loss], {x: x_train, y: y_groundtruth})\n",
    "    \n",
    "    if step % 100 == 0:\n",
    "        print(\"step: \", step, \"loss: \", loss_value, \"W: \", W_value, \"b : \", b_value)\n",
    "    \n",
    "    writer.add_summary(summary, global_step=step)\n",
    "\n",
    "# evaluate training accuracy\n",
    "curr_W, curr_b, curr_loss = sess.run([W, b, loss], {x: x_train, y: y_groundtruth})\n",
    "print(\"W: %s b: %s loss: %s\"%(curr_W, curr_b, curr_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: 5.0, y: [ 4.99999714]\n"
     ]
    }
   ],
   "source": [
    "x_test = 5.\n",
    "y_predict = sess.run(y_hat, {x: x_test})\n",
    "print(\"x: %s, y: %s\"%(x_test, y_predict)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.4.0'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
