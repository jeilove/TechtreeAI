{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 제일 간단한 수준의 linear regression의 코드를 리뷰해보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = [1., 2., 3., 4., 6., 7., 8., 9., 10.]\n",
    "y_groundtruth = [1., 2., 3., 4., 6., 7., 8., 9., 10.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = tf.Variable([100], dtype=tf.float32)\n",
    "b = tf.Variable([-1000], dtype=tf.float32)\n",
    "x = tf.placeholder(tf.float32)\n",
    "# Hypothesis / y hat\n",
    "y_hat = W * x + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(sess.run(y_hat, {x: x_train}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = tf.placeholder(tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(sess.run(loss, {x: x_train, y: y_groundtruth}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cost/loss function\n",
    "loss = tf.reduce_sum(tf.square(y_hat - y)) # sum of the squares\n",
    "# optimizer\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.001)\n",
    "train = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init) # reset values to incorrect defaults."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step:  0 loss:  2.62836e+06 Weight:  [ 128.72000122] bias:  [-991.90002441]\n",
      "step:  1 loss:  2.05871e+06 Weight:  [ 135.95159912] bias:  [-986.81781006]\n",
      "step:  2 loss:  2.00332e+06 Weight:  [ 137.4682312] bias:  [-982.55023193]\n",
      "step:  3 loss:  1.98444e+06 Weight:  [ 137.46612549] bias:  [-978.51116943]\n",
      "step:  4 loss:  1.96828e+06 Weight:  [ 137.06163025] bias:  [-974.54455566]\n",
      "step:  5 loss:  1.95242e+06 Weight:  [ 136.55171204] bias:  [-970.60888672]\n",
      "step:  6 loss:  1.9367e+06 Weight:  [ 136.0153656] bias:  [-966.69311523]\n",
      "step:  7 loss:  1.92111e+06 Weight:  [ 135.47361755] bias:  [-962.79418945]\n",
      "step:  8 loss:  1.90565e+06 Weight:  [ 134.93203735] bias:  [-958.91125488]\n",
      "step:  9 loss:  1.89031e+06 Weight:  [ 134.39208984] bias:  [-955.04406738]\n",
      "step:  10 loss:  1.87509e+06 Weight:  [ 133.85418701] bias:  [-951.19250488]\n",
      "step:  11 loss:  1.86e+06 Weight:  [ 133.31842041] bias:  [-947.35644531]\n",
      "step:  12 loss:  1.84503e+06 Weight:  [ 132.7848053] bias:  [-943.53588867]\n",
      "step:  13 loss:  1.83018e+06 Weight:  [ 132.25334167] bias:  [-939.73071289]\n",
      "step:  14 loss:  1.81544e+06 Weight:  [ 131.72401428] bias:  [-935.94091797]\n",
      "step:  15 loss:  1.80083e+06 Weight:  [ 131.19682312] bias:  [-932.16638184]\n",
      "step:  16 loss:  1.78633e+06 Weight:  [ 130.67175293] bias:  [-928.40704346]\n",
      "step:  17 loss:  1.77195e+06 Weight:  [ 130.14878845] bias:  [-924.66290283]\n",
      "step:  18 loss:  1.75769e+06 Weight:  [ 129.62794495] bias:  [-920.93383789]\n",
      "step:  19 loss:  1.74354e+06 Weight:  [ 129.10920715] bias:  [-917.21984863]\n",
      "step:  20 loss:  1.72951e+06 Weight:  [ 128.59255981] bias:  [-913.52081299]\n",
      "step:  21 loss:  1.71559e+06 Weight:  [ 128.07800293] bias:  [-909.83666992]\n",
      "step:  22 loss:  1.70178e+06 Weight:  [ 127.56550598] bias:  [-906.16741943]\n",
      "step:  23 loss:  1.68808e+06 Weight:  [ 127.05508423] bias:  [-902.51293945]\n",
      "step:  24 loss:  1.67449e+06 Weight:  [ 126.54671478] bias:  [-898.87322998]\n",
      "step:  25 loss:  1.66101e+06 Weight:  [ 126.04040527] bias:  [-895.24816895]\n",
      "step:  26 loss:  1.64764e+06 Weight:  [ 125.53613281] bias:  [-891.63775635]\n",
      "step:  27 loss:  1.63438e+06 Weight:  [ 125.03388977] bias:  [-888.04187012]\n",
      "step:  28 loss:  1.62122e+06 Weight:  [ 124.53367615] bias:  [-884.46051025]\n",
      "step:  29 loss:  1.60817e+06 Weight:  [ 124.03547668] bias:  [-880.89361572]\n",
      "step:  30 loss:  1.59523e+06 Weight:  [ 123.53929138] bias:  [-877.34106445]\n",
      "step:  31 loss:  1.58239e+06 Weight:  [ 123.04510498] bias:  [-873.80285645]\n",
      "step:  32 loss:  1.56965e+06 Weight:  [ 122.55291748] bias:  [-870.27893066]\n",
      "step:  33 loss:  1.55701e+06 Weight:  [ 122.06270599] bias:  [-866.76922607]\n",
      "step:  34 loss:  1.54448e+06 Weight:  [ 121.57447815] bias:  [-863.27362061]\n",
      "step:  35 loss:  1.53205e+06 Weight:  [ 121.08821869] bias:  [-859.79211426]\n",
      "step:  36 loss:  1.51972e+06 Weight:  [ 120.60391235] bias:  [-856.32470703]\n",
      "step:  37 loss:  1.50748e+06 Weight:  [ 120.12156677] bias:  [-852.87127686]\n",
      "step:  38 loss:  1.49535e+06 Weight:  [ 119.64116669] bias:  [-849.4317627]\n",
      "step:  39 loss:  1.48331e+06 Weight:  [ 119.16270447] bias:  [-846.00610352]\n",
      "step:  40 loss:  1.47137e+06 Weight:  [ 118.68616486] bias:  [-842.59423828]\n",
      "step:  41 loss:  1.45953e+06 Weight:  [ 118.21154785] bias:  [-839.19616699]\n",
      "step:  42 loss:  1.44778e+06 Weight:  [ 117.73885345] bias:  [-835.81176758]\n",
      "step:  43 loss:  1.43613e+06 Weight:  [ 117.26805878] bias:  [-832.44104004]\n",
      "step:  44 loss:  1.42457e+06 Weight:  [ 116.79916382] bias:  [-829.08392334]\n",
      "step:  45 loss:  1.4131e+06 Weight:  [ 116.33215332] bias:  [-825.74035645]\n",
      "step:  46 loss:  1.40172e+06 Weight:  [ 115.86703491] bias:  [-822.41021729]\n",
      "step:  47 loss:  1.39044e+06 Weight:  [ 115.40379333] bias:  [-819.09356689]\n",
      "step:  48 loss:  1.37925e+06 Weight:  [ 114.94242096] bias:  [-815.7902832]\n",
      "step:  49 loss:  1.36815e+06 Weight:  [ 114.48290253] bias:  [-812.50030518]\n",
      "step:  50 loss:  1.35713e+06 Weight:  [ 114.02524567] bias:  [-809.22357178]\n",
      "step:  51 loss:  1.34621e+06 Weight:  [ 113.56942749] bias:  [-805.96008301]\n",
      "step:  52 loss:  1.33537e+06 Weight:  [ 113.115448] bias:  [-802.7097168]\n",
      "step:  53 loss:  1.32462e+06 Weight:  [ 112.66329193] bias:  [-799.47247314]\n",
      "step:  54 loss:  1.31396e+06 Weight:  [ 112.21296692] bias:  [-796.24829102]\n",
      "step:  55 loss:  1.30339e+06 Weight:  [ 111.7644577] bias:  [-793.03710938]\n",
      "step:  56 loss:  1.29289e+06 Weight:  [ 111.31775665] bias:  [-789.83886719]\n",
      "step:  57 loss:  1.28249e+06 Weight:  [ 110.87285614] bias:  [-786.65356445]\n",
      "step:  58 loss:  1.27216e+06 Weight:  [ 110.42975616] bias:  [-783.4810791]\n",
      "step:  59 loss:  1.26192e+06 Weight:  [ 109.98844147] bias:  [-780.32141113]\n",
      "step:  60 loss:  1.25177e+06 Weight:  [ 109.54890442] bias:  [-777.17449951]\n",
      "step:  61 loss:  1.24169e+06 Weight:  [ 109.11114502] bias:  [-774.04022217]\n",
      "step:  62 loss:  1.23169e+06 Weight:  [ 108.67514038] bias:  [-770.91864014]\n",
      "step:  63 loss:  1.22178e+06 Weight:  [ 108.24090576] bias:  [-767.80963135]\n",
      "step:  64 loss:  1.21195e+06 Weight:  [ 107.80841827] bias:  [-764.71313477]\n",
      "step:  65 loss:  1.20219e+06 Weight:  [ 107.37767029] bias:  [-761.62915039]\n",
      "step:  66 loss:  1.19251e+06 Weight:  [ 106.9486618] bias:  [-758.55761719]\n",
      "step:  67 loss:  1.18291e+06 Weight:  [ 106.52138519] bias:  [-755.49847412]\n",
      "step:  68 loss:  1.17339e+06 Weight:  [ 106.09583282] bias:  [-752.45166016]\n",
      "step:  69 loss:  1.16395e+06 Weight:  [ 105.67199707] bias:  [-749.41711426]\n",
      "step:  70 loss:  1.15458e+06 Weight:  [ 105.2498703] bias:  [-746.39477539]\n",
      "step:  71 loss:  1.14528e+06 Weight:  [ 104.82944489] bias:  [-743.38464355]\n",
      "step:  72 loss:  1.13606e+06 Weight:  [ 104.41070557] bias:  [-740.38665771]\n",
      "step:  73 loss:  1.12692e+06 Weight:  [ 103.99365997] bias:  [-737.40075684]\n",
      "step:  74 loss:  1.11785e+06 Weight:  [ 103.57830048] bias:  [-734.42687988]\n",
      "step:  75 loss:  1.10885e+06 Weight:  [ 103.16461182] bias:  [-731.46502686]\n",
      "step:  76 loss:  1.09992e+06 Weight:  [ 102.75259399] bias:  [-728.51513672]\n",
      "step:  77 loss:  1.09107e+06 Weight:  [ 102.34223938] bias:  [-725.57714844]\n",
      "step:  78 loss:  1.08229e+06 Weight:  [ 101.93354034] bias:  [-722.65100098]\n",
      "step:  79 loss:  1.07358e+06 Weight:  [ 101.52648926] bias:  [-719.7366333]\n",
      "step:  80 loss:  1.06493e+06 Weight:  [ 101.12107849] bias:  [-716.83404541]\n",
      "step:  81 loss:  1.05636e+06 Weight:  [ 100.71730804] bias:  [-713.94311523]\n",
      "step:  82 loss:  1.04786e+06 Weight:  [ 100.31515503] bias:  [-711.06384277]\n",
      "step:  83 loss:  1.03942e+06 Weight:  [ 99.91462708] bias:  [-708.19622803]\n",
      "step:  84 loss:  1.03106e+06 Weight:  [ 99.51571655] bias:  [-705.34014893]\n",
      "step:  85 loss:  1.02276e+06 Weight:  [ 99.11841583] bias:  [-702.49560547]\n",
      "step:  86 loss:  1.01453e+06 Weight:  [ 98.72271729] bias:  [-699.66253662]\n",
      "step:  87 loss:  1.00636e+06 Weight:  [ 98.32861328] bias:  [-696.84088135]\n",
      "step:  88 loss:  998259.0 Weight:  [ 97.93609619] bias:  [-694.03057861]\n",
      "step:  89 loss:  990223.0 Weight:  [ 97.54516602] bias:  [-691.23162842]\n",
      "step:  90 loss:  982252.0 Weight:  [ 97.1558075] bias:  [-688.44396973]\n",
      "step:  91 loss:  974345.0 Weight:  [ 96.76802063] bias:  [-685.6675415]\n",
      "step:  92 loss:  966502.0 Weight:  [ 96.38179779] bias:  [-682.90234375]\n",
      "step:  93 loss:  958723.0 Weight:  [ 95.99713898] bias:  [-680.14825439]\n",
      "step:  94 loss:  951005.0 Weight:  [ 95.6140213] bias:  [-677.40527344]\n",
      "step:  95 loss:  943350.0 Weight:  [ 95.23245239] bias:  [-674.67340088]\n",
      "step:  96 loss:  935757.0 Weight:  [ 94.85242462] bias:  [-671.95251465]\n",
      "step:  97 loss:  928224.0 Weight:  [ 94.47393036] bias:  [-669.24261475]\n",
      "step:  98 loss:  920753.0 Weight:  [ 94.09696198] bias:  [-666.54364014]\n",
      "step:  99 loss:  913341.0 Weight:  [ 93.72151184] bias:  [-663.85552979]\n",
      "step:  100 loss:  905989.0 Weight:  [ 93.34757996] bias:  [-661.17828369]\n",
      "step:  101 loss:  898696.0 Weight:  [ 92.97515106] bias:  [-658.51184082]\n",
      "step:  102 loss:  891462.0 Weight:  [ 92.60422516] bias:  [-655.85614014]\n",
      "step:  103 loss:  884286.0 Weight:  [ 92.23479462] bias:  [-653.21118164]\n",
      "step:  104 loss:  877168.0 Weight:  [ 91.86685944] bias:  [-650.57684326]\n",
      "step:  105 loss:  870108.0 Weight:  [ 91.50040436] bias:  [-647.953125]\n",
      "step:  106 loss:  863104.0 Weight:  [ 91.13542175] bias:  [-645.34002686]\n",
      "step:  107 loss:  856156.0 Weight:  [ 90.77191925] bias:  [-642.73742676]\n",
      "step:  108 loss:  849264.0 Weight:  [ 90.40988159] bias:  [-640.14532471]\n",
      "step:  109 loss:  842428.0 Weight:  [ 90.04930115] bias:  [-637.5637207]\n",
      "step:  110 loss:  835647.0 Weight:  [ 89.69017792] bias:  [-634.99249268]\n",
      "step:  111 loss:  828921.0 Weight:  [ 89.33249664] bias:  [-632.43164062]\n",
      "step:  112 loss:  822248.0 Weight:  [ 88.97626495] bias:  [-629.88110352]\n",
      "step:  113 loss:  815630.0 Weight:  [ 88.62146759] bias:  [-627.34088135]\n",
      "step:  114 loss:  809064.0 Weight:  [ 88.26809692] bias:  [-624.81091309]\n",
      "step:  115 loss:  802552.0 Weight:  [ 87.91616058] bias:  [-622.2911377]\n",
      "step:  116 loss:  796092.0 Weight:  [ 87.56563568] bias:  [-619.78149414]\n",
      "step:  117 loss:  789683.0 Weight:  [ 87.21652985] bias:  [-617.28198242]\n",
      "step:  118 loss:  783327.0 Weight:  [ 86.86882782] bias:  [-614.7925415]\n",
      "step:  119 loss:  777021.0 Weight:  [ 86.5225296] bias:  [-612.31317139]\n",
      "step:  120 loss:  770767.0 Weight:  [ 86.17762756] bias:  [-609.84381104]\n",
      "step:  121 loss:  764563.0 Weight:  [ 85.83411407] bias:  [-607.38439941]\n",
      "step:  122 loss:  758408.0 Weight:  [ 85.49198914] bias:  [-604.93487549]\n",
      "step:  123 loss:  752303.0 Weight:  [ 85.15124512] bias:  [-602.49523926]\n",
      "step:  124 loss:  746248.0 Weight:  [ 84.81187439] bias:  [-600.06542969]\n",
      "step:  125 loss:  740241.0 Weight:  [ 84.47386932] bias:  [-597.64544678]\n",
      "step:  126 loss:  734282.0 Weight:  [ 84.13722992] bias:  [-595.23522949]\n",
      "step:  127 loss:  728372.0 Weight:  [ 83.80194855] bias:  [-592.8347168]\n",
      "step:  128 loss:  722509.0 Weight:  [ 83.46801758] bias:  [-590.44390869]\n",
      "step:  129 loss:  716693.0 Weight:  [ 83.13543701] bias:  [-588.06274414]\n",
      "step:  130 loss:  710924.0 Weight:  [ 82.80419922] bias:  [-585.69116211]\n",
      "step:  131 loss:  705201.0 Weight:  [ 82.47428894] bias:  [-583.3291626]\n",
      "step:  132 loss:  699525.0 Weight:  [ 82.14571381] bias:  [-580.97668457]\n",
      "step:  133 loss:  693894.0 Weight:  [ 81.81846619] bias:  [-578.63366699]\n",
      "step:  134 loss:  688309.0 Weight:  [ 81.49253845] bias:  [-576.30010986]\n",
      "step:  135 loss:  682768.0 Weight:  [ 81.16792297] bias:  [-573.97595215]\n",
      "step:  136 loss:  677272.0 Weight:  [ 80.84461212] bias:  [-571.66119385]\n",
      "step:  137 loss:  671820.0 Weight:  [ 80.52261353] bias:  [-569.35577393]\n",
      "step:  138 loss:  666413.0 Weight:  [ 80.20191193] bias:  [-567.05963135]\n",
      "step:  139 loss:  661048.0 Weight:  [ 79.88249969] bias:  [-564.77276611]\n",
      "step:  140 loss:  655727.0 Weight:  [ 79.56437683] bias:  [-562.49511719]\n",
      "step:  141 loss:  650449.0 Weight:  [ 79.24753571] bias:  [-560.22662354]\n",
      "step:  142 loss:  645213.0 Weight:  [ 78.93196869] bias:  [-557.96728516]\n",
      "step:  143 loss:  640020.0 Weight:  [ 78.61768341] bias:  [-555.71704102]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step:  144 loss:  634868.0 Weight:  [ 78.30465698] bias:  [-553.47589111]\n",
      "step:  145 loss:  629757.0 Weight:  [ 77.9928894] bias:  [-551.24377441]\n",
      "step:  146 loss:  624688.0 Weight:  [ 77.68238831] bias:  [-549.02069092]\n",
      "step:  147 loss:  619660.0 Weight:  [ 77.37313843] bias:  [-546.80657959]\n",
      "step:  148 loss:  614672.0 Weight:  [ 77.06513977] bias:  [-544.60137939]\n",
      "step:  149 loss:  609724.0 Weight:  [ 76.75837708] bias:  [-542.40509033]\n",
      "step:  150 loss:  604816.0 Weight:  [ 76.45285797] bias:  [-540.21765137]\n",
      "step:  151 loss:  599948.0 Weight:  [ 76.1485672] bias:  [-538.03900146]\n",
      "step:  152 loss:  595118.0 Weight:  [ 75.84549713] bias:  [-535.86914062]\n",
      "step:  153 loss:  590328.0 Weight:  [ 75.5436554] bias:  [-533.70806885]\n",
      "step:  154 loss:  585576.0 Weight:  [ 75.24303436] bias:  [-531.55566406]\n",
      "step:  155 loss:  580862.0 Weight:  [ 74.94361877] bias:  [-529.4119873]\n",
      "step:  156 loss:  576187.0 Weight:  [ 74.64541626] bias:  [-527.2769165]\n",
      "step:  157 loss:  571549.0 Weight:  [ 74.34841156] bias:  [-525.15045166]\n",
      "step:  158 loss:  566948.0 Weight:  [ 74.05259705] bias:  [-523.03259277]\n",
      "step:  159 loss:  562384.0 Weight:  [ 73.75798798] bias:  [-520.92327881]\n",
      "step:  160 loss:  557858.0 Weight:  [ 73.46456146] bias:  [-518.82244873]\n",
      "step:  161 loss:  553367.0 Weight:  [ 73.17232513] bias:  [-516.73010254]\n",
      "step:  162 loss:  548913.0 Weight:  [ 72.88126373] bias:  [-514.6461792]\n",
      "step:  163 loss:  544494.0 Weight:  [ 72.59136963] bias:  [-512.57067871]\n",
      "step:  164 loss:  540111.0 Weight:  [ 72.30265045] bias:  [-510.50354004]\n",
      "step:  165 loss:  535764.0 Weight:  [ 72.01509857] bias:  [-508.44473267]\n",
      "step:  166 loss:  531451.0 Weight:  [ 71.72869873] bias:  [-506.39422607]\n",
      "step:  167 loss:  527173.0 Weight:  [ 71.44345856] bias:  [-504.35198975]\n",
      "step:  168 loss:  522930.0 Weight:  [ 71.15937042] bias:  [-502.31799316]\n",
      "step:  169 loss:  518720.0 Weight:  [ 70.8764267] bias:  [-500.29220581]\n",
      "step:  170 loss:  514545.0 Weight:  [ 70.59461975] bias:  [-498.27459717]\n",
      "step:  171 loss:  510403.0 Weight:  [ 70.31395721] bias:  [-496.2651062]\n",
      "step:  172 loss:  506295.0 Weight:  [ 70.0344162] bias:  [-494.26373291]\n",
      "step:  173 loss:  502219.0 Weight:  [ 69.75601196] bias:  [-492.27041626]\n",
      "step:  174 loss:  498176.0 Weight:  [ 69.47872925] bias:  [-490.28515625]\n",
      "step:  175 loss:  494166.0 Weight:  [ 69.20256042] bias:  [-488.30789185]\n",
      "step:  176 loss:  490189.0 Weight:  [ 68.92750549] bias:  [-486.33859253]\n",
      "step:  177 loss:  486243.0 Weight:  [ 68.65356445] bias:  [-484.3772583]\n",
      "step:  178 loss:  482329.0 Weight:  [ 68.38072205] bias:  [-482.42382812]\n",
      "step:  179 loss:  478446.0 Weight:  [ 68.1089859] bias:  [-480.47827148]\n",
      "step:  180 loss:  474595.0 Weight:  [ 67.83834076] bias:  [-478.54055786]\n",
      "step:  181 loss:  470775.0 Weight:  [ 67.56879425] bias:  [-476.61065674]\n",
      "step:  182 loss:  466985.0 Weight:  [ 67.30033112] bias:  [-474.6885376]\n",
      "step:  183 loss:  463226.0 Weight:  [ 67.03294373] bias:  [-472.77416992]\n",
      "step:  184 loss:  459498.0 Weight:  [ 66.76663971] bias:  [-470.86752319]\n",
      "step:  185 loss:  455799.0 Weight:  [ 66.50141144] bias:  [-468.96856689]\n",
      "step:  186 loss:  452130.0 Weight:  [ 66.23725128] bias:  [-467.07727051]\n",
      "step:  187 loss:  448490.0 Weight:  [ 65.97415924] bias:  [-465.19360352]\n",
      "step:  188 loss:  444880.0 Weight:  [ 65.71212769] bias:  [-463.3175354]\n",
      "step:  189 loss:  441299.0 Weight:  [ 65.45114899] bias:  [-461.44903564]\n",
      "step:  190 loss:  437747.0 Weight:  [ 65.19122314] bias:  [-459.58807373]\n",
      "step:  191 loss:  434223.0 Weight:  [ 64.93235016] bias:  [-457.73461914]\n",
      "step:  192 loss:  430728.0 Weight:  [ 64.6745224] bias:  [-455.88864136]\n",
      "step:  193 loss:  427261.0 Weight:  [ 64.41773224] bias:  [-454.05010986]\n",
      "step:  194 loss:  423822.0 Weight:  [ 64.16197968] bias:  [-452.21899414]\n",
      "step:  195 loss:  420410.0 Weight:  [ 63.90725327] bias:  [-450.39526367]\n",
      "step:  196 loss:  417026.0 Weight:  [ 63.65355682] bias:  [-448.57888794]\n",
      "step:  197 loss:  413669.0 Weight:  [ 63.40088654] bias:  [-446.76983643]\n",
      "step:  198 loss:  410340.0 Weight:  [ 63.14923096] bias:  [-444.96807861]\n",
      "step:  199 loss:  407037.0 Weight:  [ 62.8985939] bias:  [-443.17358398]\n",
      "step:  200 loss:  403760.0 Weight:  [ 62.64896393] bias:  [-441.38632202]\n",
      "step:  201 loss:  400510.0 Weight:  [ 62.40034103] bias:  [-439.60626221]\n",
      "step:  202 loss:  397286.0 Weight:  [ 62.15272141] bias:  [-437.83337402]\n",
      "step:  203 loss:  394088.0 Weight:  [ 61.90609741] bias:  [-436.06765747]\n",
      "step:  204 loss:  390916.0 Weight:  [ 61.66047287] bias:  [-434.30905151]\n",
      "step:  205 loss:  387769.0 Weight:  [ 61.41583633] bias:  [-432.55752563]\n",
      "step:  206 loss:  384648.0 Weight:  [ 61.17218781] bias:  [-430.81307983]\n",
      "step:  207 loss:  381552.0 Weight:  [ 60.92951965] bias:  [-429.07565308]\n",
      "step:  208 loss:  378481.0 Weight:  [ 60.68783188] bias:  [-427.34524536]\n",
      "step:  209 loss:  375434.0 Weight:  [ 60.44711685] bias:  [-425.62182617]\n",
      "step:  210 loss:  372412.0 Weight:  [ 60.20737457] bias:  [-423.90533447]\n",
      "step:  211 loss:  369414.0 Weight:  [ 59.96859741] bias:  [-422.19577026]\n",
      "step:  212 loss:  366441.0 Weight:  [ 59.73078537] bias:  [-420.49310303]\n",
      "step:  213 loss:  363491.0 Weight:  [ 59.49393082] bias:  [-418.79730225]\n",
      "step:  214 loss:  360565.0 Weight:  [ 59.25802994] bias:  [-417.1083374]\n",
      "step:  215 loss:  357663.0 Weight:  [ 59.02308273] bias:  [-415.42617798]\n",
      "step:  216 loss:  354784.0 Weight:  [ 58.78908157] bias:  [-413.75082397]\n",
      "step:  217 loss:  351928.0 Weight:  [ 58.55602646] bias:  [-412.08221436]\n",
      "step:  218 loss:  349095.0 Weight:  [ 58.32390976] bias:  [-410.42034912]\n",
      "step:  219 loss:  346285.0 Weight:  [ 58.09273148] bias:  [-408.76516724]\n",
      "step:  220 loss:  343497.0 Weight:  [ 57.86248016] bias:  [-407.1166687]\n",
      "step:  221 loss:  340733.0 Weight:  [ 57.63315964] bias:  [-405.474823]\n",
      "step:  222 loss:  337990.0 Weight:  [ 57.40476608] bias:  [-403.83959961]\n",
      "step:  223 loss:  335269.0 Weight:  [ 57.17729568] bias:  [-402.21096802]\n",
      "step:  224 loss:  332570.0 Weight:  [ 56.95074081] bias:  [-400.58889771]\n",
      "step:  225 loss:  329893.0 Weight:  [ 56.72509766] bias:  [-398.97335815]\n",
      "step:  226 loss:  327238.0 Weight:  [ 56.5003624] bias:  [-397.36434937]\n",
      "step:  227 loss:  324604.0 Weight:  [ 56.27653503] bias:  [-395.76184082]\n",
      "step:  228 loss:  321991.0 Weight:  [ 56.05361176] bias:  [-394.16577148]\n",
      "step:  229 loss:  319399.0 Weight:  [ 55.83158875] bias:  [-392.57614136]\n",
      "step:  230 loss:  316828.0 Weight:  [ 55.61045837] bias:  [-390.99291992]\n",
      "step:  231 loss:  314278.0 Weight:  [ 55.39022064] bias:  [-389.41610718]\n",
      "step:  232 loss:  311748.0 Weight:  [ 55.17087173] bias:  [-387.84564209]\n",
      "step:  233 loss:  309239.0 Weight:  [ 54.95240784] bias:  [-386.28149414]\n",
      "step:  234 loss:  306749.0 Weight:  [ 54.73482513] bias:  [-384.72366333]\n",
      "step:  235 loss:  304280.0 Weight:  [ 54.518116] bias:  [-383.17211914]\n",
      "step:  236 loss:  301831.0 Weight:  [ 54.30228424] bias:  [-381.62683105]\n",
      "step:  237 loss:  299401.0 Weight:  [ 54.08732224] bias:  [-380.08776855]\n",
      "step:  238 loss:  296991.0 Weight:  [ 53.87322617] bias:  [-378.55493164]\n",
      "step:  239 loss:  294601.0 Weight:  [ 53.65999603] bias:  [-377.02825928]\n",
      "step:  240 loss:  292229.0 Weight:  [ 53.44762421] bias:  [-375.50775146]\n",
      "step:  241 loss:  289877.0 Weight:  [ 53.23611069] bias:  [-373.99337769]\n",
      "step:  242 loss:  287544.0 Weight:  [ 53.02544785] bias:  [-372.48510742]\n",
      "step:  243 loss:  285229.0 Weight:  [ 52.81563568] bias:  [-370.98291016]\n",
      "step:  244 loss:  282933.0 Weight:  [ 52.60667038] bias:  [-369.48678589]\n",
      "step:  245 loss:  280656.0 Weight:  [ 52.39854431] bias:  [-367.9967041]\n",
      "step:  246 loss:  278396.0 Weight:  [ 52.19126129] bias:  [-366.51260376]\n",
      "step:  247 loss:  276155.0 Weight:  [ 51.98481369] bias:  [-365.03451538]\n",
      "step:  248 loss:  273933.0 Weight:  [ 51.77919769] bias:  [-363.56237793]\n",
      "step:  249 loss:  271728.0 Weight:  [ 51.5744133] bias:  [-362.09616089]\n",
      "step:  250 loss:  269540.0 Weight:  [ 51.37045288] bias:  [-360.63586426]\n",
      "step:  251 loss:  267371.0 Weight:  [ 51.16731262] bias:  [-359.18145752]\n",
      "step:  252 loss:  265218.0 Weight:  [ 50.96499252] bias:  [-357.73291016]\n",
      "step:  253 loss:  263083.0 Weight:  [ 50.76348877] bias:  [-356.29022217]\n",
      "step:  254 loss:  260966.0 Weight:  [ 50.56279755] bias:  [-354.85333252]\n",
      "step:  255 loss:  258865.0 Weight:  [ 50.36291504] bias:  [-353.42224121]\n",
      "step:  256 loss:  256781.0 Weight:  [ 50.16384125] bias:  [-351.99691772]\n",
      "step:  257 loss:  254714.0 Weight:  [ 49.96556854] bias:  [-350.57736206]\n",
      "step:  258 loss:  252664.0 Weight:  [ 49.76809311] bias:  [-349.16351318]\n",
      "step:  259 loss:  250630.0 Weight:  [ 49.57141876] bias:  [-347.75537109]\n",
      "step:  260 loss:  248613.0 Weight:  [ 49.37553406] bias:  [-346.35290527]\n",
      "step:  261 loss:  246612.0 Weight:  [ 49.180439] bias:  [-344.95611572]\n",
      "step:  262 loss:  244626.0 Weight:  [ 48.98613358] bias:  [-343.56494141]\n",
      "step:  263 loss:  242657.0 Weight:  [ 48.79261017] bias:  [-342.17938232]\n",
      "step:  264 loss:  240704.0 Weight:  [ 48.59986877] bias:  [-340.79940796]\n",
      "step:  265 loss:  238767.0 Weight:  [ 48.40790558] bias:  [-339.42501831]\n",
      "step:  266 loss:  236845.0 Weight:  [ 48.21671677] bias:  [-338.05615234]\n",
      "step:  267 loss:  234938.0 Weight:  [ 48.02629471] bias:  [-336.69281006]\n",
      "step:  268 loss:  233047.0 Weight:  [ 47.83664322] bias:  [-335.33496094]\n",
      "step:  269 loss:  231171.0 Weight:  [ 47.64775467] bias:  [-333.98260498]\n",
      "step:  270 loss:  229310.0 Weight:  [ 47.45963287] bias:  [-332.63568115]\n",
      "step:  271 loss:  227464.0 Weight:  [ 47.27226639] bias:  [-331.29418945]\n",
      "step:  272 loss:  225633.0 Weight:  [ 47.08565521] bias:  [-329.95812988]\n",
      "step:  273 loss:  223817.0 Weight:  [ 46.89979553] bias:  [-328.62744141]\n",
      "step:  274 loss:  222016.0 Weight:  [ 46.71468735] bias:  [-327.30212402]\n",
      "step:  275 loss:  220228.0 Weight:  [ 46.53032684] bias:  [-325.98214722]\n",
      "step:  276 loss:  218456.0 Weight:  [ 46.34670639] bias:  [-324.66751099]\n",
      "step:  277 loss:  216697.0 Weight:  [ 46.1638298] bias:  [-323.3581543]\n",
      "step:  278 loss:  214953.0 Weight:  [ 45.98168945] bias:  [-322.05407715]\n",
      "step:  279 loss:  213223.0 Weight:  [ 45.80028152] bias:  [-320.75527954]\n",
      "step:  280 loss:  211506.0 Weight:  [ 45.61960602] bias:  [-319.46170044]\n",
      "step:  281 loss:  209804.0 Weight:  [ 45.43965912] bias:  [-318.17333984]\n",
      "step:  282 loss:  208115.0 Weight:  [ 45.26043701] bias:  [-316.89019775]\n",
      "step:  283 loss:  206440.0 Weight:  [ 45.08194351] bias:  [-315.61221313]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step:  284 loss:  204778.0 Weight:  [ 44.90416718] bias:  [-314.33938599]\n",
      "step:  285 loss:  203130.0 Weight:  [ 44.72710419] bias:  [-313.07168579]\n",
      "step:  286 loss:  201495.0 Weight:  [ 44.55075836] bias:  [-311.80911255]\n",
      "step:  287 loss:  199873.0 Weight:  [ 44.37512207] bias:  [-310.55163574]\n",
      "step:  288 loss:  198264.0 Weight:  [ 44.20019913] bias:  [-309.29922485]\n",
      "step:  289 loss:  196668.0 Weight:  [ 44.02597809] bias:  [-308.05184937]\n",
      "step:  290 loss:  195085.0 Weight:  [ 43.85245895] bias:  [-306.80950928]\n",
      "step:  291 loss:  193514.0 Weight:  [ 43.67963791] bias:  [-305.57217407]\n",
      "step:  292 loss:  191957.0 Weight:  [ 43.50751495] bias:  [-304.33984375]\n",
      "step:  293 loss:  190412.0 Weight:  [ 43.33609009] bias:  [-303.11248779]\n",
      "step:  294 loss:  188879.0 Weight:  [ 43.16535187] bias:  [-301.89007568]\n",
      "step:  295 loss:  187359.0 Weight:  [ 42.99530792] bias:  [-300.6725769]\n",
      "step:  296 loss:  185850.0 Weight:  [ 42.82594299] bias:  [-299.45999146]\n",
      "step:  297 loss:  184354.0 Weight:  [ 42.65726471] bias:  [-298.25231934]\n",
      "step:  298 loss:  182870.0 Weight:  [ 42.48926544] bias:  [-297.04949951]\n",
      "step:  299 loss:  181398.0 Weight:  [ 42.32194519] bias:  [-295.85153198]\n",
      "step:  300 loss:  179938.0 Weight:  [ 42.15529633] bias:  [-294.65838623]\n",
      "step:  301 loss:  178490.0 Weight:  [ 41.98932266] bias:  [-293.47006226]\n",
      "step:  302 loss:  177053.0 Weight:  [ 41.82401657] bias:  [-292.28652954]\n",
      "step:  303 loss:  175628.0 Weight:  [ 41.65937805] bias:  [-291.10778809]\n",
      "step:  304 loss:  174214.0 Weight:  [ 41.49540329] bias:  [-289.93377686]\n",
      "step:  305 loss:  172812.0 Weight:  [ 41.33209229] bias:  [-288.76449585]\n",
      "step:  306 loss:  171421.0 Weight:  [ 41.16943741] bias:  [-287.59994507]\n",
      "step:  307 loss:  170041.0 Weight:  [ 41.00743866] bias:  [-286.44009399]\n",
      "step:  308 loss:  168672.0 Weight:  [ 40.84609222] bias:  [-285.28491211]\n",
      "step:  309 loss:  167314.0 Weight:  [ 40.6853981] bias:  [-284.13439941]\n",
      "step:  310 loss:  165968.0 Weight:  [ 40.52535248] bias:  [-282.98852539]\n",
      "step:  311 loss:  164632.0 Weight:  [ 40.36595154] bias:  [-281.84725952]\n",
      "step:  312 loss:  163306.0 Weight:  [ 40.20719147] bias:  [-280.71060181]\n",
      "step:  313 loss:  161992.0 Weight:  [ 40.04907227] bias:  [-279.57852173]\n",
      "step:  314 loss:  160688.0 Weight:  [ 39.89159393] bias:  [-278.45101929]\n",
      "step:  315 loss:  159395.0 Weight:  [ 39.73474884] bias:  [-277.32806396]\n",
      "step:  316 loss:  158111.0 Weight:  [ 39.57853699] bias:  [-276.20962524]\n",
      "step:  317 loss:  156839.0 Weight:  [ 39.42295456] bias:  [-275.09570312]\n",
      "step:  318 loss:  155576.0 Weight:  [ 39.26799774] bias:  [-273.98626709]\n",
      "step:  319 loss:  154324.0 Weight:  [ 39.11366653] bias:  [-272.88131714]\n",
      "step:  320 loss:  153082.0 Weight:  [ 38.95995712] bias:  [-271.78082275]\n",
      "step:  321 loss:  151850.0 Weight:  [ 38.80686951] bias:  [-270.68475342]\n",
      "step:  322 loss:  150627.0 Weight:  [ 38.65439987] bias:  [-269.59310913]\n",
      "step:  323 loss:  149415.0 Weight:  [ 38.5025444] bias:  [-268.50585938]\n",
      "step:  324 loss:  148212.0 Weight:  [ 38.35129929] bias:  [-267.42300415]\n",
      "step:  325 loss:  147019.0 Weight:  [ 38.20066452] bias:  [-266.34451294]\n",
      "step:  326 loss:  145836.0 Weight:  [ 38.05063629] bias:  [-265.27038574]\n",
      "step:  327 loss:  144662.0 Weight:  [ 37.90121841] bias:  [-264.20059204]\n",
      "step:  328 loss:  143497.0 Weight:  [ 37.75239944] bias:  [-263.13510132]\n",
      "step:  329 loss:  142342.0 Weight:  [ 37.6041832] bias:  [-262.07391357]\n",
      "step:  330 loss:  141196.0 Weight:  [ 37.45656204] bias:  [-261.01699829]\n",
      "step:  331 loss:  140060.0 Weight:  [ 37.30953598] bias:  [-259.96435547]\n",
      "step:  332 loss:  138932.0 Weight:  [ 37.16310501] bias:  [-258.91595459]\n",
      "step:  333 loss:  137814.0 Weight:  [ 37.01726532] bias:  [-257.87176514]\n",
      "step:  334 loss:  136705.0 Weight:  [ 36.87200928] bias:  [-256.83178711]\n",
      "step:  335 loss:  135604.0 Weight:  [ 36.7273407] bias:  [-255.79602051]\n",
      "step:  336 loss:  134513.0 Weight:  [ 36.58325577] bias:  [-254.76441956]\n",
      "step:  337 loss:  133430.0 Weight:  [ 36.43975449] bias:  [-253.73698425]\n",
      "step:  338 loss:  132356.0 Weight:  [ 36.29682922] bias:  [-252.71369934]\n",
      "step:  339 loss:  131291.0 Weight:  [ 36.1544838] bias:  [-251.6945343]\n",
      "step:  340 loss:  130234.0 Weight:  [ 36.01271057] bias:  [-250.67947388]\n",
      "step:  341 loss:  129185.0 Weight:  [ 35.87150574] bias:  [-249.66851807]\n",
      "step:  342 loss:  128146.0 Weight:  [ 35.73087311] bias:  [-248.66163635]\n",
      "step:  343 loss:  127114.0 Weight:  [ 35.59080887] bias:  [-247.65881348]\n",
      "step:  344 loss:  126091.0 Weight:  [ 35.4513092] bias:  [-246.66003418]\n",
      "step:  345 loss:  125076.0 Weight:  [ 35.3123703] bias:  [-245.6652832]\n",
      "step:  346 loss:  124069.0 Weight:  [ 35.17399216] bias:  [-244.67454529]\n",
      "step:  347 loss:  123070.0 Weight:  [ 35.03617096] bias:  [-243.68780518]\n",
      "step:  348 loss:  122080.0 Weight:  [ 34.89891052] bias:  [-242.70504761]\n",
      "step:  349 loss:  121097.0 Weight:  [ 34.7621994] bias:  [-241.72624207]\n",
      "step:  350 loss:  120122.0 Weight:  [ 34.62604141] bias:  [-240.75138855]\n",
      "step:  351 loss:  119155.0 Weight:  [ 34.49042892] bias:  [-239.7804718]\n",
      "step:  352 loss:  118196.0 Weight:  [ 34.35536575] bias:  [-238.8134613]\n",
      "step:  353 loss:  117245.0 Weight:  [ 34.22084808] bias:  [-237.85035706]\n",
      "step:  354 loss:  116301.0 Weight:  [ 34.0868721] bias:  [-236.89112854]\n",
      "step:  355 loss:  115365.0 Weight:  [ 33.95343781] bias:  [-235.93577576]\n",
      "step:  356 loss:  114436.0 Weight:  [ 33.82054138] bias:  [-234.98426819]\n",
      "step:  357 loss:  113515.0 Weight:  [ 33.68817902] bias:  [-234.03660583]\n",
      "step:  358 loss:  112601.0 Weight:  [ 33.55635071] bias:  [-233.09275818]\n",
      "step:  359 loss:  111695.0 Weight:  [ 33.42505264] bias:  [-232.15272522]\n",
      "step:  360 loss:  110796.0 Weight:  [ 33.29428864] bias:  [-231.21647644]\n",
      "step:  361 loss:  109904.0 Weight:  [ 33.16404724] bias:  [-230.28401184]\n",
      "step:  362 loss:  109019.0 Weight:  [ 33.03433228] bias:  [-229.3553009]\n",
      "step:  363 loss:  108142.0 Weight:  [ 32.90514374] bias:  [-228.43034363]\n",
      "step:  364 loss:  107271.0 Weight:  [ 32.776474] bias:  [-227.5091095]\n",
      "step:  365 loss:  106408.0 Weight:  [ 32.64832306] bias:  [-226.59159851]\n",
      "step:  366 loss:  105551.0 Weight:  [ 32.52069092] bias:  [-225.67778015]\n",
      "step:  367 loss:  104702.0 Weight:  [ 32.39356995] bias:  [-224.76765442]\n",
      "step:  368 loss:  103859.0 Weight:  [ 32.26696396] bias:  [-223.8611908]\n",
      "step:  369 loss:  103023.0 Weight:  [ 32.14086914] bias:  [-222.95838928]\n",
      "step:  370 loss:  102194.0 Weight:  [ 32.01528168] bias:  [-222.05921936]\n",
      "step:  371 loss:  101371.0 Weight:  [ 31.89019966] bias:  [-221.16368103]\n",
      "step:  372 loss:  100555.0 Weight:  [ 31.76562309] bias:  [-220.27175903]\n",
      "step:  373 loss:  99745.5 Weight:  [ 31.64155006] bias:  [-219.38342285]\n",
      "step:  374 loss:  98942.6 Weight:  [ 31.51797676] bias:  [-218.49867249]\n",
      "step:  375 loss:  98146.2 Weight:  [ 31.39490128] bias:  [-217.61749268]\n",
      "step:  376 loss:  97356.1 Weight:  [ 31.2723217] bias:  [-216.73986816]\n",
      "step:  377 loss:  96572.5 Weight:  [ 31.15023804] bias:  [-215.86578369]\n",
      "step:  378 loss:  95795.1 Weight:  [ 31.02864456] bias:  [-214.995224]\n",
      "step:  379 loss:  95024.0 Weight:  [ 30.90754318] bias:  [-214.12817383]\n",
      "step:  380 loss:  94259.1 Weight:  [ 30.78693008] bias:  [-213.26461792]\n",
      "step:  381 loss:  93500.4 Weight:  [ 30.66680145] bias:  [-212.40454102]\n",
      "step:  382 loss:  92747.8 Weight:  [ 30.54715729] bias:  [-211.54794312]\n",
      "step:  383 loss:  92001.2 Weight:  [ 30.4279995] bias:  [-210.6947937]\n",
      "step:  384 loss:  91260.6 Weight:  [ 30.30931854] bias:  [-209.84509277]\n",
      "step:  385 loss:  90526.0 Weight:  [ 30.19111824] bias:  [-208.99880981]\n",
      "step:  386 loss:  89797.3 Weight:  [ 30.07339478] bias:  [-208.15594482]\n",
      "step:  387 loss:  89074.5 Weight:  [ 29.95614433] bias:  [-207.31648254]\n",
      "step:  388 loss:  88357.5 Weight:  [ 29.83936882] bias:  [-206.48040771]\n",
      "step:  389 loss:  87646.3 Weight:  [ 29.72306442] bias:  [-205.64768982]\n",
      "step:  390 loss:  86940.8 Weight:  [ 29.60722733] bias:  [-204.81834412]\n",
      "step:  391 loss:  86240.9 Weight:  [ 29.49185753] bias:  [-203.99234009]\n",
      "step:  392 loss:  85546.7 Weight:  [ 29.37695312] bias:  [-203.16966248]\n",
      "step:  393 loss:  84858.1 Weight:  [ 29.26251411] bias:  [-202.35031128]\n",
      "step:  394 loss:  84175.1 Weight:  [ 29.14853477] bias:  [-201.53425598]\n",
      "step:  395 loss:  83497.5 Weight:  [ 29.03501511] bias:  [-200.72149658]\n",
      "step:  396 loss:  82825.4 Weight:  [ 28.9219532] bias:  [-199.91201782]\n",
      "step:  397 loss:  82158.7 Weight:  [ 28.80934906] bias:  [-199.10578918]\n",
      "step:  398 loss:  81497.4 Weight:  [ 28.69719696] bias:  [-198.30282593]\n",
      "step:  399 loss:  80841.4 Weight:  [ 28.5854969] bias:  [-197.50309753]\n",
      "step:  400 loss:  80190.6 Weight:  [ 28.47424889] bias:  [-196.70658875]\n",
      "step:  401 loss:  79545.1 Weight:  [ 28.3634491] bias:  [-195.91329956]\n",
      "step:  402 loss:  78904.8 Weight:  [ 28.25309563] bias:  [-195.12319946]\n",
      "step:  403 loss:  78269.7 Weight:  [ 28.14318657] bias:  [-194.33628845]\n",
      "step:  404 loss:  77639.6 Weight:  [ 28.03372192] bias:  [-193.55255127]\n",
      "step:  405 loss:  77014.7 Weight:  [ 27.92469788] bias:  [-192.77197266]\n",
      "step:  406 loss:  76394.8 Weight:  [ 27.81611252] bias:  [-191.99455261]\n",
      "step:  407 loss:  75779.8 Weight:  [ 27.70796585] bias:  [-191.22026062]\n",
      "step:  408 loss:  75169.8 Weight:  [ 27.60025597] bias:  [-190.44909668]\n",
      "step:  409 loss:  74564.8 Weight:  [ 27.49298096] bias:  [-189.68104553]\n",
      "step:  410 loss:  73964.5 Weight:  [ 27.38613892] bias:  [-188.91609192]\n",
      "step:  411 loss:  73369.2 Weight:  [ 27.27972794] bias:  [-188.15422058]\n",
      "step:  412 loss:  72778.6 Weight:  [ 27.17374611] bias:  [-187.39541626]\n",
      "step:  413 loss:  72192.8 Weight:  [ 27.06818962] bias:  [-186.63967896]\n",
      "step:  414 loss:  71611.7 Weight:  [ 26.96306038] bias:  [-185.88697815]\n",
      "step:  415 loss:  71035.2 Weight:  [ 26.85835457] bias:  [-185.13731384]\n",
      "step:  416 loss:  70463.4 Weight:  [ 26.75407028] bias:  [-184.39067078]\n",
      "step:  417 loss:  69896.2 Weight:  [ 26.65020752] bias:  [-183.64704895]\n",
      "step:  418 loss:  69333.6 Weight:  [ 26.54676247] bias:  [-182.90641785]\n",
      "step:  419 loss:  68775.5 Weight:  [ 26.44373512] bias:  [-182.16877747]\n",
      "step:  420 loss:  68221.9 Weight:  [ 26.34112358] bias:  [-181.43411255]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step:  421 loss:  67672.7 Weight:  [ 26.23892593] bias:  [-180.70240784]\n",
      "step:  422 loss:  67128.0 Weight:  [ 26.13714027] bias:  [-179.97366333]\n",
      "step:  423 loss:  66587.6 Weight:  [ 26.03576469] bias:  [-179.24784851]\n",
      "step:  424 loss:  66051.6 Weight:  [ 25.93479919] bias:  [-178.52496338]\n",
      "step:  425 loss:  65520.0 Weight:  [ 25.83423996] bias:  [-177.80499268]\n",
      "step:  426 loss:  64992.6 Weight:  [ 25.73408699] bias:  [-177.08792114]\n",
      "step:  427 loss:  64469.4 Weight:  [ 25.63433647] bias:  [-176.37374878]\n",
      "step:  428 loss:  63950.5 Weight:  [ 25.5349884] bias:  [-175.66246033]\n",
      "step:  429 loss:  63435.7 Weight:  [ 25.43604279] bias:  [-174.95404053]\n",
      "step:  430 loss:  62925.1 Weight:  [ 25.3374958] bias:  [-174.24847412]\n",
      "step:  431 loss:  62418.6 Weight:  [ 25.23934555] bias:  [-173.54574585]\n",
      "step:  432 loss:  61916.1 Weight:  [ 25.14159203] bias:  [-172.84585571]\n",
      "step:  433 loss:  61417.7 Weight:  [ 25.04423141] bias:  [-172.14878845]\n",
      "step:  434 loss:  60923.3 Weight:  [ 24.94726372] bias:  [-171.45452881]\n",
      "step:  435 loss:  60432.9 Weight:  [ 24.85068703] bias:  [-170.76307678]\n",
      "step:  436 loss:  59946.5 Weight:  [ 24.75449944] bias:  [-170.07441711]\n",
      "step:  437 loss:  59464.0 Weight:  [ 24.65870094] bias:  [-169.38853455]\n",
      "step:  438 loss:  58985.3 Weight:  [ 24.56328964] bias:  [-168.70541382]\n",
      "step:  439 loss:  58510.5 Weight:  [ 24.46826172] bias:  [-168.02503967]\n",
      "step:  440 loss:  58039.5 Weight:  [ 24.37361717] bias:  [-167.34741211]\n",
      "step:  441 loss:  57572.3 Weight:  [ 24.2793541] bias:  [-166.67251587]\n",
      "step:  442 loss:  57108.9 Weight:  [ 24.18547058] bias:  [-166.00035095]\n",
      "step:  443 loss:  56649.2 Weight:  [ 24.09196663] bias:  [-165.33088684]\n",
      "step:  444 loss:  56193.2 Weight:  [ 23.99883842] bias:  [-164.66412354]\n",
      "step:  445 loss:  55740.9 Weight:  [ 23.90608788] bias:  [-164.00004578]\n",
      "step:  446 loss:  55292.2 Weight:  [ 23.81370926] bias:  [-163.33865356]\n",
      "step:  447 loss:  54847.1 Weight:  [ 23.72170448] bias:  [-162.67993164]\n",
      "step:  448 loss:  54405.6 Weight:  [ 23.63006973] bias:  [-162.02386475]\n",
      "step:  449 loss:  53967.7 Weight:  [ 23.53880501] bias:  [-161.37043762]\n",
      "step:  450 loss:  53533.3 Weight:  [ 23.4479084] bias:  [-160.71965027]\n",
      "step:  451 loss:  53102.3 Weight:  [ 23.35737991] bias:  [-160.07148743]\n",
      "step:  452 loss:  52674.9 Weight:  [ 23.26721573] bias:  [-159.42593384]\n",
      "step:  453 loss:  52250.9 Weight:  [ 23.17741394] bias:  [-158.7829895]\n",
      "step:  454 loss:  51830.3 Weight:  [ 23.08797455] bias:  [-158.14263916]\n",
      "step:  455 loss:  51413.1 Weight:  [ 22.99889755] bias:  [-157.50486755]\n",
      "step:  456 loss:  50999.2 Weight:  [ 22.91017723] bias:  [-156.86967468]\n",
      "step:  457 loss:  50588.7 Weight:  [ 22.8218174] bias:  [-156.23704529]\n",
      "step:  458 loss:  50181.5 Weight:  [ 22.73381424] bias:  [-155.60696411]\n",
      "step:  459 loss:  49777.6 Weight:  [ 22.64616394] bias:  [-154.97941589]\n",
      "step:  460 loss:  49376.9 Weight:  [ 22.5588665] bias:  [-154.35440063]\n",
      "step:  461 loss:  48979.4 Weight:  [ 22.47192192] bias:  [-153.73190308]\n",
      "step:  462 loss:  48585.2 Weight:  [ 22.38532829] bias:  [-153.11192322]\n",
      "step:  463 loss:  48194.1 Weight:  [ 22.29908371] bias:  [-152.4944458]\n",
      "step:  464 loss:  47806.2 Weight:  [ 22.21318817] bias:  [-151.87945557]\n",
      "step:  465 loss:  47421.3 Weight:  [ 22.12763786] bias:  [-151.26693726]\n",
      "step:  466 loss:  47039.6 Weight:  [ 22.04243279] bias:  [-150.65689087]\n",
      "step:  467 loss:  46661.0 Weight:  [ 21.95757103] bias:  [-150.04931641]\n",
      "step:  468 loss:  46285.4 Weight:  [ 21.87305069] bias:  [-149.44418335]\n",
      "step:  469 loss:  45912.8 Weight:  [ 21.78887177] bias:  [-148.8414917]\n",
      "step:  470 loss:  45543.2 Weight:  [ 21.70503235] bias:  [-148.2412262]\n",
      "step:  471 loss:  45176.6 Weight:  [ 21.62153244] bias:  [-147.64338684]\n",
      "step:  472 loss:  44813.0 Weight:  [ 21.53836823] bias:  [-147.04795837]\n",
      "step:  473 loss:  44452.3 Weight:  [ 21.4555397] bias:  [-146.45492554]\n",
      "step:  474 loss:  44094.4 Weight:  [ 21.37304306] bias:  [-145.86428833]\n",
      "step:  475 loss:  43739.5 Weight:  [ 21.2908802] bias:  [-145.27603149]\n",
      "step:  476 loss:  43387.4 Weight:  [ 21.20904922] bias:  [-144.69015503]\n",
      "step:  477 loss:  43038.2 Weight:  [ 21.12755013] bias:  [-144.10664368]\n",
      "step:  478 loss:  42691.7 Weight:  [ 21.04637909] bias:  [-143.52548218]\n",
      "step:  479 loss:  42348.1 Weight:  [ 20.96553421] bias:  [-142.94665527]\n",
      "step:  480 loss:  42007.2 Weight:  [ 20.88501549] bias:  [-142.37016296]\n",
      "step:  481 loss:  41669.1 Weight:  [ 20.80482101] bias:  [-141.79600525]\n",
      "step:  482 loss:  41333.6 Weight:  [ 20.72495079] bias:  [-141.22416687]\n",
      "step:  483 loss:  41000.9 Weight:  [ 20.64540291] bias:  [-140.65463257]\n",
      "step:  484 loss:  40670.9 Weight:  [ 20.56617546] bias:  [-140.08738708]\n",
      "step:  485 loss:  40343.5 Weight:  [ 20.48726845] bias:  [-139.52243042]\n",
      "step:  486 loss:  40018.8 Weight:  [ 20.40867805] bias:  [-138.95974731]\n",
      "step:  487 loss:  39696.6 Weight:  [ 20.33040428] bias:  [-138.39933777]\n",
      "step:  488 loss:  39377.1 Weight:  [ 20.25244713] bias:  [-137.84118652]\n",
      "step:  489 loss:  39060.1 Weight:  [ 20.17480469] bias:  [-137.28529358]\n",
      "step:  490 loss:  38745.7 Weight:  [ 20.09747505] bias:  [-136.73164368]\n",
      "step:  491 loss:  38433.8 Weight:  [ 20.02045822] bias:  [-136.18022156]\n",
      "step:  492 loss:  38124.5 Weight:  [ 19.94375038] bias:  [-135.63102722]\n",
      "step:  493 loss:  37817.6 Weight:  [ 19.86735344] bias:  [-135.08404541]\n",
      "step:  494 loss:  37513.2 Weight:  [ 19.79126358] bias:  [-134.53926086]\n",
      "step:  495 loss:  37211.2 Weight:  [ 19.7154808] bias:  [-133.99667358]\n",
      "step:  496 loss:  36911.7 Weight:  [ 19.6400013] bias:  [-133.45628357]\n",
      "step:  497 loss:  36614.6 Weight:  [ 19.56482887] bias:  [-132.91807556]\n",
      "step:  498 loss:  36319.8 Weight:  [ 19.48995972] bias:  [-132.3820343]\n",
      "step:  499 loss:  36027.5 Weight:  [ 19.41539192] bias:  [-131.84815979]\n",
      "step:  500 loss:  35737.5 Weight:  [ 19.34112549] bias:  [-131.31643677]\n",
      "step:  501 loss:  35449.8 Weight:  [ 19.26715851] bias:  [-130.78684998]\n",
      "step:  502 loss:  35164.5 Weight:  [ 19.19348907] bias:  [-130.25939941]\n",
      "step:  503 loss:  34881.4 Weight:  [ 19.12011719] bias:  [-129.73408508]\n",
      "step:  504 loss:  34600.6 Weight:  [ 19.04704094] bias:  [-129.21087646]\n",
      "step:  505 loss:  34322.1 Weight:  [ 18.97425842] bias:  [-128.68978882]\n",
      "step:  506 loss:  34045.8 Weight:  [ 18.90177155] bias:  [-128.17079163]\n",
      "step:  507 loss:  33771.8 Weight:  [ 18.82957458] bias:  [-127.65389252]\n",
      "step:  508 loss:  33499.9 Weight:  [ 18.75766945] bias:  [-127.13907623]\n",
      "step:  509 loss:  33230.3 Weight:  [ 18.68605423] bias:  [-126.62634277]\n",
      "step:  510 loss:  32962.8 Weight:  [ 18.61472893] bias:  [-126.11567688]\n",
      "step:  511 loss:  32697.5 Weight:  [ 18.54369164] bias:  [-125.60707092]\n",
      "step:  512 loss:  32434.3 Weight:  [ 18.47294044] bias:  [-125.10050964]\n",
      "step:  513 loss:  32173.2 Weight:  [ 18.40247345] bias:  [-124.59599304]\n",
      "step:  514 loss:  31914.2 Weight:  [ 18.33229256] bias:  [-124.09351349]\n",
      "step:  515 loss:  31657.3 Weight:  [ 18.26239395] bias:  [-123.59305573]\n",
      "step:  516 loss:  31402.5 Weight:  [ 18.19277573] bias:  [-123.09461975]\n",
      "step:  517 loss:  31149.7 Weight:  [ 18.12343979] bias:  [-122.59819794]\n",
      "step:  518 loss:  30899.0 Weight:  [ 18.05438232] bias:  [-122.10377502]\n",
      "step:  519 loss:  30650.2 Weight:  [ 17.98560524] bias:  [-121.61134338]\n",
      "step:  520 loss:  30403.5 Weight:  [ 17.91710472] bias:  [-121.12090302]\n",
      "step:  521 loss:  30158.8 Weight:  [ 17.84887886] bias:  [-120.63243866]\n",
      "step:  522 loss:  29916.0 Weight:  [ 17.78092957] bias:  [-120.14594269]\n",
      "step:  523 loss:  29675.2 Weight:  [ 17.71325493] bias:  [-119.66140747]\n",
      "step:  524 loss:  29436.3 Weight:  [ 17.64585304] bias:  [-119.17882538]\n",
      "step:  525 loss:  29199.4 Weight:  [ 17.578722] bias:  [-118.69818878]\n",
      "step:  526 loss:  28964.4 Weight:  [ 17.5118618] bias:  [-118.21949005]\n",
      "step:  527 loss:  28731.2 Weight:  [ 17.44527054] bias:  [-117.74272156]\n",
      "step:  528 loss:  28499.9 Weight:  [ 17.37894821] bias:  [-117.2678833]\n",
      "step:  529 loss:  28270.5 Weight:  [ 17.31289291] bias:  [-116.79495239]\n",
      "step:  530 loss:  28043.0 Weight:  [ 17.24710464] bias:  [-116.32392883]\n",
      "step:  531 loss:  27817.2 Weight:  [ 17.1815815] bias:  [-115.85480499]\n",
      "step:  532 loss:  27593.3 Weight:  [ 17.11632347] bias:  [-115.38757324]\n",
      "step:  533 loss:  27371.2 Weight:  [ 17.05132866] bias:  [-114.92222595]\n",
      "step:  534 loss:  27150.9 Weight:  [ 16.98659515] bias:  [-114.45875549]\n",
      "step:  535 loss:  26932.3 Weight:  [ 16.92212296] bias:  [-113.99715424]\n",
      "step:  536 loss:  26715.5 Weight:  [ 16.85791016] bias:  [-113.53741455]\n",
      "step:  537 loss:  26500.5 Weight:  [ 16.79395676] bias:  [-113.07952881]\n",
      "step:  538 loss:  26287.2 Weight:  [ 16.73026085] bias:  [-112.62348938]\n",
      "step:  539 loss:  26075.6 Weight:  [ 16.66682243] bias:  [-112.16929626]\n",
      "step:  540 loss:  25865.7 Weight:  [ 16.6036396] bias:  [-111.7169342]\n",
      "step:  541 loss:  25657.5 Weight:  [ 16.54071236] bias:  [-111.26639557]\n",
      "step:  542 loss:  25450.9 Weight:  [ 16.47803879] bias:  [-110.81767273]\n",
      "step:  543 loss:  25246.1 Weight:  [ 16.4156189] bias:  [-110.37075806]\n",
      "step:  544 loss:  25042.9 Weight:  [ 16.35344887] bias:  [-109.92564392]\n",
      "step:  545 loss:  24841.3 Weight:  [ 16.29153061] bias:  [-109.48233032]\n",
      "step:  546 loss:  24641.3 Weight:  [ 16.22986221] bias:  [-109.040802]\n",
      "step:  547 loss:  24443.0 Weight:  [ 16.16844177] bias:  [-108.60105133]\n",
      "step:  548 loss:  24246.2 Weight:  [ 16.10726929] bias:  [-108.16307831]\n",
      "step:  549 loss:  24051.0 Weight:  [ 16.04634285] bias:  [-107.72686768]\n",
      "step:  550 loss:  23857.4 Weight:  [ 15.98566246] bias:  [-107.29241943]\n",
      "step:  551 loss:  23665.4 Weight:  [ 15.92522717] bias:  [-106.85971832]\n",
      "step:  552 loss:  23474.9 Weight:  [ 15.86503506] bias:  [-106.42876434]\n",
      "step:  553 loss:  23285.9 Weight:  [ 15.80508614] bias:  [-105.99954987]\n",
      "step:  554 loss:  23098.5 Weight:  [ 15.74537945] bias:  [-105.57206726]\n",
      "step:  555 loss:  22912.6 Weight:  [ 15.68591309] bias:  [-105.1463089]\n",
      "step:  556 loss:  22728.1 Weight:  [ 15.6266861] bias:  [-104.72226715]\n",
      "step:  557 loss:  22545.2 Weight:  [ 15.56769943] bias:  [-104.29993439]\n",
      "step:  558 loss:  22363.7 Weight:  [ 15.50894928] bias:  [-103.87930298]\n",
      "step:  559 loss:  22183.7 Weight:  [ 15.45043659] bias:  [-103.46037292]\n",
      "step:  560 loss:  22005.1 Weight:  [ 15.39215946] bias:  [-103.04312897]\n",
      "step:  561 loss:  21828.0 Weight:  [ 15.33411694] bias:  [-102.62757111]\n",
      "step:  562 loss:  21652.3 Weight:  [ 15.27630997] bias:  [-102.21368408]\n",
      "step:  563 loss:  21478.0 Weight:  [ 15.21873569] bias:  [-101.8014679]\n",
      "step:  564 loss:  21305.1 Weight:  [ 15.16139221] bias:  [-101.39091492]\n",
      "step:  565 loss:  21133.6 Weight:  [ 15.10428143] bias:  [-100.98201752]\n",
      "step:  566 loss:  20963.5 Weight:  [ 15.04740047] bias:  [-100.57476807]\n",
      "step:  567 loss:  20794.8 Weight:  [ 14.99074936] bias:  [-100.16915894]\n",
      "step:  568 loss:  20627.4 Weight:  [ 14.93432522] bias:  [-99.76519012]\n",
      "step:  569 loss:  20461.3 Weight:  [ 14.87812996] bias:  [-99.36284637]\n",
      "step:  570 loss:  20296.6 Weight:  [ 14.82216072] bias:  [-98.96212769]\n",
      "step:  571 loss:  20133.2 Weight:  [ 14.7664175] bias:  [-98.56302643]\n",
      "step:  572 loss:  19971.2 Weight:  [ 14.71089935] bias:  [-98.16553497]\n",
      "step:  573 loss:  19810.4 Weight:  [ 14.65560532] bias:  [-97.76964569]\n",
      "step:  574 loss:  19651.0 Weight:  [ 14.60053444] bias:  [-97.37535095]\n",
      "step:  575 loss:  19492.8 Weight:  [ 14.54568481] bias:  [-96.98265076]\n",
      "step:  576 loss:  19335.9 Weight:  [ 14.49105644] bias:  [-96.59152985]\n",
      "step:  577 loss:  19180.2 Weight:  [ 14.43664932] bias:  [-96.20198822]\n",
      "step:  578 loss:  19025.8 Weight:  [ 14.38246059] bias:  [-95.81401825]\n",
      "step:  579 loss:  18872.7 Weight:  [ 14.32849121] bias:  [-95.4276123]\n",
      "step:  580 loss:  18720.8 Weight:  [ 14.27473831] bias:  [-95.04276276]\n",
      "step:  581 loss:  18570.1 Weight:  [ 14.22120285] bias:  [-94.6594696]\n",
      "step:  582 loss:  18420.6 Weight:  [ 14.16788387] bias:  [-94.27771759]\n",
      "step:  583 loss:  18272.3 Weight:  [ 14.11477947] bias:  [-93.89750671]\n",
      "step:  584 loss:  18125.2 Weight:  [ 14.06188869] bias:  [-93.51882935]\n",
      "step:  585 loss:  17979.3 Weight:  [ 14.00921154] bias:  [-93.14167786]\n",
      "step:  586 loss:  17834.6 Weight:  [ 13.95674706] bias:  [-92.76605225]\n",
      "step:  587 loss:  17691.1 Weight:  [ 13.90449429] bias:  [-92.39193726]\n",
      "step:  588 loss:  17548.7 Weight:  [ 13.85245228] bias:  [-92.01933289]\n",
      "step:  589 loss:  17407.4 Weight:  [ 13.80062008] bias:  [-91.64823151]\n",
      "step:  590 loss:  17267.3 Weight:  [ 13.74899673] bias:  [-91.27862549]\n",
      "step:  591 loss:  17128.3 Weight:  [ 13.69758224] bias:  [-90.9105072]\n",
      "step:  592 loss:  16990.4 Weight:  [ 13.64637375] bias:  [-90.54387665]\n",
      "step:  593 loss:  16853.7 Weight:  [ 13.5953722] bias:  [-90.1787262]\n",
      "step:  594 loss:  16718.0 Weight:  [ 13.54457664] bias:  [-89.81504822]\n",
      "step:  595 loss:  16583.4 Weight:  [ 13.49398613] bias:  [-89.45283508]\n",
      "step:  596 loss:  16449.9 Weight:  [ 13.4435997] bias:  [-89.09207916]\n",
      "step:  597 loss:  16317.5 Weight:  [ 13.39341545] bias:  [-88.73278046]\n",
      "step:  598 loss:  16186.2 Weight:  [ 13.34343433] bias:  [-88.37493134]\n",
      "step:  599 loss:  16055.9 Weight:  [ 13.29365444] bias:  [-88.01852417]\n",
      "step:  600 loss:  15926.6 Weight:  [ 13.24407578] bias:  [-87.66355896]\n",
      "step:  601 loss:  15798.4 Weight:  [ 13.19469738] bias:  [-87.31002045]\n",
      "step:  602 loss:  15671.3 Weight:  [ 13.14551735] bias:  [-86.95790863]\n",
      "step:  603 loss:  15545.1 Weight:  [ 13.09653568] bias:  [-86.60721588]\n",
      "step:  604 loss:  15420.0 Weight:  [ 13.04775143] bias:  [-86.2579422]\n",
      "step:  605 loss:  15295.9 Weight:  [ 12.99916458] bias:  [-85.91007233]\n",
      "step:  606 loss:  15172.7 Weight:  [ 12.95077324] bias:  [-85.56360626]\n",
      "step:  607 loss:  15050.6 Weight:  [ 12.9025774] bias:  [-85.21853638]\n",
      "step:  608 loss:  14929.5 Weight:  [ 12.85457516] bias:  [-84.87486267]\n",
      "step:  609 loss:  14809.3 Weight:  [ 12.80676746] bias:  [-84.53256989]\n",
      "step:  610 loss:  14690.1 Weight:  [ 12.75915146] bias:  [-84.19165802]\n",
      "step:  611 loss:  14571.8 Weight:  [ 12.7117281] bias:  [-83.85212708]\n",
      "step:  612 loss:  14454.5 Weight:  [ 12.66449642] bias:  [-83.51396179]\n",
      "step:  613 loss:  14338.2 Weight:  [ 12.61745548] bias:  [-83.17716217]\n",
      "step:  614 loss:  14222.8 Weight:  [ 12.57060337] bias:  [-82.84172058]\n",
      "step:  615 loss:  14108.3 Weight:  [ 12.52394104] bias:  [-82.50762939]\n",
      "step:  616 loss:  13994.7 Weight:  [ 12.47746658] bias:  [-82.17488861]\n",
      "step:  617 loss:  13882.1 Weight:  [ 12.43118] bias:  [-81.8434906]\n",
      "step:  618 loss:  13770.3 Weight:  [ 12.38507938] bias:  [-81.51342773]\n",
      "step:  619 loss:  13659.5 Weight:  [ 12.33916473] bias:  [-81.18469238]\n",
      "step:  620 loss:  13549.5 Weight:  [ 12.2934351] bias:  [-80.85728455]\n",
      "step:  621 loss:  13440.5 Weight:  [ 12.24789047] bias:  [-80.53119659]\n",
      "step:  622 loss:  13332.3 Weight:  [ 12.20252895] bias:  [-80.2064209]\n",
      "step:  623 loss:  13225.0 Weight:  [ 12.15735054] bias:  [-79.88295746]\n",
      "step:  624 loss:  13118.5 Weight:  [ 12.11235428] bias:  [-79.56079865]\n",
      "step:  625 loss:  13012.9 Weight:  [ 12.06753922] bias:  [-79.23993683]\n",
      "step:  626 loss:  12908.1 Weight:  [ 12.0229044] bias:  [-78.92037201]\n",
      "step:  627 loss:  12804.2 Weight:  [ 11.97845078] bias:  [-78.60209656]\n",
      "step:  628 loss:  12701.2 Weight:  [ 11.93417549] bias:  [-78.28510284]\n",
      "step:  629 loss:  12598.9 Weight:  [ 11.8900795] bias:  [-77.96939087]\n",
      "step:  630 loss:  12497.5 Weight:  [ 11.84616184] bias:  [-77.654953]\n",
      "step:  631 loss:  12396.9 Weight:  [ 11.80242062] bias:  [-77.34178162]\n",
      "step:  632 loss:  12297.1 Weight:  [ 11.75885582] bias:  [-77.02986908]\n",
      "step:  633 loss:  12198.2 Weight:  [ 11.7154665] bias:  [-76.71921539]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step:  634 loss:  12100.0 Weight:  [ 11.6722517] bias:  [-76.40981293]\n",
      "step:  635 loss:  12002.6 Weight:  [ 11.62921143] bias:  [-76.10166168]\n",
      "step:  636 loss:  11906.0 Weight:  [ 11.58634567] bias:  [-75.79475403]\n",
      "step:  637 loss:  11810.1 Weight:  [ 11.54365253] bias:  [-75.48908234]\n",
      "step:  638 loss:  11715.0 Weight:  [ 11.50113106] bias:  [-75.18464661]\n",
      "step:  639 loss:  11620.7 Weight:  [ 11.45878124] bias:  [-74.88143921]\n",
      "step:  640 loss:  11527.2 Weight:  [ 11.41660309] bias:  [-74.57945251]\n",
      "step:  641 loss:  11434.4 Weight:  [ 11.37459373] bias:  [-74.27867889]\n",
      "step:  642 loss:  11342.4 Weight:  [ 11.33275414] bias:  [-73.97911835]\n",
      "step:  643 loss:  11251.1 Weight:  [ 11.29108334] bias:  [-73.68077087]\n",
      "step:  644 loss:  11160.5 Weight:  [ 11.24958038] bias:  [-73.38362885]\n",
      "step:  645 loss:  11070.7 Weight:  [ 11.20824528] bias:  [-73.08768463]\n",
      "step:  646 loss:  10981.6 Weight:  [ 11.16707706] bias:  [-72.7929306]\n",
      "step:  647 loss:  10893.2 Weight:  [ 11.12607479] bias:  [-72.49936676]\n",
      "step:  648 loss:  10805.5 Weight:  [ 11.0852375] bias:  [-72.20698547]\n",
      "step:  649 loss:  10718.5 Weight:  [ 11.0445652] bias:  [-71.91578674]\n",
      "step:  650 loss:  10632.2 Weight:  [ 11.00405693] bias:  [-71.62576294]\n",
      "step:  651 loss:  10546.6 Weight:  [ 10.96371174] bias:  [-71.33690643]\n",
      "step:  652 loss:  10461.8 Weight:  [ 10.92352962] bias:  [-71.04920959]\n",
      "step:  653 loss:  10377.5 Weight:  [ 10.88350964] bias:  [-70.76268005]\n",
      "step:  654 loss:  10294.0 Weight:  [ 10.84365082] bias:  [-70.47730255]\n",
      "step:  655 loss:  10211.1 Weight:  [ 10.80395222] bias:  [-70.19307709]\n",
      "step:  656 loss:  10128.9 Weight:  [ 10.76441479] bias:  [-69.90999603]\n",
      "step:  657 loss:  10047.4 Weight:  [ 10.72503567] bias:  [-69.62805939]\n",
      "step:  658 loss:  9966.54 Weight:  [ 10.68581581] bias:  [-69.34725952]\n",
      "step:  659 loss:  9886.31 Weight:  [ 10.64675426] bias:  [-69.06758881]\n",
      "step:  660 loss:  9806.73 Weight:  [ 10.60785007] bias:  [-68.78904724]\n",
      "step:  661 loss:  9727.79 Weight:  [ 10.56910229] bias:  [-68.5116272]\n",
      "step:  662 loss:  9649.49 Weight:  [ 10.5305109] bias:  [-68.23532867]\n",
      "step:  663 loss:  9571.82 Weight:  [ 10.49207592] bias:  [-67.96014404]\n",
      "step:  664 loss:  9494.77 Weight:  [ 10.45379543] bias:  [-67.68606567]\n",
      "step:  665 loss:  9418.34 Weight:  [ 10.41566944] bias:  [-67.41309357]\n",
      "step:  666 loss:  9342.53 Weight:  [ 10.37769699] bias:  [-67.14122772]\n",
      "step:  667 loss:  9267.32 Weight:  [ 10.33987808] bias:  [-66.87045288]\n",
      "step:  668 loss:  9192.73 Weight:  [ 10.30221081] bias:  [-66.60076904]\n",
      "step:  669 loss:  9118.73 Weight:  [ 10.26469612] bias:  [-66.33217621]\n",
      "step:  670 loss:  9045.33 Weight:  [ 10.22733307] bias:  [-66.06466675]\n",
      "step:  671 loss:  8972.52 Weight:  [ 10.19011974] bias:  [-65.79823303]\n",
      "step:  672 loss:  8900.29 Weight:  [ 10.1530571] bias:  [-65.53287506]\n",
      "step:  673 loss:  8828.65 Weight:  [ 10.11614323] bias:  [-65.26858521]\n",
      "step:  674 loss:  8757.58 Weight:  [ 10.07937908] bias:  [-65.00536346]\n",
      "step:  675 loss:  8687.09 Weight:  [ 10.04276276] bias:  [-64.74320221]\n",
      "step:  676 loss:  8617.16 Weight:  [ 10.00629425] bias:  [-64.48210144]\n",
      "step:  677 loss:  8547.8 Weight:  [ 9.96997261] bias:  [-64.22205353]\n",
      "step:  678 loss:  8478.99 Weight:  [ 9.93379784] bias:  [-63.96305466]\n",
      "step:  679 loss:  8410.74 Weight:  [ 9.89776897] bias:  [-63.70510101]\n",
      "step:  680 loss:  8343.04 Weight:  [ 9.86188507] bias:  [-63.44818497]\n",
      "step:  681 loss:  8275.88 Weight:  [ 9.82614613] bias:  [-63.19230652]\n",
      "step:  682 loss:  8209.26 Weight:  [ 9.79055119] bias:  [-62.93745804]\n",
      "step:  683 loss:  8143.18 Weight:  [ 9.75510025] bias:  [-62.68363953]\n",
      "step:  684 loss:  8077.63 Weight:  [ 9.71979237] bias:  [-62.43084335]\n",
      "step:  685 loss:  8012.61 Weight:  [ 9.68462658] bias:  [-62.1790657]\n",
      "step:  686 loss:  7948.12 Weight:  [ 9.64960194] bias:  [-61.92830658]\n",
      "step:  687 loss:  7884.14 Weight:  [ 9.61471939] bias:  [-61.67855835]\n",
      "step:  688 loss:  7820.67 Weight:  [ 9.57997704] bias:  [-61.4298172]\n",
      "step:  689 loss:  7757.72 Weight:  [ 9.54537487] bias:  [-61.18207932]\n",
      "step:  690 loss:  7695.28 Weight:  [ 9.5109129] bias:  [-60.93534088]\n",
      "step:  691 loss:  7633.33 Weight:  [ 9.47659016] bias:  [-60.68959427]\n",
      "step:  692 loss:  7571.89 Weight:  [ 9.44240475] bias:  [-60.44483948]\n",
      "step:  693 loss:  7510.94 Weight:  [ 9.40835762] bias:  [-60.20107269]\n",
      "step:  694 loss:  7450.48 Weight:  [ 9.37444782] bias:  [-59.9582901]\n",
      "step:  695 loss:  7390.51 Weight:  [ 9.3406744] bias:  [-59.71648407]\n",
      "step:  696 loss:  7331.02 Weight:  [ 9.30703735] bias:  [-59.4756546]\n",
      "step:  697 loss:  7272.01 Weight:  [ 9.27353573] bias:  [-59.23579788]\n",
      "step:  698 loss:  7213.47 Weight:  [ 9.24016953] bias:  [-58.99690628]\n",
      "step:  699 loss:  7155.41 Weight:  [ 9.20693779] bias:  [-58.7589798]\n",
      "step:  700 loss:  7097.81 Weight:  [ 9.17384052] bias:  [-58.5220108]\n",
      "step:  701 loss:  7040.67 Weight:  [ 9.14087677] bias:  [-58.2859993]\n",
      "step:  702 loss:  6984.0 Weight:  [ 9.10804558] bias:  [-58.05093765]\n",
      "step:  703 loss:  6927.78 Weight:  [ 9.07534599] bias:  [-57.81682587]\n",
      "step:  704 loss:  6872.02 Weight:  [ 9.04277897] bias:  [-57.58365631]\n",
      "step:  705 loss:  6816.7 Weight:  [ 9.01034355] bias:  [-57.35142899]\n",
      "step:  706 loss:  6761.83 Weight:  [ 8.97803879] bias:  [-57.12013626]\n",
      "step:  707 loss:  6707.4 Weight:  [ 8.94586468] bias:  [-56.88977814]\n",
      "step:  708 loss:  6653.41 Weight:  [ 8.91382027] bias:  [-56.66034698]\n",
      "step:  709 loss:  6599.85 Weight:  [ 8.8819046] bias:  [-56.4318428]\n",
      "step:  710 loss:  6546.73 Weight:  [ 8.85011768] bias:  [-56.20426178]\n",
      "step:  711 loss:  6494.03 Weight:  [ 8.81845951] bias:  [-55.97759628]\n",
      "step:  712 loss:  6441.76 Weight:  [ 8.78692818] bias:  [-55.75184631]\n",
      "step:  713 loss:  6389.9 Weight:  [ 8.75552464] bias:  [-55.52700424]\n",
      "step:  714 loss:  6338.47 Weight:  [ 8.72424698] bias:  [-55.30307007]\n",
      "step:  715 loss:  6287.45 Weight:  [ 8.69309616] bias:  [-55.08003998]\n",
      "step:  716 loss:  6236.84 Weight:  [ 8.66207123] bias:  [-54.85791016]\n",
      "step:  717 loss:  6186.63 Weight:  [ 8.63117123] bias:  [-54.63667297]\n",
      "step:  718 loss:  6136.83 Weight:  [ 8.6003952] bias:  [-54.41632843]\n",
      "step:  719 loss:  6087.43 Weight:  [ 8.56974316] bias:  [-54.19687271]\n",
      "step:  720 loss:  6038.43 Weight:  [ 8.53921509] bias:  [-53.978302]\n",
      "step:  721 loss:  5989.83 Weight:  [ 8.50881004] bias:  [-53.76061249]\n",
      "step:  722 loss:  5941.61 Weight:  [ 8.47852802] bias:  [-53.54380417]\n",
      "step:  723 loss:  5893.78 Weight:  [ 8.44836807] bias:  [-53.32786942]\n",
      "step:  724 loss:  5846.34 Weight:  [ 8.41833019] bias:  [-53.11280441]\n",
      "step:  725 loss:  5799.28 Weight:  [ 8.38841343] bias:  [-52.89860535]\n",
      "step:  726 loss:  5752.6 Weight:  [ 8.35861588] bias:  [-52.68527222]\n",
      "step:  727 loss:  5706.3 Weight:  [ 8.32893944] bias:  [-52.47279739]\n",
      "step:  728 loss:  5660.36 Weight:  [ 8.29938316] bias:  [-52.26118088]\n",
      "step:  729 loss:  5614.8 Weight:  [ 8.26994514] bias:  [-52.05041885]\n",
      "step:  730 loss:  5569.6 Weight:  [ 8.24062634] bias:  [-51.84050751]\n",
      "step:  731 loss:  5524.77 Weight:  [ 8.21142578] bias:  [-51.63143921]\n",
      "step:  732 loss:  5480.3 Weight:  [ 8.18234348] bias:  [-51.42321777]\n",
      "step:  733 loss:  5436.19 Weight:  [ 8.15337753] bias:  [-51.21583557]\n",
      "step:  734 loss:  5392.43 Weight:  [ 8.12452888] bias:  [-51.00928879]\n",
      "step:  735 loss:  5349.02 Weight:  [ 8.09579754] bias:  [-50.80357361]\n",
      "step:  736 loss:  5305.96 Weight:  [ 8.06718063] bias:  [-50.59869003]\n",
      "step:  737 loss:  5263.25 Weight:  [ 8.03867912] bias:  [-50.39463043]\n",
      "step:  738 loss:  5220.89 Weight:  [ 8.01029301] bias:  [-50.19139481]\n",
      "step:  739 loss:  5178.86 Weight:  [ 7.98202133] bias:  [-49.98897934]\n",
      "step:  740 loss:  5137.17 Weight:  [ 7.95386362] bias:  [-49.78738022]\n",
      "step:  741 loss:  5095.82 Weight:  [ 7.92581987] bias:  [-49.58659363]\n",
      "step:  742 loss:  5054.8 Weight:  [ 7.89788866] bias:  [-49.38661575]\n",
      "step:  743 loss:  5014.12 Weight:  [ 7.87007046] bias:  [-49.18744659]\n",
      "step:  744 loss:  4973.75 Weight:  [ 7.84236431] bias:  [-48.98907852]\n",
      "step:  745 loss:  4933.72 Weight:  [ 7.81476974] bias:  [-48.79151154]\n",
      "step:  746 loss:  4894.0 Weight:  [ 7.78728676] bias:  [-48.59474182]\n",
      "step:  747 loss:  4854.61 Weight:  [ 7.7599144] bias:  [-48.39876556]\n",
      "step:  748 loss:  4815.53 Weight:  [ 7.73265266] bias:  [-48.20357895]\n",
      "step:  749 loss:  4776.77 Weight:  [ 7.7055006] bias:  [-48.00917816]\n",
      "step:  750 loss:  4738.32 Weight:  [ 7.67845821] bias:  [-47.8155632]\n",
      "step:  751 loss:  4700.18 Weight:  [ 7.65152454] bias:  [-47.62273026]\n",
      "step:  752 loss:  4662.34 Weight:  [ 7.62470007] bias:  [-47.43067169]\n",
      "step:  753 loss:  4624.81 Weight:  [ 7.59798336] bias:  [-47.23939133]\n",
      "step:  754 loss:  4587.59 Weight:  [ 7.57137442] bias:  [-47.04888153]\n",
      "step:  755 loss:  4550.66 Weight:  [ 7.54487276] bias:  [-46.85913849]\n",
      "step:  756 loss:  4514.03 Weight:  [ 7.51847792] bias:  [-46.6701622]\n",
      "step:  757 loss:  4477.69 Weight:  [ 7.49219036] bias:  [-46.48194885]\n",
      "step:  758 loss:  4441.65 Weight:  [ 7.46600819] bias:  [-46.29449463]\n",
      "step:  759 loss:  4405.9 Weight:  [ 7.43993187] bias:  [-46.10779572]\n",
      "step:  760 loss:  4370.43 Weight:  [ 7.41396046] bias:  [-45.9218483]\n",
      "step:  761 loss:  4335.25 Weight:  [ 7.38809347] bias:  [-45.73665237]\n",
      "step:  762 loss:  4300.36 Weight:  [ 7.36233139] bias:  [-45.55220032]\n",
      "step:  763 loss:  4265.74 Weight:  [ 7.33667278] bias:  [-45.36849213]\n",
      "step:  764 loss:  4231.4 Weight:  [ 7.31111765] bias:  [-45.1855278]\n",
      "step:  765 loss:  4197.34 Weight:  [ 7.28566599] bias:  [-45.00329971]\n",
      "step:  766 loss:  4163.56 Weight:  [ 7.26031637] bias:  [-44.82180786]\n",
      "step:  767 loss:  4130.04 Weight:  [ 7.23506927] bias:  [-44.64104843]\n",
      "step:  768 loss:  4096.8 Weight:  [ 7.20992422] bias:  [-44.46101761]\n",
      "step:  769 loss:  4063.82 Weight:  [ 7.18488026] bias:  [-44.28171158]\n",
      "step:  770 loss:  4031.11 Weight:  [ 7.15993738] bias:  [-44.10313034]\n",
      "step:  771 loss:  3998.66 Weight:  [ 7.1350956] bias:  [-43.92526627]\n",
      "step:  772 loss:  3966.47 Weight:  [ 7.11035347] bias:  [-43.74811935]\n",
      "step:  773 loss:  3934.54 Weight:  [ 7.085711] bias:  [-43.57168961]\n",
      "step:  774 loss:  3902.87 Weight:  [ 7.06116819] bias:  [-43.39596939]\n",
      "step:  775 loss:  3871.46 Weight:  [ 7.03672409] bias:  [-43.22095871]\n",
      "step:  776 loss:  3840.29 Weight:  [ 7.01237869] bias:  [-43.04665375]\n",
      "step:  777 loss:  3809.38 Weight:  [ 6.98813152] bias:  [-42.87305069]\n",
      "step:  778 loss:  3778.72 Weight:  [ 6.96398163] bias:  [-42.70014954]\n",
      "step:  779 loss:  3748.3 Weight:  [ 6.93992996] bias:  [-42.52794647]\n",
      "step:  780 loss:  3718.13 Weight:  [ 6.91597509] bias:  [-42.35643768]\n",
      "step:  781 loss:  3688.2 Weight:  [ 6.89211655] bias:  [-42.18561935]\n",
      "step:  782 loss:  3658.51 Weight:  [ 6.86835432] bias:  [-42.01549149]\n",
      "step:  783 loss:  3629.06 Weight:  [ 6.84468842] bias:  [-41.84604645]\n",
      "step:  784 loss:  3599.85 Weight:  [ 6.8211174] bias:  [-41.67728806]\n",
      "step:  785 loss:  3570.88 Weight:  [ 6.79764175] bias:  [-41.50920868]\n",
      "step:  786 loss:  3542.13 Weight:  [ 6.77426052] bias:  [-41.34180832]\n",
      "step:  787 loss:  3513.62 Weight:  [ 6.7509737] bias:  [-41.17508316]\n",
      "step:  788 loss:  3485.34 Weight:  [ 6.72778082] bias:  [-41.00902939]\n",
      "step:  789 loss:  3457.28 Weight:  [ 6.7046814] bias:  [-40.84364319]\n",
      "step:  790 loss:  3429.45 Weight:  [ 6.68167496] bias:  [-40.67892456]\n",
      "step:  791 loss:  3401.85 Weight:  [ 6.6587615] bias:  [-40.51486969]\n",
      "step:  792 loss:  3374.46 Weight:  [ 6.63594007] bias:  [-40.35147858]\n",
      "step:  793 loss:  3347.3 Weight:  [ 6.61321115] bias:  [-40.18874741]\n",
      "step:  794 loss:  3320.36 Weight:  [ 6.59057379] bias:  [-40.02667236]\n",
      "step:  795 loss:  3293.63 Weight:  [ 6.56802797] bias:  [-39.86524963]\n",
      "step:  796 loss:  3267.12 Weight:  [ 6.54557276] bias:  [-39.70447922]\n",
      "step:  797 loss:  3240.82 Weight:  [ 6.52320814] bias:  [-39.5443573]\n",
      "step:  798 loss:  3214.73 Weight:  [ 6.50093412] bias:  [-39.38488007]\n",
      "step:  799 loss:  3188.86 Weight:  [ 6.47874975] bias:  [-39.22604752]\n",
      "step:  800 loss:  3163.19 Weight:  [ 6.45665455] bias:  [-39.06785202]\n",
      "step:  801 loss:  3137.72 Weight:  [ 6.43464851] bias:  [-38.91029739]\n",
      "step:  802 loss:  3112.47 Weight:  [ 6.41273117] bias:  [-38.75337601]\n",
      "step:  803 loss:  3087.41 Weight:  [ 6.39090252] bias:  [-38.59708786]\n",
      "step:  804 loss:  3062.56 Weight:  [ 6.36916161] bias:  [-38.44142914]\n",
      "step:  805 loss:  3037.91 Weight:  [ 6.34750795] bias:  [-38.28639984]\n",
      "step:  806 loss:  3013.46 Weight:  [ 6.32594204] bias:  [-38.13199615]\n",
      "step:  807 loss:  2989.2 Weight:  [ 6.30446339] bias:  [-37.97821426]\n",
      "step:  808 loss:  2965.14 Weight:  [ 6.28307104] bias:  [-37.82505417]\n",
      "step:  809 loss:  2941.27 Weight:  [ 6.26176548] bias:  [-37.67251205]\n",
      "step:  810 loss:  2917.59 Weight:  [ 6.24054527] bias:  [-37.52058411]\n",
      "step:  811 loss:  2894.11 Weight:  [ 6.2194109] bias:  [-37.36926651]\n",
      "step:  812 loss:  2870.81 Weight:  [ 6.19836187] bias:  [-37.21855927]\n",
      "step:  813 loss:  2847.7 Weight:  [ 6.17739725] bias:  [-37.06846237]\n",
      "step:  814 loss:  2824.78 Weight:  [ 6.15651751] bias:  [-36.9189682]\n",
      "step:  815 loss:  2802.04 Weight:  [ 6.13572168] bias:  [-36.77007675]\n",
      "step:  816 loss:  2779.49 Weight:  [ 6.11500978] bias:  [-36.62178802]\n",
      "step:  817 loss:  2757.11 Weight:  [ 6.09438181] bias:  [-36.47409821]\n",
      "step:  818 loss:  2734.92 Weight:  [ 6.0738368] bias:  [-36.32700348]\n",
      "step:  819 loss:  2712.91 Weight:  [ 6.05337477] bias:  [-36.18050003]\n",
      "step:  820 loss:  2691.07 Weight:  [ 6.03299475] bias:  [-36.03458786]\n",
      "step:  821 loss:  2669.41 Weight:  [ 6.01269722] bias:  [-35.88926315]\n",
      "step:  822 loss:  2647.92 Weight:  [ 5.99248171] bias:  [-35.74452591]\n",
      "step:  823 loss:  2626.61 Weight:  [ 5.97234726] bias:  [-35.60037231]\n",
      "step:  824 loss:  2605.46 Weight:  [ 5.95229435] bias:  [-35.45679855]\n",
      "step:  825 loss:  2584.49 Weight:  [ 5.9323225] bias:  [-35.31380463]\n",
      "step:  826 loss:  2563.69 Weight:  [ 5.91243076] bias:  [-35.17138672]\n",
      "step:  827 loss:  2543.05 Weight:  [ 5.89261913] bias:  [-35.02954483]\n",
      "step:  828 loss:  2522.58 Weight:  [ 5.87288761] bias:  [-34.88827515]\n",
      "step:  829 loss:  2502.27 Weight:  [ 5.8532362] bias:  [-34.74757385]\n",
      "step:  830 loss:  2482.13 Weight:  [ 5.83366346] bias:  [-34.60744095]\n",
      "step:  831 loss:  2462.15 Weight:  [ 5.81416988] bias:  [-34.46787262]\n",
      "step:  832 loss:  2442.33 Weight:  [ 5.79475498] bias:  [-34.32886887]\n",
      "step:  833 loss:  2422.67 Weight:  [ 5.77541828] bias:  [-34.19042587]\n",
      "step:  834 loss:  2403.17 Weight:  [ 5.75615978] bias:  [-34.05253983]\n",
      "step:  835 loss:  2383.83 Weight:  [ 5.73697853] bias:  [-33.91521072]\n",
      "step:  836 loss:  2364.64 Weight:  [ 5.717875] bias:  [-33.77843475]\n",
      "step:  837 loss:  2345.6 Weight:  [ 5.69884825] bias:  [-33.64221191]\n",
      "step:  838 loss:  2326.72 Weight:  [ 5.67989874] bias:  [-33.50653839]\n",
      "step:  839 loss:  2308.0 Weight:  [ 5.66102552] bias:  [-33.37141037]\n",
      "step:  840 loss:  2289.42 Weight:  [ 5.64222813] bias:  [-33.23682785]\n",
      "step:  841 loss:  2270.99 Weight:  [ 5.62350655] bias:  [-33.10278702]\n",
      "step:  842 loss:  2252.71 Weight:  [ 5.60486031] bias:  [-32.96928787]\n",
      "step:  843 loss:  2234.57 Weight:  [ 5.58628988] bias:  [-32.8363266]\n",
      "step:  844 loss:  2216.59 Weight:  [ 5.56779385] bias:  [-32.7039032]\n",
      "step:  845 loss:  2198.75 Weight:  [ 5.54937267] bias:  [-32.57201385]\n",
      "step:  846 loss:  2181.05 Weight:  [ 5.53102589] bias:  [-32.44065475]\n",
      "step:  847 loss:  2163.49 Weight:  [ 5.51275253] bias:  [-32.3098259]\n",
      "step:  848 loss:  2146.08 Weight:  [ 5.49455309] bias:  [-32.17952347]\n",
      "step:  849 loss:  2128.8 Weight:  [ 5.47642708] bias:  [-32.04974747]\n",
      "step:  850 loss:  2111.67 Weight:  [ 5.4583745] bias:  [-31.92049408]\n",
      "step:  851 loss:  2094.67 Weight:  [ 5.4403944] bias:  [-31.79176331]\n",
      "step:  852 loss:  2077.81 Weight:  [ 5.42248678] bias:  [-31.66355133]\n",
      "step:  853 loss:  2061.08 Weight:  [ 5.40465164] bias:  [-31.53585625]\n",
      "step:  854 loss:  2044.49 Weight:  [ 5.38688803] bias:  [-31.40867615]\n",
      "step:  855 loss:  2028.03 Weight:  [ 5.36919641] bias:  [-31.28200912]\n",
      "step:  856 loss:  2011.71 Weight:  [ 5.35157585] bias:  [-31.15585327]\n",
      "step:  857 loss:  1995.52 Weight:  [ 5.33402634] bias:  [-31.03020477]\n",
      "step:  858 loss:  1979.45 Weight:  [ 5.31654787] bias:  [-30.90506363]\n",
      "step:  859 loss:  1963.52 Weight:  [ 5.29913998] bias:  [-30.78042793]\n",
      "step:  860 loss:  1947.71 Weight:  [ 5.2818017] bias:  [-30.65629387]\n",
      "step:  861 loss:  1932.04 Weight:  [ 5.264534] bias:  [-30.53266144]\n",
      "step:  862 loss:  1916.48 Weight:  [ 5.24733543] bias:  [-30.40952682]\n",
      "step:  863 loss:  1901.06 Weight:  [ 5.23020649] bias:  [-30.28688812]\n",
      "step:  864 loss:  1885.75 Weight:  [ 5.21314669] bias:  [-30.16474533]\n",
      "step:  865 loss:  1870.57 Weight:  [ 5.19615555] bias:  [-30.04309464]\n",
      "step:  866 loss:  1855.52 Weight:  [ 5.17923307] bias:  [-29.92193413]\n",
      "step:  867 loss:  1840.58 Weight:  [ 5.16237879] bias:  [-29.8012619]\n",
      "step:  868 loss:  1825.77 Weight:  [ 5.14559221] bias:  [-29.68107796]\n",
      "step:  869 loss:  1811.07 Weight:  [ 5.12887383] bias:  [-29.56137848]\n",
      "step:  870 loss:  1796.49 Weight:  [ 5.11222267] bias:  [-29.44216156]\n",
      "step:  871 loss:  1782.03 Weight:  [ 5.09563875] bias:  [-29.32342529]\n",
      "step:  872 loss:  1767.69 Weight:  [ 5.07912159] bias:  [-29.20516777]\n",
      "step:  873 loss:  1753.46 Weight:  [ 5.06267071] bias:  [-29.08738708]\n",
      "step:  874 loss:  1739.34 Weight:  [ 5.04628658] bias:  [-28.97008133]\n",
      "step:  875 loss:  1725.34 Weight:  [ 5.02996826] bias:  [-28.8532486]\n",
      "step:  876 loss:  1711.45 Weight:  [ 5.01371574] bias:  [-28.73688698]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step:  877 loss:  1697.68 Weight:  [ 4.99752903] bias:  [-28.62099457]\n",
      "step:  878 loss:  1684.01 Weight:  [ 4.98140764] bias:  [-28.50556946]\n",
      "step:  879 loss:  1670.46 Weight:  [ 4.9653511] bias:  [-28.39060974]\n",
      "step:  880 loss:  1657.01 Weight:  [ 4.94935942] bias:  [-28.27611351]\n",
      "step:  881 loss:  1643.67 Weight:  [ 4.9334321] bias:  [-28.16207886]\n",
      "step:  882 loss:  1630.44 Weight:  [ 4.91756868] bias:  [-28.04850388]\n",
      "step:  883 loss:  1617.32 Weight:  [ 4.90176964] bias:  [-27.93538857]\n",
      "step:  884 loss:  1604.3 Weight:  [ 4.88603449] bias:  [-27.82272911]\n",
      "step:  885 loss:  1591.38 Weight:  [ 4.87036276] bias:  [-27.71052361]\n",
      "step:  886 loss:  1578.58 Weight:  [ 4.85475397] bias:  [-27.59877014]\n",
      "step:  887 loss:  1565.87 Weight:  [ 4.83920813] bias:  [-27.48746681]\n",
      "step:  888 loss:  1553.26 Weight:  [ 4.82372475] bias:  [-27.37661362]\n",
      "step:  889 loss:  1540.76 Weight:  [ 4.80830431] bias:  [-27.26620674]\n",
      "step:  890 loss:  1528.36 Weight:  [ 4.79294586] bias:  [-27.15624619]\n",
      "step:  891 loss:  1516.06 Weight:  [ 4.7776494] bias:  [-27.04672813]\n",
      "step:  892 loss:  1503.85 Weight:  [ 4.76241446] bias:  [-26.93765259]\n",
      "step:  893 loss:  1491.75 Weight:  [ 4.7472415] bias:  [-26.82901573]\n",
      "step:  894 loss:  1479.74 Weight:  [ 4.7321291] bias:  [-26.72081757]\n",
      "step:  895 loss:  1467.83 Weight:  [ 4.71707773] bias:  [-26.61305618]\n",
      "step:  896 loss:  1456.01 Weight:  [ 4.7020874] bias:  [-26.50572968]\n",
      "step:  897 loss:  1444.29 Weight:  [ 4.68715763] bias:  [-26.39883614]\n",
      "step:  898 loss:  1432.67 Weight:  [ 4.67228794] bias:  [-26.29237366]\n",
      "step:  899 loss:  1421.14 Weight:  [ 4.65747786] bias:  [-26.18634033]\n",
      "step:  900 loss:  1409.7 Weight:  [ 4.64272785] bias:  [-26.08073425]\n",
      "step:  901 loss:  1398.35 Weight:  [ 4.62803745] bias:  [-25.97555351]\n",
      "step:  902 loss:  1387.09 Weight:  [ 4.6134057] bias:  [-25.87079811]\n",
      "step:  903 loss:  1375.93 Weight:  [ 4.59883356] bias:  [-25.76646423]\n",
      "step:  904 loss:  1364.85 Weight:  [ 4.58431959] bias:  [-25.66255188]\n",
      "step:  905 loss:  1353.87 Weight:  [ 4.56986475] bias:  [-25.55905724]\n",
      "step:  906 loss:  1342.97 Weight:  [ 4.55546761] bias:  [-25.4559803]\n",
      "step:  907 loss:  1332.16 Weight:  [ 4.54112911] bias:  [-25.35331917]\n",
      "step:  908 loss:  1321.43 Weight:  [ 4.52684784] bias:  [-25.25107193]\n",
      "step:  909 loss:  1310.8 Weight:  [ 4.51262474] bias:  [-25.14923668]\n",
      "step:  910 loss:  1300.25 Weight:  [ 4.49845839] bias:  [-25.04781342]\n",
      "step:  911 loss:  1289.78 Weight:  [ 4.48434973] bias:  [-24.94679832]\n",
      "step:  912 loss:  1279.4 Weight:  [ 4.47029781] bias:  [-24.84619141]\n",
      "step:  913 loss:  1269.1 Weight:  [ 4.45630264] bias:  [-24.74598885]\n",
      "step:  914 loss:  1258.88 Weight:  [ 4.44236374] bias:  [-24.64619064]\n",
      "step:  915 loss:  1248.75 Weight:  [ 4.4284811] bias:  [-24.54679489]\n",
      "step:  916 loss:  1238.7 Weight:  [ 4.41465425] bias:  [-24.44780159]\n",
      "step:  917 loss:  1228.73 Weight:  [ 4.4008832] bias:  [-24.34920692]\n",
      "step:  918 loss:  1218.84 Weight:  [ 4.38716793] bias:  [-24.25100899]\n",
      "step:  919 loss:  1209.03 Weight:  [ 4.37350798] bias:  [-24.15320778]\n",
      "step:  920 loss:  1199.29 Weight:  [ 4.35990286] bias:  [-24.05580139]\n",
      "step:  921 loss:  1189.64 Weight:  [ 4.34635305] bias:  [-23.95878792]\n",
      "step:  922 loss:  1180.06 Weight:  [ 4.33285761] bias:  [-23.86216545]\n",
      "step:  923 loss:  1170.56 Weight:  [ 4.31941652] bias:  [-23.76593208]\n",
      "step:  924 loss:  1161.14 Weight:  [ 4.3060298] bias:  [-23.67008781]\n",
      "step:  925 loss:  1151.8 Weight:  [ 4.29269695] bias:  [-23.57462883]\n",
      "step:  926 loss:  1142.52 Weight:  [ 4.27941799] bias:  [-23.47955513]\n",
      "step:  927 loss:  1133.33 Weight:  [ 4.26619244] bias:  [-23.38486481]\n",
      "step:  928 loss:  1124.2 Weight:  [ 4.25302029] bias:  [-23.29055595]\n",
      "step:  929 loss:  1115.16 Weight:  [ 4.23990107] bias:  [-23.19662857]\n",
      "step:  930 loss:  1106.18 Weight:  [ 4.22683525] bias:  [-23.10307884]\n",
      "step:  931 loss:  1097.28 Weight:  [ 4.21382189] bias:  [-23.00990677]\n",
      "step:  932 loss:  1088.44 Weight:  [ 4.20086098] bias:  [-22.91711044]\n",
      "step:  933 loss:  1079.68 Weight:  [ 4.18795204] bias:  [-22.82468796]\n",
      "step:  934 loss:  1070.99 Weight:  [ 4.17509556] bias:  [-22.73263931]\n",
      "step:  935 loss:  1062.37 Weight:  [ 4.16229057] bias:  [-22.64096069]\n",
      "step:  936 loss:  1053.82 Weight:  [ 4.14953756] bias:  [-22.5496521]\n",
      "step:  937 loss:  1045.33 Weight:  [ 4.13683558] bias:  [-22.45871162]\n",
      "step:  938 loss:  1036.92 Weight:  [ 4.12418509] bias:  [-22.36813927]\n",
      "step:  939 loss:  1028.57 Weight:  [ 4.11158562] bias:  [-22.27793121]\n",
      "step:  940 loss:  1020.29 Weight:  [ 4.09903717] bias:  [-22.18808746]\n",
      "step:  941 loss:  1012.08 Weight:  [ 4.08653927] bias:  [-22.09860611]\n",
      "step:  942 loss:  1003.93 Weight:  [ 4.07409143] bias:  [-22.00948524]\n",
      "step:  943 loss:  995.854 Weight:  [ 4.06169415] bias:  [-21.92072296]\n",
      "step:  944 loss:  987.838 Weight:  [ 4.04934645] bias:  [-21.83231926]\n",
      "step:  945 loss:  979.886 Weight:  [ 4.03704882] bias:  [-21.74427223]\n",
      "step:  946 loss:  971.998 Weight:  [ 4.02480078] bias:  [-21.65657997]\n",
      "step:  947 loss:  964.174 Weight:  [ 4.01260233] bias:  [-21.56924248]\n",
      "step:  948 loss:  956.413 Weight:  [ 4.000453] bias:  [-21.48225594]\n",
      "step:  949 loss:  948.715 Weight:  [ 3.9883523] bias:  [-21.39562035]\n",
      "step:  950 loss:  941.078 Weight:  [ 3.97630072] bias:  [-21.3093338]\n",
      "step:  951 loss:  933.503 Weight:  [ 3.96429753] bias:  [-21.2233963]\n",
      "step:  952 loss:  925.988 Weight:  [ 3.95234299] bias:  [-21.13780403]\n",
      "step:  953 loss:  918.535 Weight:  [ 3.94043636] bias:  [-21.05255699]\n",
      "step:  954 loss:  911.141 Weight:  [ 3.9285779] bias:  [-20.96765518]\n",
      "step:  955 loss:  903.807 Weight:  [ 3.91676736] bias:  [-20.88309479]\n",
      "step:  956 loss:  896.531 Weight:  [ 3.90500426] bias:  [-20.79887581]\n",
      "step:  957 loss:  889.315 Weight:  [ 3.89328885] bias:  [-20.71499634]\n",
      "step:  958 loss:  882.156 Weight:  [ 3.88162041] bias:  [-20.63145447]\n",
      "step:  959 loss:  875.055 Weight:  [ 3.86999917] bias:  [-20.5482502]\n",
      "step:  960 loss:  868.011 Weight:  [ 3.85842466] bias:  [-20.46538162]\n",
      "step:  961 loss:  861.024 Weight:  [ 3.84689713] bias:  [-20.38284683]\n",
      "step:  962 loss:  854.094 Weight:  [ 3.83541584] bias:  [-20.30064583]\n",
      "step:  963 loss:  847.219 Weight:  [ 3.82398105] bias:  [-20.2187767]\n",
      "step:  964 loss:  840.399 Weight:  [ 3.81259227] bias:  [-20.13723755]\n",
      "step:  965 loss:  833.634 Weight:  [ 3.8012495] bias:  [-20.05602646]\n",
      "step:  966 loss:  826.924 Weight:  [ 3.78995252] bias:  [-19.97514343]\n",
      "step:  967 loss:  820.268 Weight:  [ 3.77870107] bias:  [-19.89458656]\n",
      "step:  968 loss:  813.665 Weight:  [ 3.76749492] bias:  [-19.81435394]\n",
      "step:  969 loss:  807.115 Weight:  [ 3.75633383] bias:  [-19.73444557]\n",
      "step:  970 loss:  800.618 Weight:  [ 3.74521804] bias:  [-19.65485954]\n",
      "step:  971 loss:  794.174 Weight:  [ 3.73414707] bias:  [-19.57559395]\n",
      "step:  972 loss:  787.781 Weight:  [ 3.72312069] bias:  [-19.49664879]\n",
      "step:  973 loss:  781.44 Weight:  [ 3.71213865] bias:  [-19.41802025]\n",
      "step:  974 loss:  775.15 Weight:  [ 3.70120096] bias:  [-19.33971024]\n",
      "step:  975 loss:  768.91 Weight:  [ 3.69030738] bias:  [-19.26171494]\n",
      "step:  976 loss:  762.721 Weight:  [ 3.67945766] bias:  [-19.18403435]\n",
      "step:  977 loss:  756.582 Weight:  [ 3.66865158] bias:  [-19.10666656]\n",
      "step:  978 loss:  750.491 Weight:  [ 3.65788913] bias:  [-19.02961159]\n",
      "step:  979 loss:  744.45 Weight:  [ 3.64717007] bias:  [-18.95286751]\n",
      "step:  980 loss:  738.458 Weight:  [ 3.6364944] bias:  [-18.87643242]\n",
      "step:  981 loss:  732.513 Weight:  [ 3.62586164] bias:  [-18.80030632]\n",
      "step:  982 loss:  726.617 Weight:  [ 3.61527181] bias:  [-18.7244873]\n",
      "step:  983 loss:  720.768 Weight:  [ 3.60472488] bias:  [-18.64897346]\n",
      "step:  984 loss:  714.966 Weight:  [ 3.5942204] bias:  [-18.5737648]\n",
      "step:  985 loss:  709.211 Weight:  [ 3.58375812] bias:  [-18.49885941]\n",
      "step:  986 loss:  703.503 Weight:  [ 3.57333827] bias:  [-18.42425537]\n",
      "step:  987 loss:  697.84 Weight:  [ 3.56296015] bias:  [-18.3499527]\n",
      "step:  988 loss:  692.222 Weight:  [ 3.55262399] bias:  [-18.27594948]\n",
      "step:  989 loss:  686.65 Weight:  [ 3.54232955] bias:  [-18.20224571]\n",
      "step:  990 loss:  681.123 Weight:  [ 3.53207684] bias:  [-18.12883759]\n",
      "step:  991 loss:  675.641 Weight:  [ 3.52186513] bias:  [-18.05572701]\n",
      "step:  992 loss:  670.202 Weight:  [ 3.51169491] bias:  [-17.98291016]\n",
      "step:  993 loss:  664.807 Weight:  [ 3.50156546] bias:  [-17.91038704]\n",
      "step:  994 loss:  659.456 Weight:  [ 3.49147701] bias:  [-17.83815575]\n",
      "step:  995 loss:  654.148 Weight:  [ 3.4814291] bias:  [-17.76621628]\n",
      "step:  996 loss:  648.882 Weight:  [ 3.47142172] bias:  [-17.69456673]\n",
      "step:  997 loss:  643.659 Weight:  [ 3.46145463] bias:  [-17.62320709]\n",
      "step:  998 loss:  638.478 Weight:  [ 3.45152807] bias:  [-17.55213547]\n",
      "step:  999 loss:  633.338 Weight:  [ 3.44164133] bias:  [-17.48134995]\n",
      "step:  1000 loss:  628.24 Weight:  [ 3.43179464] bias:  [-17.41085052]\n",
      "step:  1001 loss:  623.183 Weight:  [ 3.42198753] bias:  [-17.3406353]\n",
      "step:  1002 loss:  618.167 Weight:  [ 3.41222] bias:  [-17.27070236]\n",
      "step:  1003 loss:  613.191 Weight:  [ 3.40249181] bias:  [-17.20105171]\n",
      "step:  1004 loss:  608.255 Weight:  [ 3.39280295] bias:  [-17.13168144]\n",
      "step:  1005 loss:  603.359 Weight:  [ 3.38315296] bias:  [-17.06259155]\n",
      "step:  1006 loss:  598.502 Weight:  [ 3.37354207] bias:  [-16.99378014]\n",
      "step:  1007 loss:  593.685 Weight:  [ 3.3639698] bias:  [-16.92524719]\n",
      "step:  1008 loss:  588.906 Weight:  [ 3.35443616] bias:  [-16.85698891]\n",
      "step:  1009 loss:  584.165 Weight:  [ 3.34494114] bias:  [-16.78900719]\n",
      "step:  1010 loss:  579.463 Weight:  [ 3.33548427] bias:  [-16.72129822]\n",
      "step:  1011 loss:  574.799 Weight:  [ 3.3260653] bias:  [-16.65386391]\n",
      "step:  1012 loss:  570.172 Weight:  [ 3.31668472] bias:  [-16.58670044]\n",
      "step:  1013 loss:  565.582 Weight:  [ 3.30734181] bias:  [-16.51980782]\n",
      "step:  1014 loss:  561.03 Weight:  [ 3.29803658] bias:  [-16.45318604]\n",
      "step:  1015 loss:  556.514 Weight:  [ 3.28876877] bias:  [-16.38683319]\n",
      "step:  1016 loss:  552.034 Weight:  [ 3.27953863] bias:  [-16.32074738]\n",
      "step:  1017 loss:  547.59 Weight:  [ 3.27034545] bias:  [-16.25492859]\n",
      "step:  1018 loss:  543.183 Weight:  [ 3.2611897] bias:  [-16.18937492]\n",
      "step:  1019 loss:  538.81 Weight:  [ 3.25207067] bias:  [-16.12408447]\n",
      "step:  1020 loss:  534.473 Weight:  [ 3.24298835] bias:  [-16.05905724]\n",
      "step:  1021 loss:  530.171 Weight:  [ 3.23394251] bias:  [-15.99429321]\n",
      "step:  1022 loss:  525.903 Weight:  [ 3.22493315] bias:  [-15.9297905]\n",
      "step:  1023 loss:  521.67 Weight:  [ 3.21596026] bias:  [-15.86554718]\n",
      "step:  1024 loss:  517.471 Weight:  [ 3.20702362] bias:  [-15.80156326]\n",
      "step:  1025 loss:  513.305 Weight:  [ 3.19812298] bias:  [-15.73783779]\n",
      "step:  1026 loss:  509.174 Weight:  [ 3.1892581] bias:  [-15.67436886]\n",
      "step:  1027 loss:  505.075 Weight:  [ 3.18042922] bias:  [-15.61115646]\n",
      "step:  1028 loss:  501.009 Weight:  [ 3.17163587] bias:  [-15.5481987]\n",
      "step:  1029 loss:  496.977 Weight:  [ 3.1628778] bias:  [-15.48549461]\n",
      "step:  1030 loss:  492.976 Weight:  [ 3.15415525] bias:  [-15.42304325]\n",
      "step:  1031 loss:  489.008 Weight:  [ 3.14546776] bias:  [-15.36084366]\n",
      "step:  1032 loss:  485.072 Weight:  [ 3.13681531] bias:  [-15.29889488]\n",
      "step:  1033 loss:  481.167 Weight:  [ 3.12819767] bias:  [-15.23719597]\n",
      "step:  1034 loss:  477.294 Weight:  [ 3.11961484] bias:  [-15.17574596]\n",
      "step:  1035 loss:  473.452 Weight:  [ 3.11106682] bias:  [-15.11454391]\n",
      "step:  1036 loss:  469.641 Weight:  [ 3.10255313] bias:  [-15.05358887]\n",
      "step:  1037 loss:  465.86 Weight:  [ 3.09407377] bias:  [-14.99287987]\n",
      "step:  1038 loss:  462.111 Weight:  [ 3.08562875] bias:  [-14.93241501]\n",
      "step:  1039 loss:  458.391 Weight:  [ 3.07721758] bias:  [-14.87219429]\n",
      "step:  1040 loss:  454.701 Weight:  [ 3.06884027] bias:  [-14.81221676]\n",
      "step:  1041 loss:  451.041 Weight:  [ 3.06049705] bias:  [-14.75248051]\n",
      "step:  1042 loss:  447.41 Weight:  [ 3.0521872] bias:  [-14.69298553]\n",
      "step:  1043 loss:  443.809 Weight:  [ 3.04391098] bias:  [-14.63373089]\n",
      "step:  1044 loss:  440.236 Weight:  [ 3.03566813] bias:  [-14.57471466]\n",
      "step:  1045 loss:  436.693 Weight:  [ 3.02745843] bias:  [-14.51593685]\n",
      "step:  1046 loss:  433.177 Weight:  [ 3.0192821] bias:  [-14.45739555]\n",
      "step:  1047 loss:  429.691 Weight:  [ 3.01113844] bias:  [-14.39909077]\n",
      "step:  1048 loss:  426.232 Weight:  [ 3.00302792] bias:  [-14.34102058]\n",
      "step:  1049 loss:  422.801 Weight:  [ 2.99494982] bias:  [-14.28318501]\n",
      "step:  1050 loss:  419.397 Weight:  [ 2.98690438] bias:  [-14.22558308]\n",
      "step:  1051 loss:  416.022 Weight:  [ 2.97889161] bias:  [-14.16821289]\n",
      "step:  1052 loss:  412.673 Weight:  [ 2.97091103] bias:  [-14.11107445]\n",
      "step:  1053 loss:  409.351 Weight:  [ 2.96296263] bias:  [-14.05416584]\n",
      "step:  1054 loss:  406.056 Weight:  [ 2.95504618] bias:  [-13.99748707]\n",
      "step:  1055 loss:  402.787 Weight:  [ 2.94716167] bias:  [-13.94103718]\n",
      "step:  1056 loss:  399.545 Weight:  [ 2.93930888] bias:  [-13.88481426]\n",
      "step:  1057 loss:  396.329 Weight:  [ 2.93148804] bias:  [-13.82881832]\n",
      "step:  1058 loss:  393.139 Weight:  [ 2.92369843] bias:  [-13.7730484]\n",
      "step:  1059 loss:  389.974 Weight:  [ 2.91594028] bias:  [-13.71750355]\n",
      "step:  1060 loss:  386.835 Weight:  [ 2.90821362] bias:  [-13.66218281]\n",
      "step:  1061 loss:  383.721 Weight:  [ 2.90051818] bias:  [-13.60708523]\n",
      "step:  1062 loss:  380.633 Weight:  [ 2.8928535] bias:  [-13.55220985]\n",
      "step:  1063 loss:  377.569 Weight:  [ 2.88522005] bias:  [-13.49755573]\n",
      "step:  1064 loss:  374.529 Weight:  [ 2.87761712] bias:  [-13.44312191]\n",
      "step:  1065 loss:  371.515 Weight:  [ 2.87004495] bias:  [-13.38890743]\n",
      "step:  1066 loss:  368.524 Weight:  [ 2.86250329] bias:  [-13.33491135]\n",
      "step:  1067 loss:  365.558 Weight:  [ 2.85499215] bias:  [-13.28113365]\n",
      "step:  1068 loss:  362.615 Weight:  [ 2.84751129] bias:  [-13.22757244]\n",
      "step:  1069 loss:  359.696 Weight:  [ 2.84006047] bias:  [-13.17422771]\n",
      "step:  1070 loss:  356.801 Weight:  [ 2.83263969] bias:  [-13.12109756]\n",
      "step:  1071 loss:  353.929 Weight:  [ 2.82524896] bias:  [-13.06818199]\n",
      "step:  1072 loss:  351.08 Weight:  [ 2.81788802] bias:  [-13.01548004]\n",
      "step:  1073 loss:  348.254 Weight:  [ 2.81055665] bias:  [-12.96298981]\n",
      "step:  1074 loss:  345.451 Weight:  [ 2.80325484] bias:  [-12.91071129]\n",
      "step:  1075 loss:  342.67 Weight:  [ 2.7959826] bias:  [-12.85864353]\n",
      "step:  1076 loss:  339.912 Weight:  [ 2.78873944] bias:  [-12.80678654]\n",
      "step:  1077 loss:  337.176 Weight:  [ 2.78152561] bias:  [-12.7551384]\n",
      "step:  1078 loss:  334.461 Weight:  [ 2.77434111] bias:  [-12.70369816]\n",
      "step:  1079 loss:  331.769 Weight:  [ 2.76718521] bias:  [-12.65246582]\n",
      "step:  1080 loss:  329.099 Weight:  [ 2.7600584] bias:  [-12.60144043]\n",
      "step:  1081 loss:  326.45 Weight:  [ 2.75296044] bias:  [-12.55062008]\n",
      "step:  1082 loss:  323.822 Weight:  [ 2.74589086] bias:  [-12.50000477]\n",
      "step:  1083 loss:  321.215 Weight:  [ 2.73884988] bias:  [-12.44959354]\n",
      "step:  1084 loss:  318.63 Weight:  [ 2.73183727] bias:  [-12.39938545]\n",
      "step:  1085 loss:  316.065 Weight:  [ 2.72485304] bias:  [-12.34938049]\n",
      "step:  1086 loss:  313.521 Weight:  [ 2.71789694] bias:  [-12.29957676]\n",
      "step:  1087 loss:  310.997 Weight:  [ 2.71096873] bias:  [-12.24997425]\n",
      "step:  1088 loss:  308.494 Weight:  [ 2.70406866] bias:  [-12.20057201]\n",
      "step:  1089 loss:  306.01 Weight:  [ 2.69719648] bias:  [-12.15136814]\n",
      "step:  1090 loss:  303.547 Weight:  [ 2.69035172] bias:  [-12.10236359]\n",
      "step:  1091 loss:  301.104 Weight:  [ 2.68353486] bias:  [-12.05355644]\n",
      "step:  1092 loss:  298.68 Weight:  [ 2.67674541] bias:  [-12.00494576]\n",
      "step:  1093 loss:  296.276 Weight:  [ 2.66998339] bias:  [-11.95653152]\n",
      "step:  1094 loss:  293.891 Weight:  [ 2.66324854] bias:  [-11.90831184]\n",
      "step:  1095 loss:  291.525 Weight:  [ 2.65654087] bias:  [-11.86028671]\n",
      "step:  1096 loss:  289.179 Weight:  [ 2.64986014] bias:  [-11.81245518]\n",
      "step:  1097 loss:  286.851 Weight:  [ 2.64320636] bias:  [-11.76481724]\n",
      "step:  1098 loss:  284.542 Weight:  [ 2.63657951] bias:  [-11.71737099]\n",
      "step:  1099 loss:  282.251 Weight:  [ 2.62997937] bias:  [-11.67011642]\n",
      "step:  1100 loss:  279.979 Weight:  [ 2.62340593] bias:  [-11.6230526]\n",
      "step:  1101 loss:  277.726 Weight:  [ 2.61685896] bias:  [-11.57617855]\n",
      "step:  1102 loss:  275.49 Weight:  [ 2.61033845] bias:  [-11.52949333]\n",
      "step:  1103 loss:  273.273 Weight:  [ 2.60384417] bias:  [-11.48299599]\n",
      "step:  1104 loss:  271.073 Weight:  [ 2.59737587] bias:  [-11.43668652]\n",
      "step:  1105 loss:  268.891 Weight:  [ 2.5909338] bias:  [-11.39056396]\n",
      "step:  1106 loss:  266.727 Weight:  [ 2.58451796] bias:  [-11.34462738]\n",
      "step:  1107 loss:  264.58 Weight:  [ 2.57812786] bias:  [-11.29887581]\n",
      "step:  1108 loss:  262.45 Weight:  [ 2.57176328] bias:  [-11.25330925]\n",
      "step:  1109 loss:  260.337 Weight:  [ 2.56542468] bias:  [-11.2079258]\n",
      "step:  1110 loss:  258.242 Weight:  [ 2.5591116] bias:  [-11.16272545]\n",
      "step:  1111 loss:  256.163 Weight:  [ 2.55282378] bias:  [-11.11770725]\n",
      "step:  1112 loss:  254.101 Weight:  [ 2.54656148] bias:  [-11.07287121]\n",
      "step:  1113 loss:  252.056 Weight:  [ 2.54032445] bias:  [-11.02821541]\n",
      "step:  1114 loss:  250.027 Weight:  [ 2.53411245] bias:  [-10.98373985]\n",
      "step:  1115 loss:  248.014 Weight:  [ 2.52792549] bias:  [-10.93944359]\n",
      "step:  1116 loss:  246.018 Weight:  [ 2.52176356] bias:  [-10.89532661]\n",
      "step:  1117 loss:  244.037 Weight:  [ 2.51562643] bias:  [-10.85138702]\n",
      "step:  1118 loss:  242.073 Weight:  [ 2.50951409] bias:  [-10.80762482]\n",
      "step:  1119 loss:  240.124 Weight:  [ 2.50342631] bias:  [-10.76403904]\n",
      "step:  1120 loss:  238.192 Weight:  [ 2.49736333] bias:  [-10.72062874]\n",
      "step:  1121 loss:  236.274 Weight:  [ 2.49132466] bias:  [-10.67739391]\n",
      "step:  1122 loss:  234.372 Weight:  [ 2.48531032] bias:  [-10.63433361]\n",
      "step:  1123 loss:  232.486 Weight:  [ 2.47932029] bias:  [-10.59144688]\n",
      "step:  1124 loss:  230.614 Weight:  [ 2.47335434] bias:  [-10.54873276]\n",
      "step:  1125 loss:  228.758 Weight:  [ 2.46741247] bias:  [-10.50619125]\n",
      "step:  1126 loss:  226.917 Weight:  [ 2.46149468] bias:  [-10.46382141]\n",
      "step:  1127 loss:  225.09 Weight:  [ 2.45560074] bias:  [-10.42162228]\n",
      "step:  1128 loss:  223.278 Weight:  [ 2.4497304] bias:  [-10.3795929]\n",
      "step:  1129 loss:  221.481 Weight:  [ 2.4438839] bias:  [-10.33773327]\n",
      "step:  1130 loss:  219.698 Weight:  [ 2.43806076] bias:  [-10.29604244]\n",
      "step:  1131 loss:  217.93 Weight:  [ 2.43226123] bias:  [-10.25451946]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step:  1132 loss:  216.175 Weight:  [ 2.42648506] bias:  [-10.21316433]\n",
      "step:  1133 loss:  214.435 Weight:  [ 2.42073226] bias:  [-10.17197609]\n",
      "step:  1134 loss:  212.709 Weight:  [ 2.41500258] bias:  [-10.13095379]\n",
      "step:  1135 loss:  210.997 Weight:  [ 2.40929604] bias:  [-10.09009647]\n",
      "step:  1136 loss:  209.299 Weight:  [ 2.40361261] bias:  [-10.04940414]\n",
      "step:  1137 loss:  207.614 Weight:  [ 2.39795208] bias:  [-10.00887585]\n",
      "step:  1138 loss:  205.943 Weight:  [ 2.3923142] bias:  [-9.96851158]\n",
      "step:  1139 loss:  204.285 Weight:  [ 2.3866992] bias:  [-9.92830944]\n",
      "step:  1140 loss:  202.641 Weight:  [ 2.38110662] bias:  [-9.88826942]\n",
      "step:  1141 loss:  201.009 Weight:  [ 2.37553668] bias:  [-9.84839153]\n",
      "step:  1142 loss:  199.391 Weight:  [ 2.3699894] bias:  [-9.80867386]\n",
      "step:  1143 loss:  197.786 Weight:  [ 2.36446452] bias:  [-9.7691164]\n",
      "step:  1144 loss:  196.194 Weight:  [ 2.35896158] bias:  [-9.72971916]\n",
      "step:  1145 loss:  194.615 Weight:  [ 2.35348129] bias:  [-9.69048023]\n",
      "step:  1146 loss:  193.048 Weight:  [ 2.3480227] bias:  [-9.65139961]\n",
      "step:  1147 loss:  191.495 Weight:  [ 2.34258628] bias:  [-9.61247635]\n",
      "step:  1148 loss:  189.953 Weight:  [ 2.33717179] bias:  [-9.57371044]\n",
      "step:  1149 loss:  188.424 Weight:  [ 2.33177924] bias:  [-9.53510094]\n",
      "step:  1150 loss:  186.907 Weight:  [ 2.32640839] bias:  [-9.49664688]\n",
      "step:  1151 loss:  185.403 Weight:  [ 2.32105899] bias:  [-9.45834827]\n",
      "step:  1152 loss:  183.91 Weight:  [ 2.31573129] bias:  [-9.42020416]\n",
      "step:  1153 loss:  182.43 Weight:  [ 2.31042528] bias:  [-9.38221359]\n",
      "step:  1154 loss:  180.962 Weight:  [ 2.3051405] bias:  [-9.34437656]\n",
      "step:  1155 loss:  179.505 Weight:  [ 2.29987693] bias:  [-9.30669212]\n",
      "step:  1156 loss:  178.06 Weight:  [ 2.29463482] bias:  [-9.26915932]\n",
      "step:  1157 loss:  176.627 Weight:  [ 2.28941369] bias:  [-9.23177814]\n",
      "step:  1158 loss:  175.205 Weight:  [ 2.28421378] bias:  [-9.19454765]\n",
      "step:  1159 loss:  173.795 Weight:  [ 2.27903461] bias:  [-9.15746689]\n",
      "step:  1160 loss:  172.396 Weight:  [ 2.27387643] bias:  [-9.12053585]\n",
      "step:  1161 loss:  171.008 Weight:  [ 2.26873899] bias:  [-9.08375359]\n",
      "step:  1162 loss:  169.631 Weight:  [ 2.26362228] bias:  [-9.04712009]\n",
      "step:  1163 loss:  168.266 Weight:  [ 2.25852633] bias:  [-9.01063442]\n",
      "step:  1164 loss:  166.912 Weight:  [ 2.25345087] bias:  [-8.97429562]\n",
      "step:  1165 loss:  165.568 Weight:  [ 2.24839592] bias:  [-8.93810368]\n",
      "step:  1166 loss:  164.235 Weight:  [ 2.24336123] bias:  [-8.90205765]\n",
      "step:  1167 loss:  162.913 Weight:  [ 2.23834682] bias:  [-8.86615658]\n",
      "step:  1168 loss:  161.602 Weight:  [ 2.2333529] bias:  [-8.83040047]\n",
      "step:  1169 loss:  160.301 Weight:  [ 2.22837877] bias:  [-8.79478836]\n",
      "step:  1170 loss:  159.011 Weight:  [ 2.22342491] bias:  [-8.75932026]\n",
      "step:  1171 loss:  157.731 Weight:  [ 2.21849108] bias:  [-8.72399521]\n",
      "step:  1172 loss:  156.461 Weight:  [ 2.21357703] bias:  [-8.68881226]\n",
      "step:  1173 loss:  155.202 Weight:  [ 2.20868278] bias:  [-8.6537714]\n",
      "step:  1174 loss:  153.952 Weight:  [ 2.20380831] bias:  [-8.61887169]\n",
      "step:  1175 loss:  152.713 Weight:  [ 2.19895363] bias:  [-8.58411312]\n",
      "step:  1176 loss:  151.484 Weight:  [ 2.19411826] bias:  [-8.54949474]\n",
      "step:  1177 loss:  150.265 Weight:  [ 2.18930268] bias:  [-8.5150156]\n",
      "step:  1178 loss:  149.055 Weight:  [ 2.18450642] bias:  [-8.4806757]\n",
      "step:  1179 loss:  147.855 Weight:  [ 2.17972946] bias:  [-8.44647408]\n",
      "step:  1180 loss:  146.665 Weight:  [ 2.17497158] bias:  [-8.41241074]\n",
      "step:  1181 loss:  145.484 Weight:  [ 2.17023301] bias:  [-8.37848473]\n",
      "step:  1182 loss:  144.313 Weight:  [ 2.16551375] bias:  [-8.34469509]\n",
      "step:  1183 loss:  143.152 Weight:  [ 2.16081333] bias:  [-8.31104183]\n",
      "step:  1184 loss:  141.999 Weight:  [ 2.15613198] bias:  [-8.27752399]\n",
      "step:  1185 loss:  140.856 Weight:  [ 2.15146947] bias:  [-8.24414158]\n",
      "step:  1186 loss:  139.723 Weight:  [ 2.14682555] bias:  [-8.21089363]\n",
      "step:  1187 loss:  138.598 Weight:  [ 2.14220047] bias:  [-8.17778015]\n",
      "step:  1188 loss:  137.482 Weight:  [ 2.13759422] bias:  [-8.14480019]\n",
      "step:  1189 loss:  136.376 Weight:  [ 2.13300633] bias:  [-8.11195278]\n",
      "step:  1190 loss:  135.278 Weight:  [ 2.12843704] bias:  [-8.07923794]\n",
      "step:  1191 loss:  134.189 Weight:  [ 2.12388611] bias:  [-8.04665565]\n",
      "step:  1192 loss:  133.109 Weight:  [ 2.11935377] bias:  [-8.01420403]\n",
      "step:  1193 loss:  132.037 Weight:  [ 2.11483955] bias:  [-7.98188353]\n",
      "step:  1194 loss:  130.974 Weight:  [ 2.11034346] bias:  [-7.94969368]\n",
      "step:  1195 loss:  129.92 Weight:  [ 2.10586548] bias:  [-7.91763353]\n",
      "step:  1196 loss:  128.874 Weight:  [ 2.10140562] bias:  [-7.88570261]\n",
      "step:  1197 loss:  127.837 Weight:  [ 2.09696388] bias:  [-7.85390043]\n",
      "step:  1198 loss:  126.808 Weight:  [ 2.09254003] bias:  [-7.82222652]\n",
      "step:  1199 loss:  125.787 Weight:  [ 2.08813381] bias:  [-7.79068041]\n",
      "step:  1200 loss:  124.775 Weight:  [ 2.08374548] bias:  [-7.75926161]\n",
      "step:  1201 loss:  123.77 Weight:  [ 2.07937479] bias:  [-7.72796965]\n",
      "step:  1202 loss:  122.774 Weight:  [ 2.07502198] bias:  [-7.69680357]\n",
      "step:  1203 loss:  121.786 Weight:  [ 2.07068658] bias:  [-7.66576338]\n",
      "step:  1204 loss:  120.805 Weight:  [ 2.06636858] bias:  [-7.63484812]\n",
      "step:  1205 loss:  119.833 Weight:  [ 2.06206799] bias:  [-7.60405779]\n",
      "step:  1206 loss:  118.868 Weight:  [ 2.0577848] bias:  [-7.57339144]\n",
      "step:  1207 loss:  117.912 Weight:  [ 2.05351877] bias:  [-7.54284906]\n",
      "step:  1208 loss:  116.962 Weight:  [ 2.04927015] bias:  [-7.51242971]\n",
      "step:  1209 loss:  116.021 Weight:  [ 2.0450387] bias:  [-7.48213291]\n",
      "step:  1210 loss:  115.087 Weight:  [ 2.04082417] bias:  [-7.45195818]\n",
      "step:  1211 loss:  114.161 Weight:  [ 2.03662658] bias:  [-7.42190552]\n",
      "step:  1212 loss:  113.242 Weight:  [ 2.03244591] bias:  [-7.39197397]\n",
      "step:  1213 loss:  112.33 Weight:  [ 2.02828217] bias:  [-7.36216307]\n",
      "step:  1214 loss:  111.426 Weight:  [ 2.02413535] bias:  [-7.33247232]\n",
      "step:  1215 loss:  110.529 Weight:  [ 2.02000499] bias:  [-7.30290127]\n",
      "step:  1216 loss:  109.639 Weight:  [ 2.01589155] bias:  [-7.27344942]\n",
      "step:  1217 loss:  108.757 Weight:  [ 2.01179457] bias:  [-7.24411631]\n",
      "step:  1218 loss:  107.881 Weight:  [ 2.00771403] bias:  [-7.21490145]\n",
      "step:  1219 loss:  107.013 Weight:  [ 2.00364995] bias:  [-7.18580484]\n",
      "step:  1220 loss:  106.152 Weight:  [ 1.99960244] bias:  [-7.15682554]\n",
      "step:  1221 loss:  105.297 Weight:  [ 1.99557126] bias:  [-7.12796307]\n",
      "step:  1222 loss:  104.45 Weight:  [ 1.99155617] bias:  [-7.09921694]\n",
      "step:  1223 loss:  103.609 Weight:  [ 1.98755741] bias:  [-7.07058668]\n",
      "step:  1224 loss:  102.775 Weight:  [ 1.98357475] bias:  [-7.04207182]\n",
      "step:  1225 loss:  101.948 Weight:  [ 1.97960818] bias:  [-7.01367188]\n",
      "step:  1226 loss:  101.127 Weight:  [ 1.97565746] bias:  [-6.98538637]\n",
      "step:  1227 loss:  100.313 Weight:  [ 1.97172272] bias:  [-6.95721531]\n",
      "step:  1228 loss:  99.5054 Weight:  [ 1.96780384] bias:  [-6.92915773]\n",
      "step:  1229 loss:  98.7044 Weight:  [ 1.96390092] bias:  [-6.90121317]\n",
      "step:  1230 loss:  97.9099 Weight:  [ 1.96001351] bias:  [-6.87338161]\n",
      "step:  1231 loss:  97.1218 Weight:  [ 1.95614195] bias:  [-6.84566212]\n",
      "step:  1232 loss:  96.34 Weight:  [ 1.95228589] bias:  [-6.8180542]\n",
      "step:  1233 loss:  95.5645 Weight:  [ 1.94844544] bias:  [-6.79055786]\n",
      "step:  1234 loss:  94.7953 Weight:  [ 1.94462049] bias:  [-6.76317215]\n",
      "step:  1235 loss:  94.0322 Weight:  [ 1.94081092] bias:  [-6.73589706]\n",
      "step:  1236 loss:  93.2753 Weight:  [ 1.93701673] bias:  [-6.70873213]\n",
      "step:  1237 loss:  92.5245 Weight:  [ 1.93323791] bias:  [-6.68167639]\n",
      "step:  1238 loss:  91.7797 Weight:  [ 1.92947423] bias:  [-6.65472984]\n",
      "step:  1239 loss:  91.0409 Weight:  [ 1.9257257] bias:  [-6.62789202]\n",
      "step:  1240 loss:  90.3081 Weight:  [ 1.92199242] bias:  [-6.60116243]\n",
      "step:  1241 loss:  89.5811 Weight:  [ 1.91827404] bias:  [-6.57454062]\n",
      "step:  1242 loss:  88.8601 Weight:  [ 1.91457081] bias:  [-6.54802608]\n",
      "step:  1243 loss:  88.1448 Weight:  [ 1.91088247] bias:  [-6.52161884]\n",
      "step:  1244 loss:  87.4353 Weight:  [ 1.90720892] bias:  [-6.49531794]\n",
      "step:  1245 loss:  86.7315 Weight:  [ 1.90355027] bias:  [-6.46912289]\n",
      "step:  1246 loss:  86.0333 Weight:  [ 1.8999064] bias:  [-6.4430337]\n",
      "step:  1247 loss:  85.3408 Weight:  [ 1.89627719] bias:  [-6.41704988]\n",
      "step:  1248 loss:  84.6538 Weight:  [ 1.89266264] bias:  [-6.3911705]\n",
      "step:  1249 loss:  83.9724 Weight:  [ 1.88906264] bias:  [-6.36539555]\n",
      "step:  1250 loss:  83.2965 Weight:  [ 1.88547707] bias:  [-6.33972454]\n",
      "step:  1251 loss:  82.626 Weight:  [ 1.88190603] bias:  [-6.31415701]\n",
      "step:  1252 loss:  81.9609 Weight:  [ 1.87834942] bias:  [-6.28869295]\n",
      "step:  1253 loss:  81.3011 Weight:  [ 1.87480712] bias:  [-6.26333141]\n",
      "step:  1254 loss:  80.6467 Weight:  [ 1.87127912] bias:  [-6.2380724]\n",
      "step:  1255 loss:  79.9975 Weight:  [ 1.86776543] bias:  [-6.21291494]\n",
      "step:  1256 loss:  79.3536 Weight:  [ 1.8642658] bias:  [-6.18785906]\n",
      "step:  1257 loss:  78.7148 Weight:  [ 1.86078036] bias:  [-6.16290426]\n",
      "step:  1258 loss:  78.0812 Weight:  [ 1.85730898] bias:  [-6.13805008]\n",
      "step:  1259 loss:  77.4527 Weight:  [ 1.85385156] bias:  [-6.11329603]\n",
      "step:  1260 loss:  76.8292 Weight:  [ 1.85040808] bias:  [-6.08864164]\n",
      "step:  1261 loss:  76.2108 Weight:  [ 1.84697843] bias:  [-6.06408691]\n",
      "step:  1262 loss:  75.5974 Weight:  [ 1.8435626] bias:  [-6.03963137]\n",
      "step:  1263 loss:  74.9888 Weight:  [ 1.84016061] bias:  [-6.01527405]\n",
      "step:  1264 loss:  74.3852 Weight:  [ 1.83677232] bias:  [-5.99101496]\n",
      "step:  1265 loss:  73.7864 Weight:  [ 1.83339775] bias:  [-5.9668541]\n",
      "step:  1266 loss:  73.1925 Weight:  [ 1.83003676] bias:  [-5.94279051]\n",
      "step:  1267 loss:  72.6033 Weight:  [ 1.82668936] bias:  [-5.91882372]\n",
      "step:  1268 loss:  72.0189 Weight:  [ 1.82335544] bias:  [-5.89495373]\n",
      "step:  1269 loss:  71.4392 Weight:  [ 1.82003486] bias:  [-5.87118006]\n",
      "step:  1270 loss:  70.8642 Weight:  [ 1.81672776] bias:  [-5.84750223]\n",
      "step:  1271 loss:  70.2937 Weight:  [ 1.813434] bias:  [-5.82391977]\n",
      "step:  1272 loss:  69.7279 Weight:  [ 1.81015348] bias:  [-5.80043268]\n",
      "step:  1273 loss:  69.1666 Weight:  [ 1.8068862] bias:  [-5.77704048]\n",
      "step:  1274 loss:  68.6099 Weight:  [ 1.80363214] bias:  [-5.75374222]\n",
      "step:  1275 loss:  68.0576 Weight:  [ 1.8003912] bias:  [-5.73053789]\n",
      "step:  1276 loss:  67.5098 Weight:  [ 1.79716325] bias:  [-5.7074275]\n",
      "step:  1277 loss:  66.9663 Weight:  [ 1.79394853] bias:  [-5.6844101]\n",
      "step:  1278 loss:  66.4273 Weight:  [ 1.79074657] bias:  [-5.66148567]\n",
      "step:  1279 loss:  65.8926 Weight:  [ 1.7875576] bias:  [-5.63865376]\n",
      "step:  1280 loss:  65.3622 Weight:  [ 1.78438151] bias:  [-5.61591387]\n",
      "step:  1281 loss:  64.8361 Weight:  [ 1.78121817] bias:  [-5.59326553]\n",
      "step:  1282 loss:  64.3142 Weight:  [ 1.77806759] bias:  [-5.57070875]\n",
      "step:  1283 loss:  63.7965 Weight:  [ 1.77492976] bias:  [-5.54824257]\n",
      "step:  1284 loss:  63.2829 Weight:  [ 1.77180457] bias:  [-5.52586699]\n",
      "step:  1285 loss:  62.7735 Weight:  [ 1.76869202] bias:  [-5.503582]\n",
      "step:  1286 loss:  62.2682 Weight:  [ 1.76559198] bias:  [-5.48138666]\n",
      "step:  1287 loss:  61.767 Weight:  [ 1.76250446] bias:  [-5.45928097]\n",
      "step:  1288 loss:  61.2698 Weight:  [ 1.75942934] bias:  [-5.43726444]\n",
      "step:  1289 loss:  60.7766 Weight:  [ 1.75636661] bias:  [-5.41533661]\n",
      "step:  1290 loss:  60.2874 Weight:  [ 1.75331628] bias:  [-5.39349699]\n",
      "step:  1291 loss:  59.8021 Weight:  [ 1.75027823] bias:  [-5.37174559]\n",
      "step:  1292 loss:  59.3208 Weight:  [ 1.74725246] bias:  [-5.35008192]\n",
      "step:  1293 loss:  58.8432 Weight:  [ 1.74423885] bias:  [-5.32850552]\n",
      "step:  1294 loss:  58.3696 Weight:  [ 1.7412374] bias:  [-5.30701637]\n",
      "step:  1295 loss:  57.8997 Weight:  [ 1.73824811] bias:  [-5.28561401]\n",
      "step:  1296 loss:  57.4337 Weight:  [ 1.73527086] bias:  [-5.26429796]\n",
      "step:  1297 loss:  56.9714 Weight:  [ 1.73230565] bias:  [-5.24306774]\n",
      "step:  1298 loss:  56.5128 Weight:  [ 1.72935236] bias:  [-5.22192287]\n",
      "step:  1299 loss:  56.0579 Weight:  [ 1.72641098] bias:  [-5.20086336]\n",
      "step:  1300 loss:  55.6066 Weight:  [ 1.72348142] bias:  [-5.17988873]\n",
      "step:  1301 loss:  55.159 Weight:  [ 1.72056365] bias:  [-5.15899897]\n",
      "step:  1302 loss:  54.715 Weight:  [ 1.71765769] bias:  [-5.13819313]\n",
      "step:  1303 loss:  54.2746 Weight:  [ 1.71476352] bias:  [-5.11747122]\n",
      "step:  1304 loss:  53.8377 Weight:  [ 1.71188092] bias:  [-5.09683323]\n",
      "step:  1305 loss:  53.4043 Weight:  [ 1.70901] bias:  [-5.07627821]\n",
      "step:  1306 loss:  52.9745 Weight:  [ 1.70615065] bias:  [-5.05580616]\n",
      "step:  1307 loss:  52.548 Weight:  [ 1.70330274] bias:  [-5.0354166]\n",
      "step:  1308 loss:  52.1251 Weight:  [ 1.70046639] bias:  [-5.01510954]\n",
      "step:  1309 loss:  51.7055 Weight:  [ 1.69764161] bias:  [-4.99488401]\n",
      "step:  1310 loss:  51.2893 Weight:  [ 1.69482803] bias:  [-4.97474003]\n",
      "step:  1311 loss:  50.8764 Weight:  [ 1.6920259] bias:  [-4.95467758]\n",
      "step:  1312 loss:  50.4669 Weight:  [ 1.68923497] bias:  [-4.9346962]\n",
      "step:  1313 loss:  50.0607 Weight:  [ 1.68645537] bias:  [-4.9147954]\n",
      "step:  1314 loss:  49.6577 Weight:  [ 1.68368709] bias:  [-4.89497471]\n",
      "step:  1315 loss:  49.258 Weight:  [ 1.6809299] bias:  [-4.87523365]\n",
      "step:  1316 loss:  48.8615 Weight:  [ 1.67818367] bias:  [-4.85557222]\n",
      "step:  1317 loss:  48.4682 Weight:  [ 1.67544866] bias:  [-4.83599043]\n",
      "step:  1318 loss:  48.078 Weight:  [ 1.67272472] bias:  [-4.81648731]\n",
      "step:  1319 loss:  47.691 Weight:  [ 1.67001164] bias:  [-4.79706287]\n",
      "step:  1320 loss:  47.3071 Weight:  [ 1.66730952] bias:  [-4.77771711]\n",
      "step:  1321 loss:  46.9263 Weight:  [ 1.66461837] bias:  [-4.75844908]\n",
      "step:  1322 loss:  46.5486 Weight:  [ 1.66193807] bias:  [-4.73925877]\n",
      "step:  1323 loss:  46.1739 Weight:  [ 1.6592685] bias:  [-4.7201457]\n",
      "step:  1324 loss:  45.8022 Weight:  [ 1.65660977] bias:  [-4.70110989]\n",
      "step:  1325 loss:  45.4335 Weight:  [ 1.65396178] bias:  [-4.68215084]\n",
      "step:  1326 loss:  45.0678 Weight:  [ 1.65132439] bias:  [-4.66326809]\n",
      "step:  1327 loss:  44.705 Weight:  [ 1.64869761] bias:  [-4.64446163]\n",
      "step:  1328 loss:  44.3452 Weight:  [ 1.64608145] bias:  [-4.62573099]\n",
      "step:  1329 loss:  43.9882 Weight:  [ 1.64347589] bias:  [-4.60707617]\n",
      "step:  1330 loss:  43.6341 Weight:  [ 1.64088082] bias:  [-4.58849621]\n",
      "step:  1331 loss:  43.2829 Weight:  [ 1.63829625] bias:  [-4.56999159]\n",
      "step:  1332 loss:  42.9345 Weight:  [ 1.63572204] bias:  [-4.55156136]\n",
      "step:  1333 loss:  42.5889 Weight:  [ 1.63315833] bias:  [-4.53320551]\n",
      "step:  1334 loss:  42.2461 Weight:  [ 1.63060486] bias:  [-4.51492357]\n",
      "step:  1335 loss:  41.906 Weight:  [ 1.62806165] bias:  [-4.49671555]\n",
      "step:  1336 loss:  41.5687 Weight:  [ 1.62552881] bias:  [-4.47858095]\n",
      "step:  1337 loss:  41.2341 Weight:  [ 1.62300611] bias:  [-4.46051931]\n",
      "step:  1338 loss:  40.9022 Weight:  [ 1.62049365] bias:  [-4.44253063]\n",
      "step:  1339 loss:  40.573 Weight:  [ 1.61799133] bias:  [-4.42461443]\n",
      "step:  1340 loss:  40.2464 Weight:  [ 1.61549902] bias:  [-4.40677071]\n",
      "step:  1341 loss:  39.9224 Weight:  [ 1.61301672] bias:  [-4.38899851]\n",
      "step:  1342 loss:  39.601 Weight:  [ 1.61054456] bias:  [-4.37129831]\n",
      "step:  1343 loss:  39.2823 Weight:  [ 1.60808229] bias:  [-4.35366917]\n",
      "step:  1344 loss:  38.9661 Weight:  [ 1.60563004] bias:  [-4.33611155]\n",
      "step:  1345 loss:  38.6524 Weight:  [ 1.60318756] bias:  [-4.3186245]\n",
      "step:  1346 loss:  38.3413 Weight:  [ 1.60075498] bias:  [-4.30120802]\n",
      "step:  1347 loss:  38.0327 Weight:  [ 1.59833217] bias:  [-4.28386164]\n",
      "step:  1348 loss:  37.7265 Weight:  [ 1.59591913] bias:  [-4.26658535]\n",
      "step:  1349 loss:  37.4228 Weight:  [ 1.59351587] bias:  [-4.24937868]\n",
      "step:  1350 loss:  37.1216 Weight:  [ 1.59112227] bias:  [-4.23224163]\n",
      "step:  1351 loss:  36.8228 Weight:  [ 1.58873844] bias:  [-4.21517372]\n",
      "step:  1352 loss:  36.5264 Weight:  [ 1.58636415] bias:  [-4.19817448]\n",
      "step:  1353 loss:  36.2324 Weight:  [ 1.5839994] bias:  [-4.1812439]\n",
      "step:  1354 loss:  35.9407 Weight:  [ 1.58164418] bias:  [-4.1643815]\n",
      "step:  1355 loss:  35.6514 Weight:  [ 1.5792985] bias:  [-4.14758682]\n",
      "step:  1356 loss:  35.3644 Weight:  [ 1.57696223] bias:  [-4.13086033]\n",
      "step:  1357 loss:  35.0798 Weight:  [ 1.57463551] bias:  [-4.11420107]\n",
      "step:  1358 loss:  34.7974 Weight:  [ 1.57231808] bias:  [-4.09760904]\n",
      "step:  1359 loss:  34.5173 Weight:  [ 1.57000995] bias:  [-4.08108377]\n",
      "step:  1360 loss:  34.2395 Weight:  [ 1.56771111] bias:  [-4.06462526]\n",
      "step:  1361 loss:  33.9638 Weight:  [ 1.56542158] bias:  [-4.04823303]\n",
      "step:  1362 loss:  33.6904 Weight:  [ 1.56314135] bias:  [-4.03190708]\n",
      "step:  1363 loss:  33.4193 Weight:  [ 1.56087029] bias:  [-4.01564693]\n",
      "step:  1364 loss:  33.1502 Weight:  [ 1.55860841] bias:  [-3.99945235]\n",
      "step:  1365 loss:  32.8834 Weight:  [ 1.5563556] bias:  [-3.9833231]\n",
      "step:  1366 loss:  32.6187 Weight:  [ 1.55411184] bias:  [-3.96725893]\n",
      "step:  1367 loss:  32.3562 Weight:  [ 1.55187726] bias:  [-3.95125937]\n",
      "step:  1368 loss:  32.0957 Weight:  [ 1.54965162] bias:  [-3.93532443]\n",
      "step:  1369 loss:  31.8373 Weight:  [ 1.54743493] bias:  [-3.91945386]\n",
      "step:  1370 loss:  31.5811 Weight:  [ 1.54522717] bias:  [-3.90364718]\n",
      "step:  1371 loss:  31.3269 Weight:  [ 1.54302835] bias:  [-3.88790417]\n",
      "step:  1372 loss:  31.0747 Weight:  [ 1.54083836] bias:  [-3.87222481]\n",
      "step:  1373 loss:  30.8246 Weight:  [ 1.53865719] bias:  [-3.85660863]\n",
      "step:  1374 loss:  30.5764 Weight:  [ 1.53648484] bias:  [-3.84105539]\n",
      "step:  1375 loss:  30.3303 Weight:  [ 1.53432131] bias:  [-3.82556486]\n",
      "step:  1376 loss:  30.0862 Weight:  [ 1.53216648] bias:  [-3.8101368]\n",
      "step:  1377 loss:  29.844 Weight:  [ 1.53002024] bias:  [-3.79477096]\n",
      "step:  1378 loss:  29.6038 Weight:  [ 1.52788281] bias:  [-3.77946711]\n",
      "step:  1379 loss:  29.3655 Weight:  [ 1.52575386] bias:  [-3.76422501]\n",
      "step:  1380 loss:  29.1291 Weight:  [ 1.5236336] bias:  [-3.74904442]\n",
      "step:  1381 loss:  28.8946 Weight:  [ 1.52152181] bias:  [-3.73392487]\n",
      "step:  1382 loss:  28.662 Weight:  [ 1.5194186] bias:  [-3.71886635]\n",
      "step:  1383 loss:  28.4313 Weight:  [ 1.51732385] bias:  [-3.70386863]\n",
      "step:  1384 loss:  28.2025 Weight:  [ 1.51523757] bias:  [-3.68893147]\n",
      "step:  1385 loss:  27.9754 Weight:  [ 1.51315963] bias:  [-3.67405438]\n",
      "step:  1386 loss:  27.7503 Weight:  [ 1.51109016] bias:  [-3.65923738]\n",
      "step:  1387 loss:  27.5269 Weight:  [ 1.50902903] bias:  [-3.64448023]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step:  1388 loss:  27.3053 Weight:  [ 1.50697613] bias:  [-3.62978244]\n",
      "step:  1389 loss:  27.0855 Weight:  [ 1.50493157] bias:  [-3.61514401]\n",
      "step:  1390 loss:  26.8675 Weight:  [ 1.50289524] bias:  [-3.60056448]\n",
      "step:  1391 loss:  26.6512 Weight:  [ 1.50086713] bias:  [-3.58604383]\n",
      "step:  1392 loss:  26.4367 Weight:  [ 1.49884725] bias:  [-3.57158184]\n",
      "step:  1393 loss:  26.2239 Weight:  [ 1.49683547] bias:  [-3.55717802]\n",
      "step:  1394 loss:  26.0128 Weight:  [ 1.4948318] bias:  [-3.54283237]\n",
      "step:  1395 loss:  25.8034 Weight:  [ 1.49283612] bias:  [-3.52854466]\n",
      "step:  1396 loss:  25.5957 Weight:  [ 1.49084854] bias:  [-3.51431441]\n",
      "step:  1397 loss:  25.3897 Weight:  [ 1.48886907] bias:  [-3.50014162]\n",
      "step:  1398 loss:  25.1853 Weight:  [ 1.48689747] bias:  [-3.48602605]\n",
      "step:  1399 loss:  24.9826 Weight:  [ 1.48493385] bias:  [-3.47196722]\n",
      "step:  1400 loss:  24.7815 Weight:  [ 1.48297822] bias:  [-3.45796514]\n",
      "step:  1401 loss:  24.582 Weight:  [ 1.48103034] bias:  [-3.44401956]\n",
      "step:  1402 loss:  24.3841 Weight:  [ 1.47909045] bias:  [-3.43013024]\n",
      "step:  1403 loss:  24.1878 Weight:  [ 1.47715831] bias:  [-3.41629696]\n",
      "step:  1404 loss:  23.9931 Weight:  [ 1.47523403] bias:  [-3.40251946]\n",
      "step:  1405 loss:  23.8 Weight:  [ 1.4733175] bias:  [-3.38879752]\n",
      "step:  1406 loss:  23.6084 Weight:  [ 1.47140872] bias:  [-3.37513089]\n",
      "step:  1407 loss:  23.4184 Weight:  [ 1.46950746] bias:  [-3.36151934]\n",
      "step:  1408 loss:  23.2299 Weight:  [ 1.46761405] bias:  [-3.34796286]\n",
      "step:  1409 loss:  23.0429 Weight:  [ 1.46572816] bias:  [-3.33446097]\n",
      "step:  1410 loss:  22.8574 Weight:  [ 1.46385002] bias:  [-3.32101345]\n",
      "step:  1411 loss:  22.6734 Weight:  [ 1.46197939] bias:  [-3.30762029]\n",
      "step:  1412 loss:  22.4909 Weight:  [ 1.46011627] bias:  [-3.29428101]\n",
      "step:  1413 loss:  22.3099 Weight:  [ 1.45826066] bias:  [-3.28099561]\n",
      "step:  1414 loss:  22.1303 Weight:  [ 1.45641255] bias:  [-3.26776385]\n",
      "step:  1415 loss:  21.9521 Weight:  [ 1.45457196] bias:  [-3.25458527]\n",
      "step:  1416 loss:  21.7754 Weight:  [ 1.45273864] bias:  [-3.24145985]\n",
      "step:  1417 loss:  21.6002 Weight:  [ 1.45091283] bias:  [-3.22838736]\n",
      "step:  1418 loss:  21.4263 Weight:  [ 1.4490943] bias:  [-3.21536756]\n",
      "step:  1419 loss:  21.2538 Weight:  [ 1.44728315] bias:  [-3.20240045]\n",
      "step:  1420 loss:  21.0827 Weight:  [ 1.44547927] bias:  [-3.18948555]\n",
      "step:  1421 loss:  20.913 Weight:  [ 1.44368279] bias:  [-3.17662263]\n",
      "step:  1422 loss:  20.7447 Weight:  [ 1.44189346] bias:  [-3.16381168]\n",
      "step:  1423 loss:  20.5777 Weight:  [ 1.44011128] bias:  [-3.15105247]\n",
      "step:  1424 loss:  20.4121 Weight:  [ 1.43833637] bias:  [-3.13834476]\n",
      "step:  1425 loss:  20.2478 Weight:  [ 1.43656862] bias:  [-3.12568808]\n",
      "step:  1426 loss:  20.0848 Weight:  [ 1.43480802] bias:  [-3.11308265]\n",
      "step:  1427 loss:  19.9231 Weight:  [ 1.43305457] bias:  [-3.100528]\n",
      "step:  1428 loss:  19.7627 Weight:  [ 1.43130803] bias:  [-3.0880239]\n",
      "step:  1429 loss:  19.6037 Weight:  [ 1.42956865] bias:  [-3.07557034]\n",
      "step:  1430 loss:  19.4459 Weight:  [ 1.42783618] bias:  [-3.06316686]\n",
      "step:  1431 loss:  19.2893 Weight:  [ 1.42611074] bias:  [-3.05081344]\n",
      "step:  1432 loss:  19.1341 Weight:  [ 1.42439234] bias:  [-3.03850985]\n",
      "step:  1433 loss:  18.98 Weight:  [ 1.42268085] bias:  [-3.02625585]\n",
      "step:  1434 loss:  18.8273 Weight:  [ 1.42097616] bias:  [-3.01405144]\n",
      "step:  1435 loss:  18.6757 Weight:  [ 1.4192785] bias:  [-3.00189614]\n",
      "step:  1436 loss:  18.5254 Weight:  [ 1.41758752] bias:  [-2.98978996]\n",
      "step:  1437 loss:  18.3763 Weight:  [ 1.41590345] bias:  [-2.97773242]\n",
      "step:  1438 loss:  18.2283 Weight:  [ 1.41422617] bias:  [-2.96572351]\n",
      "step:  1439 loss:  18.0816 Weight:  [ 1.41255569] bias:  [-2.95376301]\n",
      "step:  1440 loss:  17.9361 Weight:  [ 1.41089189] bias:  [-2.9418509]\n",
      "step:  1441 loss:  17.7917 Weight:  [ 1.40923476] bias:  [-2.92998672]\n",
      "step:  1442 loss:  17.6485 Weight:  [ 1.40758443] bias:  [-2.91817045]\n",
      "step:  1443 loss:  17.5064 Weight:  [ 1.40594065] bias:  [-2.90640187]\n",
      "step:  1444 loss:  17.3655 Weight:  [ 1.40430355] bias:  [-2.89468074]\n",
      "step:  1445 loss:  17.2257 Weight:  [ 1.40267313] bias:  [-2.88300681]\n",
      "step:  1446 loss:  17.0871 Weight:  [ 1.40104914] bias:  [-2.87138009]\n",
      "step:  1447 loss:  16.9495 Weight:  [ 1.39943171] bias:  [-2.8598001]\n",
      "step:  1448 loss:  16.8131 Weight:  [ 1.39782095] bias:  [-2.84826684]\n",
      "step:  1449 loss:  16.6777 Weight:  [ 1.39621651] bias:  [-2.83678007]\n",
      "step:  1450 loss:  16.5435 Weight:  [ 1.39461863] bias:  [-2.82533979]\n",
      "step:  1451 loss:  16.4103 Weight:  [ 1.39302719] bias:  [-2.81394553]\n",
      "step:  1452 loss:  16.2782 Weight:  [ 1.39144218] bias:  [-2.80259728]\n",
      "step:  1453 loss:  16.1472 Weight:  [ 1.38986349] bias:  [-2.79129481]\n",
      "step:  1454 loss:  16.0172 Weight:  [ 1.38829124] bias:  [-2.78003788]\n",
      "step:  1455 loss:  15.8883 Weight:  [ 1.38672531] bias:  [-2.76882625]\n",
      "step:  1456 loss:  15.7604 Weight:  [ 1.38516569] bias:  [-2.75765991]\n",
      "step:  1457 loss:  15.6335 Weight:  [ 1.38361239] bias:  [-2.74653864]\n",
      "step:  1458 loss:  15.5077 Weight:  [ 1.3820653] bias:  [-2.73546219]\n",
      "step:  1459 loss:  15.3829 Weight:  [ 1.38052452] bias:  [-2.72443032]\n",
      "step:  1460 loss:  15.259 Weight:  [ 1.37898982] bias:  [-2.71344304]\n",
      "step:  1461 loss:  15.1362 Weight:  [ 1.37746143] bias:  [-2.7025001]\n",
      "step:  1462 loss:  15.0144 Weight:  [ 1.37593925] bias:  [-2.69160128]\n",
      "step:  1463 loss:  14.8935 Weight:  [ 1.37442315] bias:  [-2.68074632]\n",
      "step:  1464 loss:  14.7736 Weight:  [ 1.37291312] bias:  [-2.66993523]\n",
      "step:  1465 loss:  14.6547 Weight:  [ 1.37140918] bias:  [-2.65916777]\n",
      "step:  1466 loss:  14.5367 Weight:  [ 1.36991131] bias:  [-2.6484437]\n",
      "step:  1467 loss:  14.4197 Weight:  [ 1.36841953] bias:  [-2.63776278]\n",
      "step:  1468 loss:  14.3037 Weight:  [ 1.36693382] bias:  [-2.62712502]\n",
      "step:  1469 loss:  14.1885 Weight:  [ 1.36545396] bias:  [-2.61653018]\n",
      "step:  1470 loss:  14.0743 Weight:  [ 1.36398017] bias:  [-2.60597801]\n",
      "step:  1471 loss:  13.961 Weight:  [ 1.36251223] bias:  [-2.59546852]\n",
      "step:  1472 loss:  13.8486 Weight:  [ 1.36105025] bias:  [-2.58500123]\n",
      "step:  1473 loss:  13.7372 Weight:  [ 1.35959423] bias:  [-2.57457614]\n",
      "step:  1474 loss:  13.6266 Weight:  [ 1.35814404] bias:  [-2.56419325]\n",
      "step:  1475 loss:  13.5169 Weight:  [ 1.35669971] bias:  [-2.55385208]\n",
      "step:  1476 loss:  13.4081 Weight:  [ 1.35526109] bias:  [-2.54355264]\n",
      "step:  1477 loss:  13.3002 Weight:  [ 1.35382831] bias:  [-2.53329492]\n",
      "step:  1478 loss:  13.1931 Weight:  [ 1.35240138] bias:  [-2.52307844]\n",
      "step:  1479 loss:  13.0869 Weight:  [ 1.35098016] bias:  [-2.51290321]\n",
      "step:  1480 loss:  12.9816 Weight:  [ 1.34956479] bias:  [-2.50276899]\n",
      "step:  1481 loss:  12.8771 Weight:  [ 1.34815502] bias:  [-2.49267554]\n",
      "step:  1482 loss:  12.7734 Weight:  [ 1.34675097] bias:  [-2.48262286]\n",
      "step:  1483 loss:  12.6706 Weight:  [ 1.34535253] bias:  [-2.47261071]\n",
      "step:  1484 loss:  12.5686 Weight:  [ 1.34395981] bias:  [-2.46263885]\n",
      "step:  1485 loss:  12.4674 Weight:  [ 1.34257269] bias:  [-2.45270729]\n",
      "step:  1486 loss:  12.3671 Weight:  [ 1.34119105] bias:  [-2.44281578]\n",
      "step:  1487 loss:  12.2675 Weight:  [ 1.33981502] bias:  [-2.43296409]\n",
      "step:  1488 loss:  12.1688 Weight:  [ 1.33844459] bias:  [-2.42315221]\n",
      "step:  1489 loss:  12.0708 Weight:  [ 1.33707964] bias:  [-2.41337991]\n",
      "step:  1490 loss:  11.9737 Weight:  [ 1.3357203] bias:  [-2.40364695]\n",
      "step:  1491 loss:  11.8773 Weight:  [ 1.33436632] bias:  [-2.39395332]\n",
      "step:  1492 loss:  11.7817 Weight:  [ 1.33301795] bias:  [-2.3842988]\n",
      "step:  1493 loss:  11.6868 Weight:  [ 1.33167493] bias:  [-2.37468314]\n",
      "step:  1494 loss:  11.5928 Weight:  [ 1.33033729] bias:  [-2.36510634]\n",
      "step:  1495 loss:  11.4995 Weight:  [ 1.32900512] bias:  [-2.35556817]\n",
      "step:  1496 loss:  11.4069 Weight:  [ 1.3276782] bias:  [-2.34606838]\n",
      "step:  1497 loss:  11.3151 Weight:  [ 1.32635677] bias:  [-2.33660698]\n",
      "step:  1498 loss:  11.224 Weight:  [ 1.32504058] bias:  [-2.32718372]\n",
      "step:  1499 loss:  11.1336 Weight:  [ 1.32372975] bias:  [-2.31779838]\n",
      "step:  1500 loss:  11.044 Weight:  [ 1.32242417] bias:  [-2.30845094]\n",
      "step:  1501 loss:  10.9551 Weight:  [ 1.32112384] bias:  [-2.29914117]\n",
      "step:  1502 loss:  10.8669 Weight:  [ 1.31982875] bias:  [-2.28986907]\n",
      "step:  1503 loss:  10.7795 Weight:  [ 1.3185389] bias:  [-2.2806344]\n",
      "step:  1504 loss:  10.6927 Weight:  [ 1.3172543] bias:  [-2.27143693]\n",
      "step:  1505 loss:  10.6066 Weight:  [ 1.31597495] bias:  [-2.26227641]\n",
      "step:  1506 loss:  10.5212 Weight:  [ 1.3147006] bias:  [-2.25315285]\n",
      "step:  1507 loss:  10.4366 Weight:  [ 1.3134315] bias:  [-2.24406624]\n",
      "step:  1508 loss:  10.3525 Weight:  [ 1.31216741] bias:  [-2.23501611]\n",
      "step:  1509 loss:  10.2692 Weight:  [ 1.31090844] bias:  [-2.22600245]\n",
      "step:  1510 loss:  10.1865 Weight:  [ 1.30965459] bias:  [-2.21702528]\n",
      "step:  1511 loss:  10.1046 Weight:  [ 1.30840576] bias:  [-2.20808434]\n",
      "step:  1512 loss:  10.0232 Weight:  [ 1.30716205] bias:  [-2.19917941]\n",
      "step:  1513 loss:  9.94253 Weight:  [ 1.30592334] bias:  [-2.19031048]\n",
      "step:  1514 loss:  9.8625 Weight:  [ 1.30468953] bias:  [-2.18147731]\n",
      "step:  1515 loss:  9.78311 Weight:  [ 1.30346084] bias:  [-2.17267966]\n",
      "step:  1516 loss:  9.70437 Weight:  [ 1.30223703] bias:  [-2.16391754]\n",
      "step:  1517 loss:  9.62625 Weight:  [ 1.30101812] bias:  [-2.15519071]\n",
      "step:  1518 loss:  9.54877 Weight:  [ 1.29980409] bias:  [-2.14649916]\n",
      "step:  1519 loss:  9.4719 Weight:  [ 1.29859507] bias:  [-2.13784266]\n",
      "step:  1520 loss:  9.39566 Weight:  [ 1.29739094] bias:  [-2.12922096]\n",
      "step:  1521 loss:  9.32003 Weight:  [ 1.29619157] bias:  [-2.12063408]\n",
      "step:  1522 loss:  9.24501 Weight:  [ 1.2949971] bias:  [-2.11208177]\n",
      "step:  1523 loss:  9.17059 Weight:  [ 1.29380739] bias:  [-2.10356402]\n",
      "step:  1524 loss:  9.09677 Weight:  [ 1.29262245] bias:  [-2.09508061]\n",
      "step:  1525 loss:  9.02355 Weight:  [ 1.29144228] bias:  [-2.0866313]\n",
      "step:  1526 loss:  8.95091 Weight:  [ 1.29026699] bias:  [-2.07821608]\n",
      "step:  1527 loss:  8.87886 Weight:  [ 1.28909636] bias:  [-2.06983495]\n",
      "step:  1528 loss:  8.80739 Weight:  [ 1.28793049] bias:  [-2.06148767]\n",
      "step:  1529 loss:  8.7365 Weight:  [ 1.28676927] bias:  [-2.05317402]\n",
      "step:  1530 loss:  8.66617 Weight:  [ 1.28561282] bias:  [-2.04489374]\n",
      "step:  1531 loss:  8.59641 Weight:  [ 1.2844609] bias:  [-2.03664684]\n",
      "step:  1532 loss:  8.52722 Weight:  [ 1.28331375] bias:  [-2.02843332]\n",
      "step:  1533 loss:  8.45858 Weight:  [ 1.28217113] bias:  [-2.02025294]\n",
      "step:  1534 loss:  8.39049 Weight:  [ 1.28103328] bias:  [-2.01210546]\n",
      "step:  1535 loss:  8.32295 Weight:  [ 1.27989984] bias:  [-2.00399089]\n",
      "step:  1536 loss:  8.25595 Weight:  [ 1.27877104] bias:  [-1.99590898]\n",
      "step:  1537 loss:  8.1895 Weight:  [ 1.27764678] bias:  [-1.98785973]\n",
      "step:  1538 loss:  8.12358 Weight:  [ 1.27652705] bias:  [-1.9798429]\n",
      "step:  1539 loss:  8.05818 Weight:  [ 1.27541184] bias:  [-1.97185838]\n",
      "step:  1540 loss:  7.99332 Weight:  [ 1.27430117] bias:  [-1.96390617]\n",
      "step:  1541 loss:  7.92898 Weight:  [ 1.27319491] bias:  [-1.95598602]\n",
      "step:  1542 loss:  7.86515 Weight:  [ 1.27209318] bias:  [-1.94809771]\n",
      "step:  1543 loss:  7.80184 Weight:  [ 1.27099586] bias:  [-1.94024122]\n",
      "step:  1544 loss:  7.73904 Weight:  [ 1.26990294] bias:  [-1.93241644]\n",
      "step:  1545 loss:  7.67675 Weight:  [ 1.26881444] bias:  [-1.92462325]\n",
      "step:  1546 loss:  7.61495 Weight:  [ 1.26773036] bias:  [-1.91686153]\n",
      "step:  1547 loss:  7.55366 Weight:  [ 1.26665068] bias:  [-1.90913105]\n",
      "step:  1548 loss:  7.49285 Weight:  [ 1.26557529] bias:  [-1.9014318]\n",
      "step:  1549 loss:  7.43254 Weight:  [ 1.26450431] bias:  [-1.89376354]\n",
      "step:  1550 loss:  7.37271 Weight:  [ 1.26343751] bias:  [-1.88612628]\n",
      "step:  1551 loss:  7.31336 Weight:  [ 1.26237512] bias:  [-1.87851977]\n",
      "step:  1552 loss:  7.2545 Weight:  [ 1.26131701] bias:  [-1.8709439]\n",
      "step:  1553 loss:  7.1961 Weight:  [ 1.2602632] bias:  [-1.86339867]\n",
      "step:  1554 loss:  7.13818 Weight:  [ 1.25921357] bias:  [-1.85588384]\n",
      "step:  1555 loss:  7.08072 Weight:  [ 1.25816822] bias:  [-1.84839928]\n",
      "step:  1556 loss:  7.02372 Weight:  [ 1.25712705] bias:  [-1.84094489]\n",
      "step:  1557 loss:  6.96718 Weight:  [ 1.25609004] bias:  [-1.83352053]\n",
      "step:  1558 loss:  6.9111 Weight:  [ 1.25505733] bias:  [-1.82612622]\n",
      "step:  1559 loss:  6.85547 Weight:  [ 1.25402868] bias:  [-1.81876171]\n",
      "step:  1560 loss:  6.80029 Weight:  [ 1.25300419] bias:  [-1.81142688]\n",
      "step:  1561 loss:  6.74555 Weight:  [ 1.25198388] bias:  [-1.80412161]\n",
      "step:  1562 loss:  6.69125 Weight:  [ 1.25096762] bias:  [-1.79684579]\n",
      "step:  1563 loss:  6.63739 Weight:  [ 1.24995553] bias:  [-1.7895993]\n",
      "step:  1564 loss:  6.58396 Weight:  [ 1.2489475] bias:  [-1.78238201]\n",
      "step:  1565 loss:  6.53097 Weight:  [ 1.24794352] bias:  [-1.77519393]\n",
      "step:  1566 loss:  6.47839 Weight:  [ 1.24694359] bias:  [-1.76803482]\n",
      "step:  1567 loss:  6.42625 Weight:  [ 1.24594772] bias:  [-1.76090455]\n",
      "step:  1568 loss:  6.37452 Weight:  [ 1.24495578] bias:  [-1.75380301]\n",
      "step:  1569 loss:  6.32321 Weight:  [ 1.24396789] bias:  [-1.74673009]\n",
      "step:  1570 loss:  6.27231 Weight:  [ 1.24298406] bias:  [-1.73968577]\n",
      "step:  1571 loss:  6.22182 Weight:  [ 1.24200416] bias:  [-1.73266983]\n",
      "step:  1572 loss:  6.17174 Weight:  [ 1.24102819] bias:  [-1.72568214]\n",
      "step:  1573 loss:  6.12206 Weight:  [ 1.24005616] bias:  [-1.7187227]\n",
      "step:  1574 loss:  6.07278 Weight:  [ 1.23908794] bias:  [-1.71179128]\n",
      "step:  1575 loss:  6.0239 Weight:  [ 1.23812377] bias:  [-1.70488787]\n",
      "step:  1576 loss:  5.97541 Weight:  [ 1.23716342] bias:  [-1.69801223]\n",
      "step:  1577 loss:  5.92731 Weight:  [ 1.23620701] bias:  [-1.69116437]\n",
      "step:  1578 loss:  5.8796 Weight:  [ 1.23525441] bias:  [-1.68434417]\n",
      "step:  1579 loss:  5.83227 Weight:  [ 1.23430562] bias:  [-1.67755139]\n",
      "step:  1580 loss:  5.78532 Weight:  [ 1.23336077] bias:  [-1.67078602]\n",
      "step:  1581 loss:  5.73875 Weight:  [ 1.23241961] bias:  [-1.66404796]\n",
      "step:  1582 loss:  5.69256 Weight:  [ 1.23148227] bias:  [-1.65733707]\n",
      "step:  1583 loss:  5.64674 Weight:  [ 1.23054874] bias:  [-1.65065324]\n",
      "step:  1584 loss:  5.60128 Weight:  [ 1.22961891] bias:  [-1.64399636]\n",
      "step:  1585 loss:  5.5562 Weight:  [ 1.22869289] bias:  [-1.63736629]\n",
      "step:  1586 loss:  5.51147 Weight:  [ 1.22777069] bias:  [-1.63076293]\n",
      "step:  1587 loss:  5.46711 Weight:  [ 1.22685206] bias:  [-1.62418628]\n",
      "step:  1588 loss:  5.4231 Weight:  [ 1.22593725] bias:  [-1.61763608]\n",
      "step:  1589 loss:  5.37945 Weight:  [ 1.22502601] bias:  [-1.61111236]\n",
      "step:  1590 loss:  5.33614 Weight:  [ 1.22411859] bias:  [-1.60461497]\n",
      "step:  1591 loss:  5.29319 Weight:  [ 1.22321475] bias:  [-1.59814382]\n",
      "step:  1592 loss:  5.25058 Weight:  [ 1.22231448] bias:  [-1.59169865]\n",
      "step:  1593 loss:  5.20832 Weight:  [ 1.2214179] bias:  [-1.58527946]\n",
      "step:  1594 loss:  5.16639 Weight:  [ 1.22052491] bias:  [-1.57888627]\n",
      "step:  1595 loss:  5.12481 Weight:  [ 1.21963561] bias:  [-1.57251883]\n",
      "step:  1596 loss:  5.08355 Weight:  [ 1.21874988] bias:  [-1.56617701]\n",
      "step:  1597 loss:  5.04263 Weight:  [ 1.21786773] bias:  [-1.55986083]\n",
      "step:  1598 loss:  5.00204 Weight:  [ 1.21698904] bias:  [-1.55357015]\n",
      "step:  1599 loss:  4.96178 Weight:  [ 1.21611392] bias:  [-1.54730475]\n",
      "step:  1600 loss:  4.92184 Weight:  [ 1.21524239] bias:  [-1.54106462]\n",
      "step:  1601 loss:  4.88222 Weight:  [ 1.2143743] bias:  [-1.53484964]\n",
      "step:  1602 loss:  4.84292 Weight:  [ 1.21350968] bias:  [-1.52865982]\n",
      "step:  1603 loss:  4.80394 Weight:  [ 1.21264875] bias:  [-1.52249491]\n",
      "step:  1604 loss:  4.76527 Weight:  [ 1.21179116] bias:  [-1.51635492]\n",
      "step:  1605 loss:  4.72691 Weight:  [ 1.21093702] bias:  [-1.5102396]\n",
      "step:  1606 loss:  4.68886 Weight:  [ 1.21008635] bias:  [-1.50414896]\n",
      "step:  1607 loss:  4.65112 Weight:  [ 1.20923901] bias:  [-1.49808288]\n",
      "step:  1608 loss:  4.61368 Weight:  [ 1.20839524] bias:  [-1.49204123]\n",
      "step:  1609 loss:  4.57654 Weight:  [ 1.20755482] bias:  [-1.48602402]\n",
      "step:  1610 loss:  4.5397 Weight:  [ 1.20671773] bias:  [-1.48003101]\n",
      "step:  1611 loss:  4.50316 Weight:  [ 1.2058841] bias:  [-1.4740622]\n",
      "step:  1612 loss:  4.46691 Weight:  [ 1.20505381] bias:  [-1.46811748]\n",
      "step:  1613 loss:  4.43095 Weight:  [ 1.20422685] bias:  [-1.46219671]\n",
      "step:  1614 loss:  4.39529 Weight:  [ 1.20340312] bias:  [-1.4562999]\n",
      "step:  1615 loss:  4.35991 Weight:  [ 1.20258284] bias:  [-1.45042682]\n",
      "step:  1616 loss:  4.32481 Weight:  [ 1.20176589] bias:  [-1.44457746]\n",
      "step:  1617 loss:  4.29 Weight:  [ 1.20095217] bias:  [-1.4387517]\n",
      "step:  1618 loss:  4.25547 Weight:  [ 1.20014179] bias:  [-1.43294942]\n",
      "step:  1619 loss:  4.22122 Weight:  [ 1.19933462] bias:  [-1.42717052]\n",
      "step:  1620 loss:  4.18724 Weight:  [ 1.19853079] bias:  [-1.42141485]\n",
      "step:  1621 loss:  4.15353 Weight:  [ 1.19773006] bias:  [-1.41568244]\n",
      "step:  1622 loss:  4.1201 Weight:  [ 1.19693267] bias:  [-1.40997314]\n",
      "step:  1623 loss:  4.08693 Weight:  [ 1.1961385] bias:  [-1.40428686]\n",
      "step:  1624 loss:  4.05404 Weight:  [ 1.19534743] bias:  [-1.39862359]\n",
      "step:  1625 loss:  4.0214 Weight:  [ 1.19455957] bias:  [-1.39298308]\n",
      "step:  1626 loss:  3.98903 Weight:  [ 1.19377506] bias:  [-1.38736534]\n",
      "step:  1627 loss:  3.95692 Weight:  [ 1.19299352] bias:  [-1.38177025]\n",
      "step:  1628 loss:  3.92507 Weight:  [ 1.1922152] bias:  [-1.3761977]\n",
      "step:  1629 loss:  3.89347 Weight:  [ 1.19144011] bias:  [-1.37064767]\n",
      "step:  1630 loss:  3.86214 Weight:  [ 1.19066799] bias:  [-1.36512005]\n",
      "step:  1631 loss:  3.83105 Weight:  [ 1.18989909] bias:  [-1.35961473]\n",
      "step:  1632 loss:  3.80021 Weight:  [ 1.18913317] bias:  [-1.35413158]\n",
      "step:  1633 loss:  3.76962 Weight:  [ 1.18837047] bias:  [-1.34867048]\n",
      "step:  1634 loss:  3.73928 Weight:  [ 1.18761075] bias:  [-1.34323144]\n",
      "step:  1635 loss:  3.70918 Weight:  [ 1.18685412] bias:  [-1.33781433]\n",
      "step:  1636 loss:  3.67932 Weight:  [ 1.1861006] bias:  [-1.33241904]\n",
      "step:  1637 loss:  3.6497 Weight:  [ 1.18535006] bias:  [-1.32704556]\n",
      "step:  1638 loss:  3.62032 Weight:  [ 1.18460262] bias:  [-1.32169378]\n",
      "step:  1639 loss:  3.59118 Weight:  [ 1.18385804] bias:  [-1.31636357]\n",
      "step:  1640 loss:  3.56227 Weight:  [ 1.18311656] bias:  [-1.31105483]\n",
      "step:  1641 loss:  3.5336 Weight:  [ 1.18237817] bias:  [-1.30576754]\n",
      "step:  1642 loss:  3.50516 Weight:  [ 1.18164265] bias:  [-1.30050159]\n",
      "step:  1643 loss:  3.47694 Weight:  [ 1.18091011] bias:  [-1.29525685]\n",
      "step:  1644 loss:  3.44896 Weight:  [ 1.18018055] bias:  [-1.29003322]\n",
      "step:  1645 loss:  3.42119 Weight:  [ 1.17945385] bias:  [-1.28483069]\n",
      "step:  1646 loss:  3.39365 Weight:  [ 1.17873013] bias:  [-1.27964914]\n",
      "step:  1647 loss:  3.36634 Weight:  [ 1.17800939] bias:  [-1.27448845]\n",
      "step:  1648 loss:  3.33924 Weight:  [ 1.17729151] bias:  [-1.26934862]\n",
      "step:  1649 loss:  3.31236 Weight:  [ 1.1765765] bias:  [-1.26422954]\n",
      "step:  1650 loss:  3.2857 Weight:  [ 1.17586434] bias:  [-1.25913107]\n",
      "step:  1651 loss:  3.25925 Weight:  [ 1.17515516] bias:  [-1.25405312]\n",
      "step:  1652 loss:  3.23301 Weight:  [ 1.17444873] bias:  [-1.24899566]\n",
      "step:  1653 loss:  3.20699 Weight:  [ 1.17374516] bias:  [-1.24395859]\n",
      "step:  1654 loss:  3.18118 Weight:  [ 1.17304444] bias:  [-1.23894191]\n",
      "step:  1655 loss:  3.15557 Weight:  [ 1.17234671] bias:  [-1.23394537]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step:  1656 loss:  3.13017 Weight:  [ 1.1716516] bias:  [-1.22896898]\n",
      "step:  1657 loss:  3.10497 Weight:  [ 1.17095935] bias:  [-1.22401273]\n",
      "step:  1658 loss:  3.07998 Weight:  [ 1.17026985] bias:  [-1.2190764]\n",
      "step:  1659 loss:  3.05519 Weight:  [ 1.1695832] bias:  [-1.21415997]\n",
      "step:  1660 loss:  3.03059 Weight:  [ 1.1688993] bias:  [-1.20926344]\n",
      "step:  1661 loss:  3.0062 Weight:  [ 1.16821814] bias:  [-1.20438659]\n",
      "step:  1662 loss:  2.982 Weight:  [ 1.16753972] bias:  [-1.19952941]\n",
      "step:  1663 loss:  2.958 Weight:  [ 1.16686404] bias:  [-1.1946919]\n",
      "step:  1664 loss:  2.93419 Weight:  [ 1.1661911] bias:  [-1.18987381]\n",
      "step:  1665 loss:  2.91057 Weight:  [ 1.16552091] bias:  [-1.18507516]\n",
      "step:  1666 loss:  2.88714 Weight:  [ 1.16485333] bias:  [-1.18029594]\n",
      "step:  1667 loss:  2.8639 Weight:  [ 1.1641885] bias:  [-1.17553592]\n",
      "step:  1668 loss:  2.84084 Weight:  [ 1.16352642] bias:  [-1.17079508]\n",
      "step:  1669 loss:  2.81798 Weight:  [ 1.16286695] bias:  [-1.16607344]\n",
      "step:  1670 loss:  2.79529 Weight:  [ 1.16221011] bias:  [-1.16137075]\n",
      "step:  1671 loss:  2.77279 Weight:  [ 1.16155589] bias:  [-1.15668714]\n",
      "step:  1672 loss:  2.75048 Weight:  [ 1.16090429] bias:  [-1.15202236]\n",
      "step:  1673 loss:  2.72833 Weight:  [ 1.16025543] bias:  [-1.14737642]\n",
      "step:  1674 loss:  2.70637 Weight:  [ 1.1596092] bias:  [-1.14274919]\n",
      "step:  1675 loss:  2.68459 Weight:  [ 1.15896547] bias:  [-1.13814068]\n",
      "step:  1676 loss:  2.66298 Weight:  [ 1.15832436] bias:  [-1.13355064]\n",
      "step:  1677 loss:  2.64154 Weight:  [ 1.15768588] bias:  [-1.12897921]\n",
      "step:  1678 loss:  2.62028 Weight:  [ 1.15705001] bias:  [-1.12442613]\n",
      "step:  1679 loss:  2.59919 Weight:  [ 1.15641665] bias:  [-1.11989141]\n",
      "step:  1680 loss:  2.57827 Weight:  [ 1.1557858] bias:  [-1.11537504]\n",
      "step:  1681 loss:  2.55751 Weight:  [ 1.15515757] bias:  [-1.11087692]\n",
      "step:  1682 loss:  2.53693 Weight:  [ 1.15453184] bias:  [-1.10639691]\n",
      "step:  1683 loss:  2.51651 Weight:  [ 1.15390861] bias:  [-1.10193491]\n",
      "step:  1684 loss:  2.49625 Weight:  [ 1.15328789] bias:  [-1.09749091]\n",
      "step:  1685 loss:  2.47615 Weight:  [ 1.15266967] bias:  [-1.0930649]\n",
      "step:  1686 loss:  2.45622 Weight:  [ 1.15205395] bias:  [-1.08865666]\n",
      "step:  1687 loss:  2.43645 Weight:  [ 1.15144074] bias:  [-1.08426619]\n",
      "step:  1688 loss:  2.41684 Weight:  [ 1.15083003] bias:  [-1.07989347]\n",
      "step:  1689 loss:  2.39739 Weight:  [ 1.15022171] bias:  [-1.0755384]\n",
      "step:  1690 loss:  2.37809 Weight:  [ 1.14961588] bias:  [-1.07120085]\n",
      "step:  1691 loss:  2.35894 Weight:  [ 1.14901257] bias:  [-1.06688082]\n",
      "step:  1692 loss:  2.33995 Weight:  [ 1.14841163] bias:  [-1.0625782]\n",
      "step:  1693 loss:  2.32112 Weight:  [ 1.14781308] bias:  [-1.05829298]\n",
      "step:  1694 loss:  2.30244 Weight:  [ 1.14721704] bias:  [-1.05402505]\n",
      "step:  1695 loss:  2.2839 Weight:  [ 1.14662325] bias:  [-1.04977429]\n",
      "step:  1696 loss:  2.26552 Weight:  [ 1.14603198] bias:  [-1.04554069]\n",
      "step:  1697 loss:  2.24728 Weight:  [ 1.14544308] bias:  [-1.04132414]\n",
      "step:  1698 loss:  2.22919 Weight:  [ 1.14485645] bias:  [-1.03712463]\n",
      "step:  1699 loss:  2.21125 Weight:  [ 1.14427233] bias:  [-1.03294206]\n",
      "step:  1700 loss:  2.19345 Weight:  [ 1.14369047] bias:  [-1.02877629]\n",
      "step:  1701 loss:  2.17579 Weight:  [ 1.14311099] bias:  [-1.02462733]\n",
      "step:  1702 loss:  2.15828 Weight:  [ 1.14253378] bias:  [-1.02049518]\n",
      "step:  1703 loss:  2.14091 Weight:  [ 1.14195895] bias:  [-1.01637959]\n",
      "step:  1704 loss:  2.12367 Weight:  [ 1.14138639] bias:  [-1.0122807]\n",
      "step:  1705 loss:  2.10658 Weight:  [ 1.14081621] bias:  [-1.00819826]\n",
      "step:  1706 loss:  2.08962 Weight:  [ 1.14024842] bias:  [-1.00413227]\n",
      "step:  1707 loss:  2.0728 Weight:  [ 1.13968277] bias:  [-1.00008273]\n",
      "step:  1708 loss:  2.05612 Weight:  [ 1.13911951] bias:  [-0.99604952]\n",
      "step:  1709 loss:  2.03956 Weight:  [ 1.13855839] bias:  [-0.99203259]\n",
      "step:  1710 loss:  2.02315 Weight:  [ 1.13799965] bias:  [-0.98803186]\n",
      "step:  1711 loss:  2.00686 Weight:  [ 1.13744307] bias:  [-0.98404723]\n",
      "step:  1712 loss:  1.99071 Weight:  [ 1.13688874] bias:  [-0.9800787]\n",
      "step:  1713 loss:  1.97468 Weight:  [ 1.13633668] bias:  [-0.97612613]\n",
      "step:  1714 loss:  1.95879 Weight:  [ 1.13578689] bias:  [-0.97218955]\n",
      "step:  1715 loss:  1.94302 Weight:  [ 1.13523924] bias:  [-0.96826881]\n",
      "step:  1716 loss:  1.92738 Weight:  [ 1.13469386] bias:  [-0.96436387]\n",
      "step:  1717 loss:  1.91187 Weight:  [ 1.13415062] bias:  [-0.96047473]\n",
      "step:  1718 loss:  1.89648 Weight:  [ 1.13360965] bias:  [-0.95660126]\n",
      "step:  1719 loss:  1.88121 Weight:  [ 1.13307083] bias:  [-0.95274341]\n",
      "step:  1720 loss:  1.86607 Weight:  [ 1.13253415] bias:  [-0.94890112]\n",
      "step:  1721 loss:  1.85105 Weight:  [ 1.13199973] bias:  [-0.94507432]\n",
      "step:  1722 loss:  1.83615 Weight:  [ 1.13146734] bias:  [-0.94126296]\n",
      "step:  1723 loss:  1.82137 Weight:  [ 1.1309371] bias:  [-0.93746698]\n",
      "step:  1724 loss:  1.80671 Weight:  [ 1.13040912] bias:  [-0.93368626]\n",
      "step:  1725 loss:  1.79216 Weight:  [ 1.12988317] bias:  [-0.92992079]\n",
      "step:  1726 loss:  1.77774 Weight:  [ 1.12935936] bias:  [-0.92617053]\n",
      "step:  1727 loss:  1.76343 Weight:  [ 1.1288377] bias:  [-0.9224354]\n",
      "step:  1728 loss:  1.74923 Weight:  [ 1.12831807] bias:  [-0.91871536]\n",
      "step:  1729 loss:  1.73515 Weight:  [ 1.12780058] bias:  [-0.91501027]\n",
      "step:  1730 loss:  1.72119 Weight:  [ 1.12728524] bias:  [-0.91132015]\n",
      "step:  1731 loss:  1.70733 Weight:  [ 1.12677193] bias:  [-0.90764493]\n",
      "step:  1732 loss:  1.69359 Weight:  [ 1.12626064] bias:  [-0.90398449]\n",
      "step:  1733 loss:  1.67996 Weight:  [ 1.12575138] bias:  [-0.90033883]\n",
      "step:  1734 loss:  1.66643 Weight:  [ 1.12524426] bias:  [-0.89670789]\n",
      "step:  1735 loss:  1.65302 Weight:  [ 1.12473917] bias:  [-0.89309156]\n",
      "step:  1736 loss:  1.63971 Weight:  [ 1.12423611] bias:  [-0.88948983]\n",
      "step:  1737 loss:  1.62651 Weight:  [ 1.12373507] bias:  [-0.88590264]\n",
      "step:  1738 loss:  1.61342 Weight:  [ 1.12323606] bias:  [-0.88232988]\n",
      "step:  1739 loss:  1.60043 Weight:  [ 1.12273908] bias:  [-0.87877154]\n",
      "step:  1740 loss:  1.58755 Weight:  [ 1.12224412] bias:  [-0.87522757]\n",
      "step:  1741 loss:  1.57477 Weight:  [ 1.12175119] bias:  [-0.8716979]\n",
      "step:  1742 loss:  1.5621 Weight:  [ 1.12126017] bias:  [-0.86818248]\n",
      "step:  1743 loss:  1.54952 Weight:  [ 1.12077105] bias:  [-0.86468118]\n",
      "step:  1744 loss:  1.53705 Weight:  [ 1.12028396] bias:  [-0.86119401]\n",
      "step:  1745 loss:  1.52468 Weight:  [ 1.1197989] bias:  [-0.85772091]\n",
      "step:  1746 loss:  1.5124 Weight:  [ 1.11931586] bias:  [-0.85426182]\n",
      "step:  1747 loss:  1.50023 Weight:  [ 1.11883461] bias:  [-0.85081667]\n",
      "step:  1748 loss:  1.48815 Weight:  [ 1.11835539] bias:  [-0.84738541]\n",
      "step:  1749 loss:  1.47617 Weight:  [ 1.11787808] bias:  [-0.84396803]\n",
      "step:  1750 loss:  1.46429 Weight:  [ 1.11740267] bias:  [-0.84056443]\n",
      "step:  1751 loss:  1.45251 Weight:  [ 1.11692917] bias:  [-0.83717453]\n",
      "step:  1752 loss:  1.44081 Weight:  [ 1.11645758] bias:  [-0.83379829]\n",
      "step:  1753 loss:  1.42922 Weight:  [ 1.1159879] bias:  [-0.83043569]\n",
      "step:  1754 loss:  1.41771 Weight:  [ 1.11552024] bias:  [-0.82708663]\n",
      "step:  1755 loss:  1.4063 Weight:  [ 1.11505425] bias:  [-0.82375109]\n",
      "step:  1756 loss:  1.39498 Weight:  [ 1.11459029] bias:  [-0.82042897]\n",
      "step:  1757 loss:  1.38375 Weight:  [ 1.11412811] bias:  [-0.81712025]\n",
      "step:  1758 loss:  1.37261 Weight:  [ 1.11366796] bias:  [-0.81382489]\n",
      "step:  1759 loss:  1.36156 Weight:  [ 1.11320949] bias:  [-0.81054282]\n",
      "step:  1760 loss:  1.3506 Weight:  [ 1.11275291] bias:  [-0.80727398]\n",
      "step:  1761 loss:  1.33973 Weight:  [ 1.11229825] bias:  [-0.80401832]\n",
      "step:  1762 loss:  1.32895 Weight:  [ 1.11184537] bias:  [-0.80077583]\n",
      "step:  1763 loss:  1.31825 Weight:  [ 1.11139429] bias:  [-0.79754639]\n",
      "step:  1764 loss:  1.30764 Weight:  [ 1.11094499] bias:  [-0.79433]\n",
      "step:  1765 loss:  1.29711 Weight:  [ 1.11049759] bias:  [-0.79112655]\n",
      "step:  1766 loss:  1.28667 Weight:  [ 1.11005199] bias:  [-0.78793603]\n",
      "step:  1767 loss:  1.27631 Weight:  [ 1.10960817] bias:  [-0.78475839]\n",
      "step:  1768 loss:  1.26604 Weight:  [ 1.10916615] bias:  [-0.78159356]\n",
      "step:  1769 loss:  1.25585 Weight:  [ 1.10872591] bias:  [-0.77844149]\n",
      "step:  1770 loss:  1.24574 Weight:  [ 1.10828745] bias:  [-0.77530211]\n",
      "step:  1771 loss:  1.23571 Weight:  [ 1.10785067] bias:  [-0.77217543]\n",
      "step:  1772 loss:  1.22577 Weight:  [ 1.1074158] bias:  [-0.76906133]\n",
      "step:  1773 loss:  1.2159 Weight:  [ 1.10698259] bias:  [-0.7659598]\n",
      "step:  1774 loss:  1.20611 Weight:  [ 1.10655105] bias:  [-0.76287079]\n",
      "step:  1775 loss:  1.1964 Weight:  [ 1.10612142] bias:  [-0.75979424]\n",
      "step:  1776 loss:  1.18677 Weight:  [ 1.10569346] bias:  [-0.75673008]\n",
      "step:  1777 loss:  1.17722 Weight:  [ 1.10526717] bias:  [-0.75367826]\n",
      "step:  1778 loss:  1.16774 Weight:  [ 1.10484266] bias:  [-0.75063878]\n",
      "step:  1779 loss:  1.15834 Weight:  [ 1.10441983] bias:  [-0.74761152]\n",
      "step:  1780 loss:  1.14902 Weight:  [ 1.10399866] bias:  [-0.74459648]\n",
      "step:  1781 loss:  1.13977 Weight:  [ 1.10357928] bias:  [-0.7415936]\n",
      "step:  1782 loss:  1.1306 Weight:  [ 1.10316157] bias:  [-0.73860282]\n",
      "step:  1783 loss:  1.1215 Weight:  [ 1.10274553] bias:  [-0.73562413]\n",
      "step:  1784 loss:  1.11247 Weight:  [ 1.10233116] bias:  [-0.73265743]\n",
      "step:  1785 loss:  1.10351 Weight:  [ 1.10191846] bias:  [-0.72970271]\n",
      "step:  1786 loss:  1.09463 Weight:  [ 1.10150743] bias:  [-0.72675991]\n",
      "step:  1787 loss:  1.08582 Weight:  [ 1.10109806] bias:  [-0.72382897]\n",
      "step:  1788 loss:  1.07708 Weight:  [ 1.10069036] bias:  [-0.72090983]\n",
      "step:  1789 loss:  1.06841 Weight:  [ 1.10028434] bias:  [-0.7180025]\n",
      "step:  1790 loss:  1.05981 Weight:  [ 1.09987986] bias:  [-0.7151069]\n",
      "step:  1791 loss:  1.05128 Weight:  [ 1.09947705] bias:  [-0.71222299]\n",
      "step:  1792 loss:  1.04282 Weight:  [ 1.09907591] bias:  [-0.70935071]\n",
      "step:  1793 loss:  1.03442 Weight:  [ 1.09867632] bias:  [-0.70648998]\n",
      "step:  1794 loss:  1.0261 Weight:  [ 1.09827828] bias:  [-0.70364082]\n",
      "step:  1795 loss:  1.01784 Weight:  [ 1.09788203] bias:  [-0.7008031]\n",
      "step:  1796 loss:  1.00964 Weight:  [ 1.09748721] bias:  [-0.69797683]\n",
      "step:  1797 loss:  1.00152 Weight:  [ 1.09709406] bias:  [-0.69516194]\n",
      "step:  1798 loss:  0.993454 Weight:  [ 1.09670258] bias:  [-0.69235843]\n",
      "step:  1799 loss:  0.985458 Weight:  [ 1.09631252] bias:  [-0.68956625]\n",
      "step:  1800 loss:  0.977524 Weight:  [ 1.09592414] bias:  [-0.68678534]\n",
      "step:  1801 loss:  0.969656 Weight:  [ 1.0955373] bias:  [-0.68401563]\n",
      "step:  1802 loss:  0.961851 Weight:  [ 1.09515202] bias:  [-0.68125707]\n",
      "step:  1803 loss:  0.954108 Weight:  [ 1.09476829] bias:  [-0.67850965]\n",
      "step:  1804 loss:  0.946429 Weight:  [ 1.0943861] bias:  [-0.67577332]\n",
      "step:  1805 loss:  0.93881 Weight:  [ 1.09400547] bias:  [-0.67304802]\n",
      "step:  1806 loss:  0.931254 Weight:  [ 1.09362638] bias:  [-0.67033368]\n",
      "step:  1807 loss:  0.923757 Weight:  [ 1.09324872] bias:  [-0.66763031]\n",
      "step:  1808 loss:  0.916322 Weight:  [ 1.09287262] bias:  [-0.66493785]\n",
      "step:  1809 loss:  0.908946 Weight:  [ 1.09249818] bias:  [-0.66225624]\n",
      "step:  1810 loss:  0.901629 Weight:  [ 1.09212518] bias:  [-0.65958542]\n",
      "step:  1811 loss:  0.894372 Weight:  [ 1.0917536] bias:  [-0.65692538]\n",
      "step:  1812 loss:  0.887172 Weight:  [ 1.09138358] bias:  [-0.65427607]\n",
      "step:  1813 loss:  0.880031 Weight:  [ 1.09101498] bias:  [-0.65163743]\n",
      "step:  1814 loss:  0.872947 Weight:  [ 1.09064794] bias:  [-0.64900947]\n",
      "step:  1815 loss:  0.865919 Weight:  [ 1.09028244] bias:  [-0.64639211]\n",
      "step:  1816 loss:  0.85895 Weight:  [ 1.08991826] bias:  [-0.6437853]\n",
      "step:  1817 loss:  0.852035 Weight:  [ 1.08955562] bias:  [-0.64118898]\n",
      "step:  1818 loss:  0.845178 Weight:  [ 1.08919442] bias:  [-0.63860315]\n",
      "step:  1819 loss:  0.838374 Weight:  [ 1.08883476] bias:  [-0.63602775]\n",
      "step:  1820 loss:  0.831626 Weight:  [ 1.08847654] bias:  [-0.63346273]\n",
      "step:  1821 loss:  0.824931 Weight:  [ 1.08811975] bias:  [-0.63090807]\n",
      "step:  1822 loss:  0.818291 Weight:  [ 1.08776438] bias:  [-0.62836373]\n",
      "step:  1823 loss:  0.811704 Weight:  [ 1.08741033] bias:  [-0.62582964]\n",
      "step:  1824 loss:  0.805171 Weight:  [ 1.08705783] bias:  [-0.62330574]\n",
      "step:  1825 loss:  0.79869 Weight:  [ 1.08670676] bias:  [-0.62079203]\n",
      "step:  1826 loss:  0.79226 Weight:  [ 1.08635712] bias:  [-0.61828846]\n",
      "step:  1827 loss:  0.785884 Weight:  [ 1.08600879] bias:  [-0.61579496]\n",
      "step:  1828 loss:  0.779557 Weight:  [ 1.08566201] bias:  [-0.61331153]\n",
      "step:  1829 loss:  0.773282 Weight:  [ 1.08531654] bias:  [-0.61083812]\n",
      "step:  1830 loss:  0.767058 Weight:  [ 1.08497238] bias:  [-0.60837466]\n",
      "step:  1831 loss:  0.760883 Weight:  [ 1.08462977] bias:  [-0.60592115]\n",
      "step:  1832 loss:  0.754759 Weight:  [ 1.08428848] bias:  [-0.60347754]\n",
      "step:  1833 loss:  0.748683 Weight:  [ 1.08394849] bias:  [-0.60104376]\n",
      "step:  1834 loss:  0.742657 Weight:  [ 1.08360994] bias:  [-0.59861982]\n",
      "step:  1835 loss:  0.736679 Weight:  [ 1.08327281] bias:  [-0.59620565]\n",
      "step:  1836 loss:  0.730749 Weight:  [ 1.082937] bias:  [-0.59380126]\n",
      "step:  1837 loss:  0.724867 Weight:  [ 1.0826025] bias:  [-0.59140652]\n",
      "step:  1838 loss:  0.719031 Weight:  [ 1.08226931] bias:  [-0.58902144]\n",
      "step:  1839 loss:  0.713243 Weight:  [ 1.08193755] bias:  [-0.58664596]\n",
      "step:  1840 loss:  0.707503 Weight:  [ 1.0816071] bias:  [-0.58428007]\n",
      "step:  1841 loss:  0.701807 Weight:  [ 1.08127797] bias:  [-0.58192372]\n",
      "step:  1842 loss:  0.696158 Weight:  [ 1.08095014] bias:  [-0.57957691]\n",
      "step:  1843 loss:  0.690555 Weight:  [ 1.08062375] bias:  [-0.57723951]\n",
      "step:  1844 loss:  0.684996 Weight:  [ 1.08029854] bias:  [-0.57491159]\n",
      "step:  1845 loss:  0.679481 Weight:  [ 1.07997477] bias:  [-0.57259303]\n",
      "step:  1846 loss:  0.674012 Weight:  [ 1.07965219] bias:  [-0.57028383]\n",
      "step:  1847 loss:  0.668587 Weight:  [ 1.07933104] bias:  [-0.56798393]\n",
      "step:  1848 loss:  0.663205 Weight:  [ 1.07901108] bias:  [-0.56569332]\n",
      "step:  1849 loss:  0.657867 Weight:  [ 1.07869244] bias:  [-0.56341195]\n",
      "step:  1850 loss:  0.652571 Weight:  [ 1.0783751] bias:  [-0.56113976]\n",
      "step:  1851 loss:  0.647318 Weight:  [ 1.07805908] bias:  [-0.55887675]\n",
      "step:  1852 loss:  0.642108 Weight:  [ 1.07774425] bias:  [-0.55662286]\n",
      "step:  1853 loss:  0.63694 Weight:  [ 1.07743061] bias:  [-0.55437809]\n",
      "step:  1854 loss:  0.631812 Weight:  [ 1.0771184] bias:  [-0.55214232]\n",
      "step:  1855 loss:  0.626726 Weight:  [ 1.07680738] bias:  [-0.54991561]\n",
      "step:  1856 loss:  0.621682 Weight:  [ 1.07649767] bias:  [-0.54769784]\n",
      "step:  1857 loss:  0.616677 Weight:  [ 1.07618916] bias:  [-0.54548907]\n",
      "step:  1858 loss:  0.611713 Weight:  [ 1.07588184] bias:  [-0.54328918]\n",
      "step:  1859 loss:  0.60679 Weight:  [ 1.07557583] bias:  [-0.54109818]\n",
      "step:  1860 loss:  0.601905 Weight:  [ 1.07527101] bias:  [-0.53891599]\n",
      "step:  1861 loss:  0.59706 Weight:  [ 1.0749675] bias:  [-0.53674263]\n",
      "step:  1862 loss:  0.592254 Weight:  [ 1.07466519] bias:  [-0.53457803]\n",
      "step:  1863 loss:  0.587486 Weight:  [ 1.07436407] bias:  [-0.53242213]\n",
      "step:  1864 loss:  0.582758 Weight:  [ 1.07406414] bias:  [-0.53027493]\n",
      "step:  1865 loss:  0.578066 Weight:  [ 1.07376552] bias:  [-0.52813637]\n",
      "step:  1866 loss:  0.573414 Weight:  [ 1.07346797] bias:  [-0.52600646]\n",
      "step:  1867 loss:  0.568798 Weight:  [ 1.07317173] bias:  [-0.52388513]\n",
      "step:  1868 loss:  0.564219 Weight:  [ 1.07287657] bias:  [-0.52177238]\n",
      "step:  1869 loss:  0.559677 Weight:  [ 1.07258272] bias:  [-0.51966816]\n",
      "step:  1870 loss:  0.555172 Weight:  [ 1.07229006] bias:  [-0.5175724]\n",
      "step:  1871 loss:  0.550704 Weight:  [ 1.07199848] bias:  [-0.51548511]\n",
      "step:  1872 loss:  0.546271 Weight:  [ 1.07170808] bias:  [-0.51340622]\n",
      "step:  1873 loss:  0.541874 Weight:  [ 1.07141888] bias:  [-0.51133573]\n",
      "step:  1874 loss:  0.537512 Weight:  [ 1.07113087] bias:  [-0.50927359]\n",
      "step:  1875 loss:  0.533185 Weight:  [ 1.07084405] bias:  [-0.50721973]\n",
      "step:  1876 loss:  0.528894 Weight:  [ 1.07055831] bias:  [-0.50517416]\n",
      "step:  1877 loss:  0.524636 Weight:  [ 1.07027376] bias:  [-0.50313687]\n",
      "step:  1878 loss:  0.520413 Weight:  [ 1.06999028] bias:  [-0.50110781]\n",
      "step:  1879 loss:  0.516223 Weight:  [ 1.06970811] bias:  [-0.49908689]\n",
      "step:  1880 loss:  0.512068 Weight:  [ 1.06942701] bias:  [-0.49707413]\n",
      "step:  1881 loss:  0.507947 Weight:  [ 1.06914699] bias:  [-0.4950695]\n",
      "step:  1882 loss:  0.503858 Weight:  [ 1.06886816] bias:  [-0.49307296]\n",
      "step:  1883 loss:  0.499803 Weight:  [ 1.0685904] bias:  [-0.49108446]\n",
      "step:  1884 loss:  0.495779 Weight:  [ 1.06831372] bias:  [-0.48910397]\n",
      "step:  1885 loss:  0.491788 Weight:  [ 1.06803823] bias:  [-0.48713148]\n",
      "step:  1886 loss:  0.48783 Weight:  [ 1.06776381] bias:  [-0.48516694]\n",
      "step:  1887 loss:  0.483903 Weight:  [ 1.06749058] bias:  [-0.48321033]\n",
      "step:  1888 loss:  0.480008 Weight:  [ 1.06721842] bias:  [-0.48126158]\n",
      "step:  1889 loss:  0.476144 Weight:  [ 1.06694734] bias:  [-0.4793207]\n",
      "step:  1890 loss:  0.472311 Weight:  [ 1.06667733] bias:  [-0.47738767]\n",
      "step:  1891 loss:  0.468509 Weight:  [ 1.0664084] bias:  [-0.47546244]\n",
      "step:  1892 loss:  0.464738 Weight:  [ 1.06614053] bias:  [-0.47354496]\n",
      "step:  1893 loss:  0.460997 Weight:  [ 1.06587386] bias:  [-0.47163519]\n",
      "step:  1894 loss:  0.457287 Weight:  [ 1.06560814] bias:  [-0.46973315]\n",
      "step:  1895 loss:  0.453605 Weight:  [ 1.06534362] bias:  [-0.46783876]\n",
      "step:  1896 loss:  0.449955 Weight:  [ 1.06508005] bias:  [-0.46595204]\n",
      "step:  1897 loss:  0.446332 Weight:  [ 1.06481767] bias:  [-0.46407291]\n",
      "step:  1898 loss:  0.442739 Weight:  [ 1.06455624] bias:  [-0.46220136]\n",
      "step:  1899 loss:  0.439176 Weight:  [ 1.06429589] bias:  [-0.46033737]\n",
      "step:  1900 loss:  0.43564 Weight:  [ 1.06403661] bias:  [-0.45848089]\n",
      "step:  1901 loss:  0.432134 Weight:  [ 1.0637784] bias:  [-0.4566319]\n",
      "step:  1902 loss:  0.428655 Weight:  [ 1.06352115] bias:  [-0.45479035]\n",
      "step:  1903 loss:  0.425205 Weight:  [ 1.06326497] bias:  [-0.45295626]\n",
      "step:  1904 loss:  0.421782 Weight:  [ 1.06300986] bias:  [-0.45112956]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step:  1905 loss:  0.418387 Weight:  [ 1.0627557] bias:  [-0.44931021]\n",
      "step:  1906 loss:  0.415019 Weight:  [ 1.06250262] bias:  [-0.4474982]\n",
      "step:  1907 loss:  0.411679 Weight:  [ 1.06225049] bias:  [-0.44569349]\n",
      "step:  1908 loss:  0.408365 Weight:  [ 1.06199944] bias:  [-0.44389606]\n",
      "step:  1909 loss:  0.405077 Weight:  [ 1.06174946] bias:  [-0.44210586]\n",
      "step:  1910 loss:  0.401817 Weight:  [ 1.06150043] bias:  [-0.44032291]\n",
      "step:  1911 loss:  0.398583 Weight:  [ 1.06125236] bias:  [-0.43854713]\n",
      "step:  1912 loss:  0.395374 Weight:  [ 1.06100535] bias:  [-0.43677852]\n",
      "step:  1913 loss:  0.392192 Weight:  [ 1.06075931] bias:  [-0.43501705]\n",
      "step:  1914 loss:  0.389035 Weight:  [ 1.06051433] bias:  [-0.43326268]\n",
      "step:  1915 loss:  0.385903 Weight:  [ 1.06027031] bias:  [-0.43151537]\n",
      "step:  1916 loss:  0.382797 Weight:  [ 1.06002724] bias:  [-0.42977512]\n",
      "step:  1917 loss:  0.379715 Weight:  [ 1.05978513] bias:  [-0.42804188]\n",
      "step:  1918 loss:  0.376659 Weight:  [ 1.05954397] bias:  [-0.42631564]\n",
      "step:  1919 loss:  0.373627 Weight:  [ 1.05930388] bias:  [-0.42459634]\n",
      "step:  1920 loss:  0.370619 Weight:  [ 1.05906475] bias:  [-0.42288399]\n",
      "step:  1921 loss:  0.367636 Weight:  [ 1.05882657] bias:  [-0.42117855]\n",
      "step:  1922 loss:  0.364677 Weight:  [ 1.05858934] bias:  [-0.41948]\n",
      "step:  1923 loss:  0.361741 Weight:  [ 1.05835307] bias:  [-0.4177883]\n",
      "step:  1924 loss:  0.358829 Weight:  [ 1.05811775] bias:  [-0.41610342]\n",
      "step:  1925 loss:  0.355941 Weight:  [ 1.05788326] bias:  [-0.41442534]\n",
      "step:  1926 loss:  0.353076 Weight:  [ 1.05764985] bias:  [-0.41275403]\n",
      "step:  1927 loss:  0.350234 Weight:  [ 1.05741739] bias:  [-0.41108945]\n",
      "step:  1928 loss:  0.347414 Weight:  [ 1.05718589] bias:  [-0.40943158]\n",
      "step:  1929 loss:  0.344619 Weight:  [ 1.05695522] bias:  [-0.40778041]\n",
      "step:  1930 loss:  0.341844 Weight:  [ 1.0567255] bias:  [-0.40613589]\n",
      "step:  1931 loss:  0.339093 Weight:  [ 1.05649674] bias:  [-0.40449798]\n",
      "step:  1932 loss:  0.336363 Weight:  [ 1.05626893] bias:  [-0.40286669]\n",
      "step:  1933 loss:  0.333656 Weight:  [ 1.05604196] bias:  [-0.40124199]\n",
      "step:  1934 loss:  0.33097 Weight:  [ 1.05581594] bias:  [-0.39962381]\n",
      "step:  1935 loss:  0.328306 Weight:  [ 1.05559087] bias:  [-0.39801216]\n",
      "step:  1936 loss:  0.325663 Weight:  [ 1.05536664] bias:  [-0.39640704]\n",
      "step:  1937 loss:  0.323041 Weight:  [ 1.05514336] bias:  [-0.39480838]\n",
      "step:  1938 loss:  0.320441 Weight:  [ 1.05492103] bias:  [-0.39321616]\n",
      "step:  1939 loss:  0.317862 Weight:  [ 1.05469954] bias:  [-0.39163038]\n",
      "step:  1940 loss:  0.315303 Weight:  [ 1.05447888] bias:  [-0.39005098]\n",
      "step:  1941 loss:  0.312765 Weight:  [ 1.05425918] bias:  [-0.38847795]\n",
      "step:  1942 loss:  0.310248 Weight:  [ 1.05404043] bias:  [-0.38691127]\n",
      "step:  1943 loss:  0.30775 Weight:  [ 1.0538224] bias:  [-0.38535091]\n",
      "step:  1944 loss:  0.305273 Weight:  [ 1.05360532] bias:  [-0.38379684]\n",
      "step:  1945 loss:  0.302816 Weight:  [ 1.05338919] bias:  [-0.38224903]\n",
      "step:  1946 loss:  0.300378 Weight:  [ 1.0531739] bias:  [-0.38070747]\n",
      "step:  1947 loss:  0.29796 Weight:  [ 1.05295944] bias:  [-0.37917212]\n",
      "step:  1948 loss:  0.295562 Weight:  [ 1.05274582] bias:  [-0.37764296]\n",
      "step:  1949 loss:  0.293183 Weight:  [ 1.05253315] bias:  [-0.37611997]\n",
      "step:  1950 loss:  0.290823 Weight:  [ 1.05232131] bias:  [-0.37460312]\n",
      "step:  1951 loss:  0.288482 Weight:  [ 1.05211031] bias:  [-0.37309238]\n",
      "step:  1952 loss:  0.28616 Weight:  [ 1.05190015] bias:  [-0.37158775]\n",
      "step:  1953 loss:  0.283856 Weight:  [ 1.05169082] bias:  [-0.37008917]\n",
      "step:  1954 loss:  0.281571 Weight:  [ 1.05148232] bias:  [-0.36859664]\n",
      "step:  1955 loss:  0.279304 Weight:  [ 1.05127478] bias:  [-0.36711013]\n",
      "step:  1956 loss:  0.277057 Weight:  [ 1.05106795] bias:  [-0.36562961]\n",
      "step:  1957 loss:  0.274827 Weight:  [ 1.05086195] bias:  [-0.36415508]\n",
      "step:  1958 loss:  0.272614 Weight:  [ 1.0506568] bias:  [-0.36268649]\n",
      "step:  1959 loss:  0.27042 Weight:  [ 1.05045247] bias:  [-0.36122382]\n",
      "step:  1960 loss:  0.268243 Weight:  [ 1.0502491] bias:  [-0.35976702]\n",
      "step:  1961 loss:  0.266084 Weight:  [ 1.05004644] bias:  [-0.35831612]\n",
      "step:  1962 loss:  0.263942 Weight:  [ 1.04984462] bias:  [-0.35687107]\n",
      "step:  1963 loss:  0.261818 Weight:  [ 1.04964364] bias:  [-0.35543185]\n",
      "step:  1964 loss:  0.25971 Weight:  [ 1.04944336] bias:  [-0.35399845]\n",
      "step:  1965 loss:  0.257619 Weight:  [ 1.04924393] bias:  [-0.35257083]\n",
      "step:  1966 loss:  0.255546 Weight:  [ 1.04904532] bias:  [-0.35114896]\n",
      "step:  1967 loss:  0.253489 Weight:  [ 1.04884756] bias:  [-0.34973282]\n",
      "step:  1968 loss:  0.251449 Weight:  [ 1.04865062] bias:  [-0.34832239]\n",
      "step:  1969 loss:  0.249424 Weight:  [ 1.0484544] bias:  [-0.34691766]\n",
      "step:  1970 loss:  0.247416 Weight:  [ 1.04825902] bias:  [-0.34551859]\n",
      "step:  1971 loss:  0.245425 Weight:  [ 1.04806435] bias:  [-0.34412515]\n",
      "step:  1972 loss:  0.243449 Weight:  [ 1.04787052] bias:  [-0.34273735]\n",
      "step:  1973 loss:  0.24149 Weight:  [ 1.04767752] bias:  [-0.34135512]\n",
      "step:  1974 loss:  0.239546 Weight:  [ 1.04748523] bias:  [-0.33997849]\n",
      "step:  1975 loss:  0.237618 Weight:  [ 1.04729366] bias:  [-0.3386074]\n",
      "step:  1976 loss:  0.235705 Weight:  [ 1.04710293] bias:  [-0.33724183]\n",
      "step:  1977 loss:  0.233807 Weight:  [ 1.04691303] bias:  [-0.33588177]\n",
      "step:  1978 loss:  0.231925 Weight:  [ 1.04672384] bias:  [-0.33452719]\n",
      "step:  1979 loss:  0.230059 Weight:  [ 1.04653537] bias:  [-0.3331781]\n",
      "step:  1980 loss:  0.228207 Weight:  [ 1.04634774] bias:  [-0.33183444]\n",
      "step:  1981 loss:  0.226369 Weight:  [ 1.04616082] bias:  [-0.33049619]\n",
      "step:  1982 loss:  0.224548 Weight:  [ 1.04597461] bias:  [-0.32916334]\n",
      "step:  1983 loss:  0.22274 Weight:  [ 1.04578924] bias:  [-0.32783586]\n",
      "step:  1984 loss:  0.220947 Weight:  [ 1.04560459] bias:  [-0.32651374]\n",
      "step:  1985 loss:  0.219169 Weight:  [ 1.04542065] bias:  [-0.32519695]\n",
      "step:  1986 loss:  0.217404 Weight:  [ 1.04523754] bias:  [-0.32388547]\n",
      "step:  1987 loss:  0.215654 Weight:  [ 1.04505503] bias:  [-0.32257929]\n",
      "step:  1988 loss:  0.213919 Weight:  [ 1.04487336] bias:  [-0.32127836]\n",
      "step:  1989 loss:  0.212196 Weight:  [ 1.0446924] bias:  [-0.31998268]\n",
      "step:  1990 loss:  0.210489 Weight:  [ 1.04451215] bias:  [-0.31869224]\n",
      "step:  1991 loss:  0.208794 Weight:  [ 1.04433262] bias:  [-0.31740698]\n",
      "step:  1992 loss:  0.207114 Weight:  [ 1.04415381] bias:  [-0.31612691]\n",
      "step:  1993 loss:  0.205446 Weight:  [ 1.04397571] bias:  [-0.314852]\n",
      "step:  1994 loss:  0.203793 Weight:  [ 1.04379833] bias:  [-0.31358224]\n",
      "step:  1995 loss:  0.202152 Weight:  [ 1.04362178] bias:  [-0.31231758]\n",
      "step:  1996 loss:  0.200525 Weight:  [ 1.04344583] bias:  [-0.31105804]\n",
      "step:  1997 loss:  0.198911 Weight:  [ 1.04327059] bias:  [-0.30980358]\n",
      "step:  1998 loss:  0.197309 Weight:  [ 1.04309618] bias:  [-0.30855417]\n",
      "step:  1999 loss:  0.195721 Weight:  [ 1.04292238] bias:  [-0.30730981]\n",
      "step:  2000 loss:  0.194146 Weight:  [ 1.04274929] bias:  [-0.30607048]\n",
      "step:  2001 loss:  0.192584 Weight:  [ 1.04257679] bias:  [-0.30483612]\n",
      "step:  2002 loss:  0.191033 Weight:  [ 1.04240513] bias:  [-0.30360675]\n",
      "step:  2003 loss:  0.189495 Weight:  [ 1.04223418] bias:  [-0.30238235]\n",
      "step:  2004 loss:  0.18797 Weight:  [ 1.04206383] bias:  [-0.3011629]\n",
      "step:  2005 loss:  0.186457 Weight:  [ 1.0418942] bias:  [-0.29994836]\n",
      "step:  2006 loss:  0.184956 Weight:  [ 1.04172528] bias:  [-0.29873872]\n",
      "step:  2007 loss:  0.183467 Weight:  [ 1.04155695] bias:  [-0.29753396]\n",
      "step:  2008 loss:  0.181991 Weight:  [ 1.04138935] bias:  [-0.29633406]\n",
      "step:  2009 loss:  0.180526 Weight:  [ 1.04122245] bias:  [-0.29513898]\n",
      "step:  2010 loss:  0.179073 Weight:  [ 1.04105616] bias:  [-0.29394874]\n",
      "step:  2011 loss:  0.177631 Weight:  [ 1.04089057] bias:  [-0.29276326]\n",
      "step:  2012 loss:  0.176201 Weight:  [ 1.04072571] bias:  [-0.29158258]\n",
      "step:  2013 loss:  0.174783 Weight:  [ 1.04056144] bias:  [-0.29040667]\n",
      "step:  2014 loss:  0.173376 Weight:  [ 1.04039788] bias:  [-0.2892355]\n",
      "step:  2015 loss:  0.17198 Weight:  [ 1.04023492] bias:  [-0.28806904]\n",
      "step:  2016 loss:  0.170596 Weight:  [ 1.04007268] bias:  [-0.28690729]\n",
      "step:  2017 loss:  0.169223 Weight:  [ 1.03991103] bias:  [-0.28575021]\n",
      "step:  2018 loss:  0.167861 Weight:  [ 1.0397501] bias:  [-0.28459781]\n",
      "step:  2019 loss:  0.16651 Weight:  [ 1.03958976] bias:  [-0.28345007]\n",
      "step:  2020 loss:  0.165169 Weight:  [ 1.03943014] bias:  [-0.28230694]\n",
      "step:  2021 loss:  0.16384 Weight:  [ 1.03927112] bias:  [-0.28116843]\n",
      "step:  2022 loss:  0.162521 Weight:  [ 1.03911269] bias:  [-0.28003451]\n",
      "step:  2023 loss:  0.161213 Weight:  [ 1.03895497] bias:  [-0.27890515]\n",
      "step:  2024 loss:  0.159915 Weight:  [ 1.03879786] bias:  [-0.27778035]\n",
      "step:  2025 loss:  0.158627 Weight:  [ 1.03864145] bias:  [-0.27666008]\n",
      "step:  2026 loss:  0.157351 Weight:  [ 1.03848565] bias:  [-0.27554435]\n",
      "step:  2027 loss:  0.156084 Weight:  [ 1.03833044] bias:  [-0.27443311]\n",
      "step:  2028 loss:  0.154828 Weight:  [ 1.03817582] bias:  [-0.27332634]\n",
      "step:  2029 loss:  0.153582 Weight:  [ 1.0380218] bias:  [-0.27222404]\n",
      "step:  2030 loss:  0.152345 Weight:  [ 1.0378685] bias:  [-0.27112618]\n",
      "step:  2031 loss:  0.151119 Weight:  [ 1.03771579] bias:  [-0.27003276]\n",
      "step:  2032 loss:  0.149903 Weight:  [ 1.03756368] bias:  [-0.26894376]\n",
      "step:  2033 loss:  0.148696 Weight:  [ 1.03741217] bias:  [-0.26785913]\n",
      "step:  2034 loss:  0.147499 Weight:  [ 1.03726137] bias:  [-0.26677889]\n",
      "step:  2035 loss:  0.146312 Weight:  [ 1.03711104] bias:  [-0.26570299]\n",
      "step:  2036 loss:  0.145134 Weight:  [ 1.03696144] bias:  [-0.26463145]\n",
      "step:  2037 loss:  0.143966 Weight:  [ 1.03681231] bias:  [-0.26356423]\n",
      "step:  2038 loss:  0.142807 Weight:  [ 1.03666389] bias:  [-0.2625013]\n",
      "step:  2039 loss:  0.141657 Weight:  [ 1.03651607] bias:  [-0.26144266]\n",
      "step:  2040 loss:  0.140517 Weight:  [ 1.03636885] bias:  [-0.26038828]\n",
      "step:  2041 loss:  0.139386 Weight:  [ 1.0362221] bias:  [-0.25933817]\n",
      "step:  2042 loss:  0.138264 Weight:  [ 1.03607607] bias:  [-0.25829229]\n",
      "step:  2043 loss:  0.137151 Weight:  [ 1.03593051] bias:  [-0.25725064]\n",
      "step:  2044 loss:  0.136047 Weight:  [ 1.03578568] bias:  [-0.25621319]\n",
      "step:  2045 loss:  0.134952 Weight:  [ 1.03564131] bias:  [-0.25517991]\n",
      "step:  2046 loss:  0.133865 Weight:  [ 1.03549755] bias:  [-0.25415081]\n",
      "step:  2047 loss:  0.132788 Weight:  [ 1.03535438] bias:  [-0.25312585]\n",
      "step:  2048 loss:  0.131719 Weight:  [ 1.0352118] bias:  [-0.25210503]\n",
      "step:  2049 loss:  0.130659 Weight:  [ 1.03506982] bias:  [-0.25108832]\n",
      "step:  2050 loss:  0.129607 Weight:  [ 1.03492832] bias:  [-0.25007573]\n",
      "step:  2051 loss:  0.128564 Weight:  [ 1.03478754] bias:  [-0.2490672]\n",
      "step:  2052 loss:  0.127529 Weight:  [ 1.03464723] bias:  [-0.24806274]\n",
      "step:  2053 loss:  0.126502 Weight:  [ 1.03450751] bias:  [-0.24706234]\n",
      "step:  2054 loss:  0.125484 Weight:  [ 1.0343684] bias:  [-0.24606597]\n",
      "step:  2055 loss:  0.124474 Weight:  [ 1.03422976] bias:  [-0.24507363]\n",
      "step:  2056 loss:  0.123472 Weight:  [ 1.03409171] bias:  [-0.24408528]\n",
      "step:  2057 loss:  0.122478 Weight:  [ 1.03395414] bias:  [-0.24310091]\n",
      "step:  2058 loss:  0.121492 Weight:  [ 1.03381729] bias:  [-0.2421205]\n",
      "step:  2059 loss:  0.120514 Weight:  [ 1.03368092] bias:  [-0.24114406]\n",
      "step:  2060 loss:  0.119544 Weight:  [ 1.03354502] bias:  [-0.24017155]\n",
      "step:  2061 loss:  0.118582 Weight:  [ 1.03340971] bias:  [-0.23920296]\n",
      "step:  2062 loss:  0.117628 Weight:  [ 1.03327501] bias:  [-0.23823829]\n",
      "step:  2063 loss:  0.116681 Weight:  [ 1.0331409] bias:  [-0.23727749]\n",
      "step:  2064 loss:  0.115742 Weight:  [ 1.03300714] bias:  [-0.23632059]\n",
      "step:  2065 loss:  0.11481 Weight:  [ 1.03287411] bias:  [-0.23536754]\n",
      "step:  2066 loss:  0.113886 Weight:  [ 1.03274155] bias:  [-0.23441833]\n",
      "step:  2067 loss:  0.112969 Weight:  [ 1.03260946] bias:  [-0.23347296]\n",
      "step:  2068 loss:  0.11206 Weight:  [ 1.03247797] bias:  [-0.2325314]\n",
      "step:  2069 loss:  0.111157 Weight:  [ 1.03234696] bias:  [-0.23159362]\n",
      "step:  2070 loss:  0.110262 Weight:  [ 1.03221655] bias:  [-0.23065963]\n",
      "step:  2071 loss:  0.109375 Weight:  [ 1.03208661] bias:  [-0.22972941]\n",
      "step:  2072 loss:  0.108495 Weight:  [ 1.03195727] bias:  [-0.22880295]\n",
      "step:  2073 loss:  0.107622 Weight:  [ 1.03182828] bias:  [-0.22788022]\n",
      "step:  2074 loss:  0.106755 Weight:  [ 1.03170002] bias:  [-0.22696121]\n",
      "step:  2075 loss:  0.105896 Weight:  [ 1.0315721] bias:  [-0.22604591]\n",
      "step:  2076 loss:  0.105044 Weight:  [ 1.03144479] bias:  [-0.2251343]\n",
      "step:  2077 loss:  0.104198 Weight:  [ 1.03131795] bias:  [-0.22422636]\n",
      "step:  2078 loss:  0.103359 Weight:  [ 1.03119171] bias:  [-0.22332208]\n",
      "step:  2079 loss:  0.102527 Weight:  [ 1.03106582] bias:  [-0.22242145]\n",
      "step:  2080 loss:  0.101702 Weight:  [ 1.03094053] bias:  [-0.22152445]\n",
      "step:  2081 loss:  0.100883 Weight:  [ 1.03081572] bias:  [-0.22063106]\n",
      "step:  2082 loss:  0.100071 Weight:  [ 1.0306915] bias:  [-0.21974128]\n",
      "step:  2083 loss:  0.0992659 Weight:  [ 1.03056777] bias:  [-0.2188551]\n",
      "step:  2084 loss:  0.0984667 Weight:  [ 1.0304445] bias:  [-0.21797249]\n",
      "step:  2085 loss:  0.097674 Weight:  [ 1.03032172] bias:  [-0.21709344]\n",
      "step:  2086 loss:  0.0968878 Weight:  [ 1.03019941] bias:  [-0.21621794]\n",
      "step:  2087 loss:  0.096108 Weight:  [ 1.0300777] bias:  [-0.21534595]\n",
      "step:  2088 loss:  0.0953343 Weight:  [ 1.02995634] bias:  [-0.21447749]\n",
      "step:  2089 loss:  0.094567 Weight:  [ 1.02983546] bias:  [-0.21361253]\n",
      "step:  2090 loss:  0.0938058 Weight:  [ 1.02971518] bias:  [-0.21275105]\n",
      "step:  2091 loss:  0.0930505 Weight:  [ 1.02959538] bias:  [-0.21189304]\n",
      "step:  2092 loss:  0.0923016 Weight:  [ 1.02947605] bias:  [-0.2110385]\n",
      "step:  2093 loss:  0.0915587 Weight:  [ 1.02935719] bias:  [-0.21018741]\n",
      "step:  2094 loss:  0.0908217 Weight:  [ 1.0292387] bias:  [-0.20933975]\n",
      "step:  2095 loss:  0.0900904 Weight:  [ 1.0291208] bias:  [-0.20849551]\n",
      "step:  2096 loss:  0.0893656 Weight:  [ 1.02900338] bias:  [-0.20765467]\n",
      "step:  2097 loss:  0.0886458 Weight:  [ 1.02888644] bias:  [-0.20681722]\n",
      "step:  2098 loss:  0.0879323 Weight:  [ 1.02876997] bias:  [-0.20598316]\n",
      "step:  2099 loss:  0.0872247 Weight:  [ 1.02865386] bias:  [-0.20515247]\n",
      "step:  2100 loss:  0.0865227 Weight:  [ 1.02853835] bias:  [-0.20432511]\n",
      "step:  2101 loss:  0.0858262 Weight:  [ 1.02842319] bias:  [-0.20350109]\n",
      "step:  2102 loss:  0.0851354 Weight:  [ 1.02830863] bias:  [-0.20268039]\n",
      "step:  2103 loss:  0.0844499 Weight:  [ 1.02819443] bias:  [-0.20186301]\n",
      "step:  2104 loss:  0.0837701 Weight:  [ 1.0280807] bias:  [-0.20104891]\n",
      "step:  2105 loss:  0.0830959 Weight:  [ 1.02796745] bias:  [-0.20023811]\n",
      "step:  2106 loss:  0.082427 Weight:  [ 1.02785468] bias:  [-0.19943057]\n",
      "step:  2107 loss:  0.0817635 Weight:  [ 1.02774239] bias:  [-0.19862629]\n",
      "step:  2108 loss:  0.0811052 Weight:  [ 1.02763057] bias:  [-0.19782525]\n",
      "step:  2109 loss:  0.0804525 Weight:  [ 1.02751911] bias:  [-0.19702746]\n",
      "step:  2110 loss:  0.079805 Weight:  [ 1.02740812] bias:  [-0.19623287]\n",
      "step:  2111 loss:  0.0791625 Weight:  [ 1.0272975] bias:  [-0.1954415]\n",
      "step:  2112 loss:  0.0785254 Weight:  [ 1.02718747] bias:  [-0.1946533]\n",
      "step:  2113 loss:  0.077893 Weight:  [ 1.02707779] bias:  [-0.19386829]\n",
      "step:  2114 loss:  0.077266 Weight:  [ 1.0269686] bias:  [-0.19308645]\n",
      "step:  2115 loss:  0.0766441 Weight:  [ 1.02685988] bias:  [-0.19230776]\n",
      "step:  2116 loss:  0.0760273 Weight:  [ 1.02675152] bias:  [-0.19153221]\n",
      "step:  2117 loss:  0.0754153 Weight:  [ 1.02664363] bias:  [-0.19075978]\n",
      "step:  2118 loss:  0.0748083 Weight:  [ 1.02653623] bias:  [-0.18999046]\n",
      "step:  2119 loss:  0.0742061 Weight:  [ 1.02642918] bias:  [-0.18922426]\n",
      "step:  2120 loss:  0.0736087 Weight:  [ 1.0263226] bias:  [-0.18846114]\n",
      "step:  2121 loss:  0.0730161 Weight:  [ 1.02621651] bias:  [-0.18770109]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step:  2122 loss:  0.0724284 Weight:  [ 1.02611077] bias:  [-0.18694413]\n",
      "step:  2123 loss:  0.0718454 Weight:  [ 1.02600539] bias:  [-0.18619022]\n",
      "step:  2124 loss:  0.071267 Weight:  [ 1.02590048] bias:  [-0.18543933]\n",
      "step:  2125 loss:  0.0706935 Weight:  [ 1.02579606] bias:  [-0.18469147]\n",
      "step:  2126 loss:  0.0701243 Weight:  [ 1.02569199] bias:  [-0.18394664]\n",
      "step:  2127 loss:  0.06956 Weight:  [ 1.02558839] bias:  [-0.1832048]\n",
      "step:  2128 loss:  0.0690001 Weight:  [ 1.02548528] bias:  [-0.18246596]\n",
      "step:  2129 loss:  0.0684447 Weight:  [ 1.02538252] bias:  [-0.18173009]\n",
      "step:  2130 loss:  0.0678937 Weight:  [ 1.02528012] bias:  [-0.18099721]\n",
      "step:  2131 loss:  0.0673472 Weight:  [ 1.02517819] bias:  [-0.18026727]\n",
      "step:  2132 loss:  0.066805 Weight:  [ 1.02507663] bias:  [-0.17954028]\n",
      "step:  2133 loss:  0.0662673 Weight:  [ 1.02497554] bias:  [-0.17881621]\n",
      "step:  2134 loss:  0.0657342 Weight:  [ 1.02487469] bias:  [-0.17809507]\n",
      "step:  2135 loss:  0.0652049 Weight:  [ 1.02477443] bias:  [-0.17737684]\n",
      "step:  2136 loss:  0.06468 Weight:  [ 1.02467453] bias:  [-0.17666149]\n",
      "step:  2137 loss:  0.0641592 Weight:  [ 1.024575] bias:  [-0.17594904]\n",
      "step:  2138 loss:  0.063643 Weight:  [ 1.02447593] bias:  [-0.17523946]\n",
      "step:  2139 loss:  0.0631306 Weight:  [ 1.02437723] bias:  [-0.17453274]\n",
      "step:  2140 loss:  0.0626224 Weight:  [ 1.02427888] bias:  [-0.17382887]\n",
      "step:  2141 loss:  0.0621181 Weight:  [ 1.02418101] bias:  [-0.17312783]\n",
      "step:  2142 loss:  0.0616182 Weight:  [ 1.0240835] bias:  [-0.17242964]\n",
      "step:  2143 loss:  0.0611222 Weight:  [ 1.02398634] bias:  [-0.17173424]\n",
      "step:  2144 loss:  0.0606304 Weight:  [ 1.02388954] bias:  [-0.17104167]\n",
      "step:  2145 loss:  0.0601423 Weight:  [ 1.02379322] bias:  [-0.17035188]\n",
      "step:  2146 loss:  0.0596581 Weight:  [ 1.02369726] bias:  [-0.16966486]\n",
      "step:  2147 loss:  0.0591779 Weight:  [ 1.02360177] bias:  [-0.16898061]\n",
      "step:  2148 loss:  0.0587015 Weight:  [ 1.02350652] bias:  [-0.16829914]\n",
      "step:  2149 loss:  0.0582292 Weight:  [ 1.02341175] bias:  [-0.16762041]\n",
      "step:  2150 loss:  0.0577602 Weight:  [ 1.02331734] bias:  [-0.16694441]\n",
      "step:  2151 loss:  0.0572954 Weight:  [ 1.02322328] bias:  [-0.16627115]\n",
      "step:  2152 loss:  0.0568342 Weight:  [ 1.0231297] bias:  [-0.1656006]\n",
      "step:  2153 loss:  0.0563765 Weight:  [ 1.02303636] bias:  [-0.16493276]\n",
      "step:  2154 loss:  0.0559228 Weight:  [ 1.0229435] bias:  [-0.1642676]\n",
      "step:  2155 loss:  0.0554727 Weight:  [ 1.02285099] bias:  [-0.16360514]\n",
      "step:  2156 loss:  0.0550263 Weight:  [ 1.02275872] bias:  [-0.16294535]\n",
      "step:  2157 loss:  0.0545831 Weight:  [ 1.02266705] bias:  [-0.1622882]\n",
      "step:  2158 loss:  0.0541437 Weight:  [ 1.02257562] bias:  [-0.16163372]\n",
      "step:  2159 loss:  0.0537081 Weight:  [ 1.02248454] bias:  [-0.16098186]\n",
      "step:  2160 loss:  0.0532758 Weight:  [ 1.02239382] bias:  [-0.16033265]\n",
      "step:  2161 loss:  0.0528469 Weight:  [ 1.02230358] bias:  [-0.15968604]\n",
      "step:  2162 loss:  0.0524215 Weight:  [ 1.02221358] bias:  [-0.15904205]\n",
      "step:  2163 loss:  0.0519994 Weight:  [ 1.02212405] bias:  [-0.15840064]\n",
      "step:  2164 loss:  0.0515811 Weight:  [ 1.02203476] bias:  [-0.15776184]\n",
      "step:  2165 loss:  0.0511658 Weight:  [ 1.02194595] bias:  [-0.15712561]\n",
      "step:  2166 loss:  0.050754 Weight:  [ 1.02185738] bias:  [-0.15649194]\n",
      "step:  2167 loss:  0.0503453 Weight:  [ 1.02176929] bias:  [-0.15586081]\n",
      "step:  2168 loss:  0.0499402 Weight:  [ 1.02168143] bias:  [-0.15523225]\n",
      "step:  2169 loss:  0.049538 Weight:  [ 1.02159405] bias:  [-0.15460621]\n",
      "step:  2170 loss:  0.0491394 Weight:  [ 1.02150691] bias:  [-0.1539827]\n",
      "step:  2171 loss:  0.0487437 Weight:  [ 1.02142024] bias:  [-0.15336169]\n",
      "step:  2172 loss:  0.0483517 Weight:  [ 1.02133381] bias:  [-0.15274321]\n",
      "step:  2173 loss:  0.0479621 Weight:  [ 1.02124774] bias:  [-0.15212721]\n",
      "step:  2174 loss:  0.0475763 Weight:  [ 1.02116215] bias:  [-0.1515137]\n",
      "step:  2175 loss:  0.047193 Weight:  [ 1.0210768] bias:  [-0.15090266]\n",
      "step:  2176 loss:  0.0468133 Weight:  [ 1.0209918] bias:  [-0.1502941]\n",
      "step:  2177 loss:  0.0464365 Weight:  [ 1.02090716] bias:  [-0.14968798]\n",
      "step:  2178 loss:  0.0460629 Weight:  [ 1.02082276] bias:  [-0.14908431]\n",
      "step:  2179 loss:  0.0456919 Weight:  [ 1.02073884] bias:  [-0.14848307]\n",
      "step:  2180 loss:  0.045324 Weight:  [ 1.02065516] bias:  [-0.14788425]\n",
      "step:  2181 loss:  0.0449593 Weight:  [ 1.02057195] bias:  [-0.14728785]\n",
      "step:  2182 loss:  0.0445974 Weight:  [ 1.02048898] bias:  [-0.14669386]\n",
      "step:  2183 loss:  0.0442383 Weight:  [ 1.02040637] bias:  [-0.14610226]\n",
      "step:  2184 loss:  0.0438824 Weight:  [ 1.02032399] bias:  [-0.14551306]\n",
      "step:  2185 loss:  0.043529 Weight:  [ 1.02024209] bias:  [-0.14492622]\n",
      "step:  2186 loss:  0.0431788 Weight:  [ 1.02016044] bias:  [-0.14434175]\n",
      "step:  2187 loss:  0.0428313 Weight:  [ 1.02007902] bias:  [-0.14375965]\n",
      "step:  2188 loss:  0.0424864 Weight:  [ 1.01999807] bias:  [-0.14317988]\n",
      "step:  2189 loss:  0.0421444 Weight:  [ 1.01991749] bias:  [-0.14260244]\n",
      "step:  2190 loss:  0.0418051 Weight:  [ 1.01983714] bias:  [-0.14202735]\n",
      "step:  2191 loss:  0.0414688 Weight:  [ 1.01975715] bias:  [-0.14145458]\n",
      "step:  2192 loss:  0.0411349 Weight:  [ 1.01967752] bias:  [-0.1408841]\n",
      "step:  2193 loss:  0.0408037 Weight:  [ 1.01959813] bias:  [-0.14031594]\n",
      "step:  2194 loss:  0.0404752 Weight:  [ 1.01951909] bias:  [-0.13975006]\n",
      "step:  2195 loss:  0.0401493 Weight:  [ 1.01944041] bias:  [-0.13918647]\n",
      "step:  2196 loss:  0.0398263 Weight:  [ 1.01936197] bias:  [-0.13862516]\n",
      "step:  2197 loss:  0.0395057 Weight:  [ 1.01928389] bias:  [-0.1380661]\n",
      "step:  2198 loss:  0.0391876 Weight:  [ 1.01920617] bias:  [-0.1375093]\n",
      "step:  2199 loss:  0.0388722 Weight:  [ 1.01912868] bias:  [-0.13695475]\n",
      "step:  2200 loss:  0.0385596 Weight:  [ 1.01905143] bias:  [-0.13640244]\n",
      "step:  2201 loss:  0.0382491 Weight:  [ 1.01897466] bias:  [-0.13585234]\n",
      "step:  2202 loss:  0.0379411 Weight:  [ 1.01889813] bias:  [-0.13530447]\n",
      "step:  2203 loss:  0.0376356 Weight:  [ 1.01882195] bias:  [-0.1347588]\n",
      "step:  2204 loss:  0.0373327 Weight:  [ 1.01874602] bias:  [-0.13421534]\n",
      "step:  2205 loss:  0.0370322 Weight:  [ 1.01867044] bias:  [-0.13367407]\n",
      "step:  2206 loss:  0.0367343 Weight:  [ 1.0185951] bias:  [-0.13313498]\n",
      "step:  2207 loss:  0.0364385 Weight:  [ 1.01852012] bias:  [-0.13259806]\n",
      "step:  2208 loss:  0.0361453 Weight:  [ 1.01844537] bias:  [-0.1320633]\n",
      "step:  2209 loss:  0.0358543 Weight:  [ 1.01837099] bias:  [-0.1315307]\n",
      "step:  2210 loss:  0.0355656 Weight:  [ 1.01829696] bias:  [-0.13100025]\n",
      "step:  2211 loss:  0.0352793 Weight:  [ 1.01822317] bias:  [-0.13047194]\n",
      "step:  2212 loss:  0.0349953 Weight:  [ 1.01814973] bias:  [-0.12994577]\n",
      "step:  2213 loss:  0.0347137 Weight:  [ 1.01807654] bias:  [-0.12942173]\n",
      "step:  2214 loss:  0.0344341 Weight:  [ 1.01800358] bias:  [-0.12889978]\n",
      "step:  2215 loss:  0.034157 Weight:  [ 1.01793098] bias:  [-0.12837994]\n",
      "step:  2216 loss:  0.0338821 Weight:  [ 1.01785862] bias:  [-0.1278622]\n",
      "step:  2217 loss:  0.0336093 Weight:  [ 1.01778662] bias:  [-0.12734655]\n",
      "step:  2218 loss:  0.0333387 Weight:  [ 1.01771498] bias:  [-0.12683296]\n",
      "step:  2219 loss:  0.0330705 Weight:  [ 1.01764345] bias:  [-0.12632146]\n",
      "step:  2220 loss:  0.0328042 Weight:  [ 1.01757228] bias:  [-0.12581202]\n",
      "step:  2221 loss:  0.0325404 Weight:  [ 1.01750147] bias:  [-0.12530464]\n",
      "step:  2222 loss:  0.0322783 Weight:  [ 1.0174309] bias:  [-0.1247993]\n",
      "step:  2223 loss:  0.0320183 Weight:  [ 1.01736057] bias:  [-0.124296]\n",
      "step:  2224 loss:  0.0317607 Weight:  [ 1.01729059] bias:  [-0.12379473]\n",
      "step:  2225 loss:  0.031505 Weight:  [ 1.01722085] bias:  [-0.12329548]\n",
      "step:  2226 loss:  0.0312514 Weight:  [ 1.01715136] bias:  [-0.12279824]\n",
      "step:  2227 loss:  0.0309999 Weight:  [ 1.01708221] bias:  [-0.12230301]\n",
      "step:  2228 loss:  0.0307503 Weight:  [ 1.01701331] bias:  [-0.12180977]\n",
      "step:  2229 loss:  0.0305028 Weight:  [ 1.01694477] bias:  [-0.12131853]\n",
      "step:  2230 loss:  0.0302573 Weight:  [ 1.01687634] bias:  [-0.12082927]\n",
      "step:  2231 loss:  0.0300136 Weight:  [ 1.01680827] bias:  [-0.12034197]\n",
      "step:  2232 loss:  0.0297722 Weight:  [ 1.01674056] bias:  [-0.11985664]\n",
      "step:  2233 loss:  0.0295325 Weight:  [ 1.01667297] bias:  [-0.11937328]\n",
      "step:  2234 loss:  0.0292948 Weight:  [ 1.01660573] bias:  [-0.11889185]\n",
      "step:  2235 loss:  0.029059 Weight:  [ 1.01653874] bias:  [-0.11841237]\n",
      "step:  2236 loss:  0.028825 Weight:  [ 1.0164721] bias:  [-0.11793482]\n",
      "step:  2237 loss:  0.0285929 Weight:  [ 1.0164057] bias:  [-0.1174592]\n",
      "step:  2238 loss:  0.0283628 Weight:  [ 1.01633954] bias:  [-0.11698551]\n",
      "step:  2239 loss:  0.0281345 Weight:  [ 1.01627362] bias:  [-0.11651372]\n",
      "step:  2240 loss:  0.0279081 Weight:  [ 1.01620793] bias:  [-0.11604384]\n",
      "step:  2241 loss:  0.0276833 Weight:  [ 1.01614261] bias:  [-0.11557584]\n",
      "step:  2242 loss:  0.0274606 Weight:  [ 1.01607752] bias:  [-0.11510973]\n",
      "step:  2243 loss:  0.0272396 Weight:  [ 1.01601267] bias:  [-0.11464551]\n",
      "step:  2244 loss:  0.0270202 Weight:  [ 1.01594818] bias:  [-0.11418316]\n",
      "step:  2245 loss:  0.0268028 Weight:  [ 1.0158838] bias:  [-0.11372267]\n",
      "step:  2246 loss:  0.026587 Weight:  [ 1.01581979] bias:  [-0.11326405]\n",
      "step:  2247 loss:  0.026373 Weight:  [ 1.01575601] bias:  [-0.11280727]\n",
      "step:  2248 loss:  0.0261608 Weight:  [ 1.01569247] bias:  [-0.11235234]\n",
      "step:  2249 loss:  0.0259502 Weight:  [ 1.01562917] bias:  [-0.11189925]\n",
      "step:  2250 loss:  0.0257413 Weight:  [ 1.01556611] bias:  [-0.11144798]\n",
      "step:  2251 loss:  0.025534 Weight:  [ 1.01550329] bias:  [-0.11099852]\n",
      "step:  2252 loss:  0.0253287 Weight:  [ 1.01544082] bias:  [-0.11055087]\n",
      "step:  2253 loss:  0.0251247 Weight:  [ 1.01537848] bias:  [-0.11010504]\n",
      "step:  2254 loss:  0.0249224 Weight:  [ 1.01531649] bias:  [-0.109661]\n",
      "step:  2255 loss:  0.0247218 Weight:  [ 1.01525474] bias:  [-0.10921875]\n",
      "step:  2256 loss:  0.0245229 Weight:  [ 1.01519322] bias:  [-0.10877828]\n",
      "step:  2257 loss:  0.0243254 Weight:  [ 1.01513195] bias:  [-0.10833959]\n",
      "step:  2258 loss:  0.0241296 Weight:  [ 1.01507092] bias:  [-0.10790268]\n",
      "step:  2259 loss:  0.0239354 Weight:  [ 1.01501012] bias:  [-0.10746752]\n",
      "step:  2260 loss:  0.0237426 Weight:  [ 1.01494956] bias:  [-0.10703411]\n",
      "step:  2261 loss:  0.0235517 Weight:  [ 1.01488924] bias:  [-0.10660245]\n",
      "step:  2262 loss:  0.0233619 Weight:  [ 1.01482928] bias:  [-0.10617253]\n",
      "step:  2263 loss:  0.0231739 Weight:  [ 1.01476943] bias:  [-0.10574435]\n",
      "step:  2264 loss:  0.0229875 Weight:  [ 1.01470983] bias:  [-0.1053179]\n",
      "step:  2265 loss:  0.0228024 Weight:  [ 1.01465058] bias:  [-0.10489316]\n",
      "step:  2266 loss:  0.0226189 Weight:  [ 1.01459146] bias:  [-0.10447014]\n",
      "step:  2267 loss:  0.0224367 Weight:  [ 1.01453269] bias:  [-0.10404882]\n",
      "step:  2268 loss:  0.0222561 Weight:  [ 1.01447403] bias:  [-0.10362921]\n",
      "step:  2269 loss:  0.022077 Weight:  [ 1.01441562] bias:  [-0.10321129]\n",
      "step:  2270 loss:  0.0218994 Weight:  [ 1.01435757] bias:  [-0.10279505]\n",
      "step:  2271 loss:  0.0217231 Weight:  [ 1.01429963] bias:  [-0.1023805]\n",
      "step:  2272 loss:  0.0215481 Weight:  [ 1.01424193] bias:  [-0.10196761]\n",
      "step:  2273 loss:  0.0213747 Weight:  [ 1.01418447] bias:  [-0.10155638]\n",
      "step:  2274 loss:  0.0212026 Weight:  [ 1.01412725] bias:  [-0.10114682]\n",
      "step:  2275 loss:  0.021032 Weight:  [ 1.01407027] bias:  [-0.10073891]\n",
      "step:  2276 loss:  0.0208628 Weight:  [ 1.01401353] bias:  [-0.10033263]\n",
      "step:  2277 loss:  0.0206948 Weight:  [ 1.01395702] bias:  [-0.099928]\n",
      "step:  2278 loss:  0.0205282 Weight:  [ 1.01390076] bias:  [-0.099525]\n",
      "step:  2279 loss:  0.0203628 Weight:  [ 1.01384473] bias:  [-0.09912362]\n",
      "step:  2280 loss:  0.0201989 Weight:  [ 1.01378894] bias:  [-0.09872387]\n",
      "step:  2281 loss:  0.0200366 Weight:  [ 1.01373327] bias:  [-0.09832574]\n",
      "step:  2282 loss:  0.0198752 Weight:  [ 1.01367784] bias:  [-0.0979292]\n",
      "step:  2283 loss:  0.0197151 Weight:  [ 1.01362276] bias:  [-0.09753426]\n",
      "step:  2284 loss:  0.0195564 Weight:  [ 1.01356781] bias:  [-0.09714092]\n",
      "step:  2285 loss:  0.019399 Weight:  [ 1.01351309] bias:  [-0.09674916]\n",
      "step:  2286 loss:  0.0192429 Weight:  [ 1.01345861] bias:  [-0.09635898]\n",
      "step:  2287 loss:  0.019088 Weight:  [ 1.01340437] bias:  [-0.09597038]\n",
      "step:  2288 loss:  0.0189344 Weight:  [ 1.01335025] bias:  [-0.09558336]\n",
      "step:  2289 loss:  0.0187821 Weight:  [ 1.01329637] bias:  [-0.09519789]\n",
      "step:  2290 loss:  0.0186307 Weight:  [ 1.01324272] bias:  [-0.09481397]\n",
      "step:  2291 loss:  0.0184807 Weight:  [ 1.01318932] bias:  [-0.09443159]\n",
      "step:  2292 loss:  0.018332 Weight:  [ 1.01313615] bias:  [-0.09405075]\n",
      "step:  2293 loss:  0.0181844 Weight:  [ 1.01308322] bias:  [-0.09367146]\n",
      "step:  2294 loss:  0.0180381 Weight:  [ 1.01303041] bias:  [-0.09329369]\n",
      "step:  2295 loss:  0.0178928 Weight:  [ 1.01297796] bias:  [-0.09291744]\n",
      "step:  2296 loss:  0.0177489 Weight:  [ 1.01292562] bias:  [-0.09254272]\n",
      "step:  2297 loss:  0.017606 Weight:  [ 1.01287341] bias:  [-0.09216952]\n",
      "step:  2298 loss:  0.0174643 Weight:  [ 1.01282156] bias:  [-0.09179781]\n",
      "step:  2299 loss:  0.0173237 Weight:  [ 1.01276982] bias:  [-0.0914276]\n",
      "step:  2300 loss:  0.0171842 Weight:  [ 1.01271832] bias:  [-0.09105889]\n",
      "step:  2301 loss:  0.0170459 Weight:  [ 1.01266706] bias:  [-0.09069166]\n",
      "step:  2302 loss:  0.0169087 Weight:  [ 1.01261592] bias:  [-0.09032592]\n",
      "step:  2303 loss:  0.0167726 Weight:  [ 1.01256502] bias:  [-0.08996165]\n",
      "step:  2304 loss:  0.0166375 Weight:  [ 1.01251435] bias:  [-0.08959883]\n",
      "step:  2305 loss:  0.0165037 Weight:  [ 1.01246393] bias:  [-0.08923749]\n",
      "step:  2306 loss:  0.0163709 Weight:  [ 1.01241362] bias:  [-0.0888776]\n",
      "step:  2307 loss:  0.016239 Weight:  [ 1.01236355] bias:  [-0.08851917]\n",
      "step:  2308 loss:  0.0161083 Weight:  [ 1.01231372] bias:  [-0.08816218]\n",
      "step:  2309 loss:  0.0159787 Weight:  [ 1.01226413] bias:  [-0.08780663]\n",
      "step:  2310 loss:  0.0158501 Weight:  [ 1.01221466] bias:  [-0.08745253]\n",
      "step:  2311 loss:  0.0157224 Weight:  [ 1.01216543] bias:  [-0.08709985]\n",
      "step:  2312 loss:  0.0155959 Weight:  [ 1.01211631] bias:  [-0.08674859]\n",
      "step:  2313 loss:  0.0154705 Weight:  [ 1.01206744] bias:  [-0.08639875]\n",
      "step:  2314 loss:  0.0153458 Weight:  [ 1.0120188] bias:  [-0.08605032]\n",
      "step:  2315 loss:  0.0152223 Weight:  [ 1.01197028] bias:  [-0.08570329]\n",
      "step:  2316 loss:  0.0150999 Weight:  [ 1.011922] bias:  [-0.08535767]\n",
      "step:  2317 loss:  0.0149782 Weight:  [ 1.01187396] bias:  [-0.08501343]\n",
      "step:  2318 loss:  0.0148577 Weight:  [ 1.01182604] bias:  [-0.08467058]\n",
      "step:  2319 loss:  0.014738 Weight:  [ 1.01177835] bias:  [-0.08432911]\n",
      "step:  2320 loss:  0.0146195 Weight:  [ 1.01173079] bias:  [-0.08398902]\n",
      "step:  2321 loss:  0.0145018 Weight:  [ 1.01168358] bias:  [-0.0836503]\n",
      "step:  2322 loss:  0.014385 Weight:  [ 1.0116365] bias:  [-0.08331295]\n",
      "step:  2323 loss:  0.0142693 Weight:  [ 1.01158953] bias:  [-0.08297697]\n",
      "step:  2324 loss:  0.0141543 Weight:  [ 1.0115428] bias:  [-0.08264233]\n",
      "step:  2325 loss:  0.0140404 Weight:  [ 1.01149619] bias:  [-0.08230904]\n",
      "step:  2326 loss:  0.0139274 Weight:  [ 1.01144981] bias:  [-0.0819771]\n",
      "step:  2327 loss:  0.0138153 Weight:  [ 1.01140368] bias:  [-0.08164649]\n",
      "step:  2328 loss:  0.013704 Weight:  [ 1.01135767] bias:  [-0.08131722]\n",
      "step:  2329 loss:  0.0135938 Weight:  [ 1.01131189] bias:  [-0.08098928]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step:  2330 loss:  0.0134845 Weight:  [ 1.01126623] bias:  [-0.08066266]\n",
      "step:  2331 loss:  0.0133759 Weight:  [ 1.01122081] bias:  [-0.08033735]\n",
      "step:  2332 loss:  0.0132682 Weight:  [ 1.01117563] bias:  [-0.08001336]\n",
      "step:  2333 loss:  0.0131613 Weight:  [ 1.01113057] bias:  [-0.07969068]\n",
      "step:  2334 loss:  0.0130554 Weight:  [ 1.01108563] bias:  [-0.07936931]\n",
      "step:  2335 loss:  0.0129503 Weight:  [ 1.01104093] bias:  [-0.07904922]\n",
      "step:  2336 loss:  0.012846 Weight:  [ 1.01099634] bias:  [-0.07873043]\n",
      "step:  2337 loss:  0.0127426 Weight:  [ 1.010952] bias:  [-0.07841291]\n",
      "step:  2338 loss:  0.0126401 Weight:  [ 1.01090789] bias:  [-0.07809668]\n",
      "step:  2339 loss:  0.0125384 Weight:  [ 1.0108639] bias:  [-0.07778173]\n",
      "step:  2340 loss:  0.0124374 Weight:  [ 1.01082003] bias:  [-0.07746805]\n",
      "step:  2341 loss:  0.0123373 Weight:  [ 1.0107764] bias:  [-0.07715563]\n",
      "step:  2342 loss:  0.012238 Weight:  [ 1.01073301] bias:  [-0.07684447]\n",
      "step:  2343 loss:  0.0121396 Weight:  [ 1.01068974] bias:  [-0.07653457]\n",
      "step:  2344 loss:  0.0120418 Weight:  [ 1.01064658] bias:  [-0.07622592]\n",
      "step:  2345 loss:  0.0119449 Weight:  [ 1.01060367] bias:  [-0.07591851]\n",
      "step:  2346 loss:  0.0118488 Weight:  [ 1.01056087] bias:  [-0.07561234]\n",
      "step:  2347 loss:  0.0117534 Weight:  [ 1.01051831] bias:  [-0.07530741]\n",
      "step:  2348 loss:  0.0116586 Weight:  [ 1.01047587] bias:  [-0.0750037]\n",
      "step:  2349 loss:  0.0115649 Weight:  [ 1.01043355] bias:  [-0.07470122]\n",
      "step:  2350 loss:  0.0114718 Weight:  [ 1.01039147] bias:  [-0.07439995]\n",
      "step:  2351 loss:  0.0113794 Weight:  [ 1.01034963] bias:  [-0.0740999]\n",
      "step:  2352 loss:  0.0112879 Weight:  [ 1.01030791] bias:  [-0.07380106]\n",
      "step:  2353 loss:  0.011197 Weight:  [ 1.0102663] bias:  [-0.07350343]\n",
      "step:  2354 loss:  0.0111069 Weight:  [ 1.01022494] bias:  [-0.07320701]\n",
      "step:  2355 loss:  0.0110174 Weight:  [ 1.01018369] bias:  [-0.07291178]\n",
      "step:  2356 loss:  0.0109287 Weight:  [ 1.01014268] bias:  [-0.07261773]\n",
      "step:  2357 loss:  0.0108408 Weight:  [ 1.01010168] bias:  [-0.07232488]\n",
      "step:  2358 loss:  0.0107536 Weight:  [ 1.01006091] bias:  [-0.0720332]\n",
      "step:  2359 loss:  0.010667 Weight:  [ 1.01002038] bias:  [-0.07174269]\n",
      "step:  2360 loss:  0.010581 Weight:  [ 1.00997996] bias:  [-0.07145336]\n",
      "step:  2361 loss:  0.010496 Weight:  [ 1.00993967] bias:  [-0.0711652]\n",
      "step:  2362 loss:  0.0104114 Weight:  [ 1.00989962] bias:  [-0.07087819]\n",
      "step:  2363 loss:  0.0103276 Weight:  [ 1.00985968] bias:  [-0.07059234]\n",
      "step:  2364 loss:  0.0102445 Weight:  [ 1.00981987] bias:  [-0.07030765]\n",
      "step:  2365 loss:  0.0101621 Weight:  [ 1.00978029] bias:  [-0.0700241]\n",
      "step:  2366 loss:  0.0100802 Weight:  [ 1.00974095] bias:  [-0.06974169]\n",
      "step:  2367 loss:  0.00999912 Weight:  [ 1.00970161] bias:  [-0.06946044]\n",
      "step:  2368 loss:  0.00991859 Weight:  [ 1.00966251] bias:  [-0.06918031]\n",
      "step:  2369 loss:  0.00983877 Weight:  [ 1.00962353] bias:  [-0.06890132]\n",
      "step:  2370 loss:  0.0097596 Weight:  [ 1.00958467] bias:  [-0.06862345]\n",
      "step:  2371 loss:  0.00968098 Weight:  [ 1.00954604] bias:  [-0.06834669]\n",
      "step:  2372 loss:  0.00960309 Weight:  [ 1.00950754] bias:  [-0.06807105]\n",
      "step:  2373 loss:  0.00952577 Weight:  [ 1.00946927] bias:  [-0.06779652]\n",
      "step:  2374 loss:  0.00944904 Weight:  [ 1.00943112] bias:  [-0.06752311]\n",
      "step:  2375 loss:  0.00937301 Weight:  [ 1.00939298] bias:  [-0.0672508]\n",
      "step:  2376 loss:  0.00929751 Weight:  [ 1.00935519] bias:  [-0.06697959]\n",
      "step:  2377 loss:  0.00922277 Weight:  [ 1.0093174] bias:  [-0.06670947]\n",
      "step:  2378 loss:  0.00914852 Weight:  [ 1.00927985] bias:  [-0.06644044]\n",
      "step:  2379 loss:  0.00907484 Weight:  [ 1.00924242] bias:  [-0.0661725]\n",
      "step:  2380 loss:  0.00900185 Weight:  [ 1.0092051] bias:  [-0.06590563]\n",
      "step:  2381 loss:  0.00892933 Weight:  [ 1.00916803] bias:  [-0.06563984]\n",
      "step:  2382 loss:  0.00885754 Weight:  [ 1.00913107] bias:  [-0.06537513]\n",
      "step:  2383 loss:  0.00878616 Weight:  [ 1.00909424] bias:  [-0.06511148]\n",
      "step:  2384 loss:  0.0087155 Weight:  [ 1.00905752] bias:  [-0.0648489]\n",
      "step:  2385 loss:  0.00864533 Weight:  [ 1.00902104] bias:  [-0.06458737]\n",
      "step:  2386 loss:  0.00857567 Weight:  [ 1.00898468] bias:  [-0.0643269]\n",
      "step:  2387 loss:  0.00850664 Weight:  [ 1.00894845] bias:  [-0.06406748]\n",
      "step:  2388 loss:  0.00843814 Weight:  [ 1.00891232] bias:  [-0.06380911]\n",
      "step:  2389 loss:  0.00837025 Weight:  [ 1.00887644] bias:  [-0.06355178]\n",
      "step:  2390 loss:  0.00830289 Weight:  [ 1.00884056] bias:  [-0.06329549]\n",
      "step:  2391 loss:  0.00823612 Weight:  [ 1.00880492] bias:  [-0.06304023]\n",
      "step:  2392 loss:  0.00816984 Weight:  [ 1.00876939] bias:  [-0.062786]\n",
      "step:  2393 loss:  0.00810405 Weight:  [ 1.00873399] bias:  [-0.06253279]\n",
      "step:  2394 loss:  0.00803882 Weight:  [ 1.00869882] bias:  [-0.0622806]\n",
      "step:  2395 loss:  0.00797412 Weight:  [ 1.00866377] bias:  [-0.06202943]\n",
      "step:  2396 loss:  0.00790985 Weight:  [ 1.00862885] bias:  [-0.06177928]\n",
      "step:  2397 loss:  0.00784626 Weight:  [ 1.00859404] bias:  [-0.06153014]\n",
      "step:  2398 loss:  0.00778309 Weight:  [ 1.00855935] bias:  [-0.061282]\n",
      "step:  2399 loss:  0.00772038 Weight:  [ 1.00852478] bias:  [-0.06103485]\n",
      "step:  2400 loss:  0.00765829 Weight:  [ 1.00849044] bias:  [-0.0607887]\n",
      "step:  2401 loss:  0.00759662 Weight:  [ 1.00845623] bias:  [-0.06054355]\n",
      "step:  2402 loss:  0.00753544 Weight:  [ 1.00842214] bias:  [-0.06029939]\n",
      "step:  2403 loss:  0.00747485 Weight:  [ 1.00838816] bias:  [-0.06005621]\n",
      "step:  2404 loss:  0.00741461 Weight:  [ 1.00835431] bias:  [-0.05981402]\n",
      "step:  2405 loss:  0.00735491 Weight:  [ 1.00832069] bias:  [-0.05957279]\n",
      "step:  2406 loss:  0.00729577 Weight:  [ 1.00828707] bias:  [-0.05933255]\n",
      "step:  2407 loss:  0.00723702 Weight:  [ 1.00825369] bias:  [-0.05909327]\n",
      "step:  2408 loss:  0.00717877 Weight:  [ 1.00822031] bias:  [-0.05885496]\n",
      "step:  2409 loss:  0.007121 Weight:  [ 1.00818717] bias:  [-0.0586176]\n",
      "step:  2410 loss:  0.00706372 Weight:  [ 1.00815415] bias:  [-0.0583812]\n",
      "step:  2411 loss:  0.00700682 Weight:  [ 1.00812125] bias:  [-0.05814576]\n",
      "step:  2412 loss:  0.00695048 Weight:  [ 1.00808847] bias:  [-0.05791126]\n",
      "step:  2413 loss:  0.00689452 Weight:  [ 1.00805593] bias:  [-0.05767771]\n",
      "step:  2414 loss:  0.006839 Weight:  [ 1.00802338] bias:  [-0.05744511]\n",
      "step:  2415 loss:  0.00678386 Weight:  [ 1.00799108] bias:  [-0.05721343]\n",
      "step:  2416 loss:  0.00672928 Weight:  [ 1.00795889] bias:  [-0.05698269]\n",
      "step:  2417 loss:  0.00667515 Weight:  [ 1.0079267] bias:  [-0.05675289]\n",
      "step:  2418 loss:  0.00662144 Weight:  [ 1.00789475] bias:  [-0.05652401]\n",
      "step:  2419 loss:  0.00656808 Weight:  [ 1.00786293] bias:  [-0.05629605]\n",
      "step:  2420 loss:  0.00651527 Weight:  [ 1.00783122] bias:  [-0.05606902]\n",
      "step:  2421 loss:  0.00646275 Weight:  [ 1.00779963] bias:  [-0.0558429]\n",
      "step:  2422 loss:  0.00641073 Weight:  [ 1.00776827] bias:  [-0.05561768]\n",
      "step:  2423 loss:  0.0063592 Weight:  [ 1.0077368] bias:  [-0.05539339]\n",
      "step:  2424 loss:  0.006308 Weight:  [ 1.00770557] bias:  [-0.05517]\n",
      "step:  2425 loss:  0.00625725 Weight:  [ 1.00767457] bias:  [-0.05494749]\n",
      "step:  2426 loss:  0.00620682 Weight:  [ 1.0076437] bias:  [-0.05472589]\n",
      "step:  2427 loss:  0.00615692 Weight:  [ 1.00761282] bias:  [-0.0545052]\n",
      "step:  2428 loss:  0.00610731 Weight:  [ 1.00758207] bias:  [-0.05428539]\n",
      "step:  2429 loss:  0.00605821 Weight:  [ 1.00755155] bias:  [-0.05406646]\n",
      "step:  2430 loss:  0.0060094 Weight:  [ 1.00752115] bias:  [-0.05384842]\n",
      "step:  2431 loss:  0.00596104 Weight:  [ 1.00749075] bias:  [-0.05363126]\n",
      "step:  2432 loss:  0.00591308 Weight:  [ 1.00746047] bias:  [-0.05341498]\n",
      "step:  2433 loss:  0.00586546 Weight:  [ 1.00743043] bias:  [-0.05319956]\n",
      "step:  2434 loss:  0.00581823 Weight:  [ 1.00740051] bias:  [-0.05298501]\n",
      "step:  2435 loss:  0.00577139 Weight:  [ 1.00737071] bias:  [-0.05277133]\n",
      "step:  2436 loss:  0.00572496 Weight:  [ 1.00734091] bias:  [-0.05255852]\n",
      "step:  2437 loss:  0.00567884 Weight:  [ 1.00731134] bias:  [-0.05234655]\n",
      "step:  2438 loss:  0.00563317 Weight:  [ 1.00728178] bias:  [-0.05213545]\n",
      "step:  2439 loss:  0.00558783 Weight:  [ 1.00725245] bias:  [-0.05192519]\n",
      "step:  2440 loss:  0.00554282 Weight:  [ 1.00722325] bias:  [-0.05171578]\n",
      "step:  2441 loss:  0.0054982 Weight:  [ 1.00719404] bias:  [-0.05150722]\n",
      "step:  2442 loss:  0.00545399 Weight:  [ 1.00716507] bias:  [-0.0512995]\n",
      "step:  2443 loss:  0.00541007 Weight:  [ 1.00713611] bias:  [-0.05109262]\n",
      "step:  2444 loss:  0.00536657 Weight:  [ 1.00710738] bias:  [-0.05088657]\n",
      "step:  2445 loss:  0.00532334 Weight:  [ 1.00707865] bias:  [-0.05068135]\n",
      "step:  2446 loss:  0.00528046 Weight:  [ 1.00705016] bias:  [-0.05047695]\n",
      "step:  2447 loss:  0.00523795 Weight:  [ 1.00702178] bias:  [-0.05027338]\n",
      "step:  2448 loss:  0.00519586 Weight:  [ 1.00699341] bias:  [-0.05007064]\n",
      "step:  2449 loss:  0.00515393 Weight:  [ 1.00696528] bias:  [-0.04986871]\n",
      "step:  2450 loss:  0.00511249 Weight:  [ 1.00693715] bias:  [-0.0496676]\n",
      "step:  2451 loss:  0.00507133 Weight:  [ 1.00690913] bias:  [-0.0494673]\n",
      "step:  2452 loss:  0.00503056 Weight:  [ 1.00688124] bias:  [-0.04926781]\n",
      "step:  2453 loss:  0.00499003 Weight:  [ 1.00685346] bias:  [-0.04906911]\n",
      "step:  2454 loss:  0.00494984 Weight:  [ 1.00682592] bias:  [-0.04887121]\n",
      "step:  2455 loss:  0.00491003 Weight:  [ 1.00679839] bias:  [-0.04867413]\n",
      "step:  2456 loss:  0.00487043 Weight:  [ 1.00677097] bias:  [-0.04847783]\n",
      "step:  2457 loss:  0.00483129 Weight:  [ 1.00674367] bias:  [-0.04828232]\n",
      "step:  2458 loss:  0.00479234 Weight:  [ 1.00671649] bias:  [-0.0480876]\n",
      "step:  2459 loss:  0.0047538 Weight:  [ 1.00668943] bias:  [-0.04789367]\n",
      "step:  2460 loss:  0.00471557 Weight:  [ 1.00666237] bias:  [-0.04770053]\n",
      "step:  2461 loss:  0.00467757 Weight:  [ 1.00663555] bias:  [-0.04750816]\n",
      "step:  2462 loss:  0.00463996 Weight:  [ 1.00660872] bias:  [-0.04731657]\n",
      "step:  2463 loss:  0.00460261 Weight:  [ 1.00658214] bias:  [-0.04712574]\n",
      "step:  2464 loss:  0.00456556 Weight:  [ 1.00655556] bias:  [-0.04693569]\n",
      "step:  2465 loss:  0.00452877 Weight:  [ 1.00652909] bias:  [-0.0467464]\n",
      "step:  2466 loss:  0.00449232 Weight:  [ 1.00650275] bias:  [-0.04655788]\n",
      "step:  2467 loss:  0.0044562 Weight:  [ 1.00647652] bias:  [-0.04637012]\n",
      "step:  2468 loss:  0.00442024 Weight:  [ 1.00645041] bias:  [-0.04618311]\n",
      "step:  2469 loss:  0.00438478 Weight:  [ 1.00642443] bias:  [-0.04599686]\n",
      "step:  2470 loss:  0.00434943 Weight:  [ 1.00639856] bias:  [-0.04581136]\n",
      "step:  2471 loss:  0.00431441 Weight:  [ 1.00637281] bias:  [-0.0456266]\n",
      "step:  2472 loss:  0.0042797 Weight:  [ 1.00634706] bias:  [-0.0454426]\n",
      "step:  2473 loss:  0.00424525 Weight:  [ 1.00632143] bias:  [-0.04525934]\n",
      "step:  2474 loss:  0.00421104 Weight:  [ 1.00629592] bias:  [-0.04507681]\n",
      "step:  2475 loss:  0.00417723 Weight:  [ 1.00627053] bias:  [-0.04489503]\n",
      "step:  2476 loss:  0.00414351 Weight:  [ 1.00624526] bias:  [-0.04471397]\n",
      "step:  2477 loss:  0.00411018 Weight:  [ 1.0062201] bias:  [-0.04453364]\n",
      "step:  2478 loss:  0.00407708 Weight:  [ 1.00619507] bias:  [-0.04435404]\n",
      "step:  2479 loss:  0.00404426 Weight:  [ 1.00617003] bias:  [-0.04417518]\n",
      "step:  2480 loss:  0.00401175 Weight:  [ 1.00614512] bias:  [-0.04399703]\n",
      "step:  2481 loss:  0.00397945 Weight:  [ 1.00612032] bias:  [-0.0438196]\n",
      "step:  2482 loss:  0.00394741 Weight:  [ 1.00609565] bias:  [-0.04364288]\n",
      "step:  2483 loss:  0.00391566 Weight:  [ 1.00607109] bias:  [-0.04346687]\n",
      "step:  2484 loss:  0.00388414 Weight:  [ 1.00604653] bias:  [-0.04329158]\n",
      "step:  2485 loss:  0.0038528 Weight:  [ 1.00602221] bias:  [-0.04311698]\n",
      "step:  2486 loss:  0.00382189 Weight:  [ 1.0059979] bias:  [-0.0429431]\n",
      "step:  2487 loss:  0.00379105 Weight:  [ 1.0059737] bias:  [-0.04276992]\n",
      "step:  2488 loss:  0.00376055 Weight:  [ 1.00594962] bias:  [-0.04259742]\n",
      "step:  2489 loss:  0.00373026 Weight:  [ 1.00592566] bias:  [-0.04242563]\n",
      "step:  2490 loss:  0.00370027 Weight:  [ 1.00590169] bias:  [-0.04225454]\n",
      "step:  2491 loss:  0.00367049 Weight:  [ 1.00587797] bias:  [-0.04208412]\n",
      "step:  2492 loss:  0.00364097 Weight:  [ 1.00585425] bias:  [-0.04191441]\n",
      "step:  2493 loss:  0.00361165 Weight:  [ 1.00583065] bias:  [-0.04174538]\n",
      "step:  2494 loss:  0.00358259 Weight:  [ 1.00580716] bias:  [-0.04157703]\n",
      "step:  2495 loss:  0.00355368 Weight:  [ 1.00578368] bias:  [-0.04140936]\n",
      "step:  2496 loss:  0.00352509 Weight:  [ 1.00576031] bias:  [-0.04124236]\n",
      "step:  2497 loss:  0.0034967 Weight:  [ 1.00573719] bias:  [-0.04107603]\n",
      "step:  2498 loss:  0.00346862 Weight:  [ 1.00571394] bias:  [-0.04091038]\n",
      "step:  2499 loss:  0.00344066 Weight:  [ 1.00569093] bias:  [-0.04074538]\n",
      "step:  2500 loss:  0.00341296 Weight:  [ 1.00566804] bias:  [-0.04058106]\n",
      "step:  2501 loss:  0.00338553 Weight:  [ 1.00564516] bias:  [-0.0404174]\n",
      "step:  2502 loss:  0.00335824 Weight:  [ 1.00562239] bias:  [-0.0402544]\n",
      "step:  2503 loss:  0.00333121 Weight:  [ 1.00559974] bias:  [-0.04009206]\n",
      "step:  2504 loss:  0.00330439 Weight:  [ 1.00557709] bias:  [-0.03993038]\n",
      "step:  2505 loss:  0.0032778 Weight:  [ 1.00555468] bias:  [-0.03976934]\n",
      "step:  2506 loss:  0.00325142 Weight:  [ 1.00553226] bias:  [-0.03960896]\n",
      "step:  2507 loss:  0.00322524 Weight:  [ 1.00550997] bias:  [-0.03944923]\n",
      "step:  2508 loss:  0.00319925 Weight:  [ 1.00548768] bias:  [-0.03929014]\n",
      "step:  2509 loss:  0.0031735 Weight:  [ 1.00546563] bias:  [-0.03913168]\n",
      "step:  2510 loss:  0.00314794 Weight:  [ 1.00544357] bias:  [-0.03897387]\n",
      "step:  2511 loss:  0.00312265 Weight:  [ 1.00542164] bias:  [-0.0388167]\n",
      "step:  2512 loss:  0.00309753 Weight:  [ 1.0053997] bias:  [-0.03866016]\n",
      "step:  2513 loss:  0.00307258 Weight:  [ 1.00537789] bias:  [-0.03850425]\n",
      "step:  2514 loss:  0.00304781 Weight:  [ 1.00535631] bias:  [-0.03834897]\n",
      "step:  2515 loss:  0.00302329 Weight:  [ 1.00533473] bias:  [-0.03819431]\n",
      "step:  2516 loss:  0.00299898 Weight:  [ 1.00531316] bias:  [-0.03804029]\n",
      "step:  2517 loss:  0.00297483 Weight:  [ 1.0052917] bias:  [-0.03788688]\n",
      "step:  2518 loss:  0.00295093 Weight:  [ 1.00527036] bias:  [-0.03773408]\n",
      "step:  2519 loss:  0.00292714 Weight:  [ 1.00524914] bias:  [-0.03758191]\n",
      "step:  2520 loss:  0.00290359 Weight:  [ 1.00522792] bias:  [-0.03743035]\n",
      "step:  2521 loss:  0.0028802 Weight:  [ 1.00520682] bias:  [-0.03727939]\n",
      "step:  2522 loss:  0.00285705 Weight:  [ 1.00518584] bias:  [-0.03712905]\n",
      "step:  2523 loss:  0.00283403 Weight:  [ 1.00516498] bias:  [-0.03697931]\n",
      "step:  2524 loss:  0.0028112 Weight:  [ 1.00514412] bias:  [-0.03683018]\n",
      "step:  2525 loss:  0.00278859 Weight:  [ 1.00512338] bias:  [-0.03668165]\n",
      "step:  2526 loss:  0.00276609 Weight:  [ 1.00510275] bias:  [-0.03653372]\n",
      "step:  2527 loss:  0.00274389 Weight:  [ 1.00508213] bias:  [-0.03638639]\n",
      "step:  2528 loss:  0.00272179 Weight:  [ 1.00506163] bias:  [-0.03623965]\n",
      "step:  2529 loss:  0.00269988 Weight:  [ 1.00504124] bias:  [-0.0360935]\n",
      "step:  2530 loss:  0.00267811 Weight:  [ 1.00502086] bias:  [-0.03594793]\n",
      "step:  2531 loss:  0.00265661 Weight:  [ 1.00500059] bias:  [-0.03580296]\n",
      "step:  2532 loss:  0.0026352 Weight:  [ 1.00498044] bias:  [-0.03565857]\n",
      "step:  2533 loss:  0.00261398 Weight:  [ 1.00496042] bias:  [-0.03551476]\n",
      "step:  2534 loss:  0.00259296 Weight:  [ 1.00494039] bias:  [-0.03537153]\n",
      "step:  2535 loss:  0.00257207 Weight:  [ 1.00492048] bias:  [-0.03522888]\n",
      "step:  2536 loss:  0.00255139 Weight:  [ 1.00490057] bias:  [-0.03508681]\n",
      "step:  2537 loss:  0.00253084 Weight:  [ 1.00488079] bias:  [-0.03494531]\n",
      "step:  2538 loss:  0.00251047 Weight:  [ 1.00486112] bias:  [-0.03480437]\n",
      "step:  2539 loss:  0.00249023 Weight:  [ 1.00484157] bias:  [-0.03466401]\n",
      "step:  2540 loss:  0.00247022 Weight:  [ 1.00482202] bias:  [-0.03452421]\n",
      "step:  2541 loss:  0.00245032 Weight:  [ 1.00480258] bias:  [-0.03438498]\n",
      "step:  2542 loss:  0.00243063 Weight:  [ 1.00478315] bias:  [-0.03424631]\n",
      "step:  2543 loss:  0.00241103 Weight:  [ 1.00476396] bias:  [-0.03410819]\n",
      "step:  2544 loss:  0.00239163 Weight:  [ 1.00474477] bias:  [-0.03397064]\n",
      "step:  2545 loss:  0.00237235 Weight:  [ 1.00472558] bias:  [-0.03383364]\n",
      "step:  2546 loss:  0.00235327 Weight:  [ 1.0047065] bias:  [-0.03369719]\n",
      "step:  2547 loss:  0.00233432 Weight:  [ 1.00468755] bias:  [-0.03356129]\n",
      "step:  2548 loss:  0.0023155 Weight:  [ 1.00466871] bias:  [-0.03342594]\n",
      "step:  2549 loss:  0.0022969 Weight:  [ 1.00464988] bias:  [-0.03329115]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step:  2550 loss:  0.00227845 Weight:  [ 1.00463104] bias:  [-0.03315689]\n",
      "step:  2551 loss:  0.00226005 Weight:  [ 1.00461245] bias:  [-0.03302317]\n",
      "step:  2552 loss:  0.00224189 Weight:  [ 1.00459385] bias:  [-0.03289]\n",
      "step:  2553 loss:  0.00222383 Weight:  [ 1.00457525] bias:  [-0.03275736]\n",
      "step:  2554 loss:  0.00220592 Weight:  [ 1.00455678] bias:  [-0.03262526]\n",
      "step:  2555 loss:  0.00218816 Weight:  [ 1.00453842] bias:  [-0.03249368]\n",
      "step:  2556 loss:  0.00217058 Weight:  [ 1.00452006] bias:  [-0.03236264]\n",
      "step:  2557 loss:  0.00215307 Weight:  [ 1.00450194] bias:  [-0.03223211]\n",
      "step:  2558 loss:  0.00213575 Weight:  [ 1.00448382] bias:  [-0.03210213]\n",
      "step:  2559 loss:  0.00211854 Weight:  [ 1.0044657] bias:  [-0.03197267]\n",
      "step:  2560 loss:  0.0021015 Weight:  [ 1.0044477] bias:  [-0.03184373]\n",
      "step:  2561 loss:  0.0020846 Weight:  [ 1.0044297] bias:  [-0.03171531]\n",
      "step:  2562 loss:  0.00206784 Weight:  [ 1.00441182] bias:  [-0.03158741]\n",
      "step:  2563 loss:  0.00205115 Weight:  [ 1.00439405] bias:  [-0.03146001]\n",
      "step:  2564 loss:  0.00203469 Weight:  [ 1.00437629] bias:  [-0.03133314]\n",
      "step:  2565 loss:  0.00201831 Weight:  [ 1.00435865] bias:  [-0.03120678]\n",
      "step:  2566 loss:  0.00200204 Weight:  [ 1.00434113] bias:  [-0.03108092]\n",
      "step:  2567 loss:  0.00198592 Weight:  [ 1.0043236] bias:  [-0.03095557]\n",
      "step:  2568 loss:  0.00196995 Weight:  [ 1.0043062] bias:  [-0.03083073]\n",
      "step:  2569 loss:  0.0019541 Weight:  [ 1.00428879] bias:  [-0.0307064]\n",
      "step:  2570 loss:  0.00193834 Weight:  [ 1.00427151] bias:  [-0.03058256]\n",
      "step:  2571 loss:  0.00192276 Weight:  [ 1.00425422] bias:  [-0.03045923]\n",
      "step:  2572 loss:  0.00190728 Weight:  [ 1.00423717] bias:  [-0.03033638]\n",
      "step:  2573 loss:  0.00189192 Weight:  [ 1.00422001] bias:  [-0.03021405]\n",
      "step:  2574 loss:  0.00187668 Weight:  [ 1.00420296] bias:  [-0.03009219]\n",
      "step:  2575 loss:  0.00186157 Weight:  [ 1.00418603] bias:  [-0.02997083]\n",
      "step:  2576 loss:  0.0018466 Weight:  [ 1.00416923] bias:  [-0.02984996]\n",
      "step:  2577 loss:  0.00183176 Weight:  [ 1.00415242] bias:  [-0.02972958]\n",
      "step:  2578 loss:  0.00181697 Weight:  [ 1.00413561] bias:  [-0.02960969]\n",
      "step:  2579 loss:  0.00180236 Weight:  [ 1.00411892] bias:  [-0.02949027]\n",
      "step:  2580 loss:  0.00178785 Weight:  [ 1.00410235] bias:  [-0.02937134]\n",
      "step:  2581 loss:  0.00177346 Weight:  [ 1.00408578] bias:  [-0.02925289]\n",
      "step:  2582 loss:  0.00175918 Weight:  [ 1.00406933] bias:  [-0.02913491]\n",
      "step:  2583 loss:  0.00174504 Weight:  [ 1.00405288] bias:  [-0.02901742]\n",
      "step:  2584 loss:  0.001731 Weight:  [ 1.00403655] bias:  [-0.02890039]\n",
      "step:  2585 loss:  0.00171706 Weight:  [ 1.00402021] bias:  [-0.02878384]\n",
      "step:  2586 loss:  0.00170323 Weight:  [ 1.004004] bias:  [-0.02866776]\n",
      "step:  2587 loss:  0.00168952 Weight:  [ 1.00398791] bias:  [-0.02855214]\n",
      "step:  2588 loss:  0.00167591 Weight:  [ 1.00397182] bias:  [-0.02843699]\n",
      "step:  2589 loss:  0.00166246 Weight:  [ 1.00395584] bias:  [-0.02832231]\n",
      "step:  2590 loss:  0.00164905 Weight:  [ 1.00393987] bias:  [-0.0282081]\n",
      "step:  2591 loss:  0.00163577 Weight:  [ 1.00392401] bias:  [-0.02809434]\n",
      "step:  2592 loss:  0.0016226 Weight:  [ 1.00390816] bias:  [-0.02798104]\n",
      "step:  2593 loss:  0.00160955 Weight:  [ 1.00389242] bias:  [-0.0278682]\n",
      "step:  2594 loss:  0.0015966 Weight:  [ 1.00387669] bias:  [-0.02775582]\n",
      "step:  2595 loss:  0.00158374 Weight:  [ 1.00386107] bias:  [-0.02764388]\n",
      "step:  2596 loss:  0.00157097 Weight:  [ 1.00384545] bias:  [-0.0275324]\n",
      "step:  2597 loss:  0.00155832 Weight:  [ 1.00382996] bias:  [-0.02742136]\n",
      "step:  2598 loss:  0.00154582 Weight:  [ 1.00381446] bias:  [-0.02731077]\n",
      "step:  2599 loss:  0.00153339 Weight:  [ 1.00379908] bias:  [-0.02720062]\n",
      "step:  2600 loss:  0.00152101 Weight:  [ 1.00378382] bias:  [-0.02709092]\n",
      "step:  2601 loss:  0.00150877 Weight:  [ 1.00376856] bias:  [-0.02698167]\n",
      "step:  2602 loss:  0.00149663 Weight:  [ 1.0037533] bias:  [-0.02687286]\n",
      "step:  2603 loss:  0.00148458 Weight:  [ 1.00373816] bias:  [-0.02676448]\n",
      "step:  2604 loss:  0.00147262 Weight:  [ 1.00372314] bias:  [-0.02665653]\n",
      "step:  2605 loss:  0.00146079 Weight:  [ 1.00370812] bias:  [-0.02654903]\n",
      "step:  2606 loss:  0.00144899 Weight:  [ 1.00369322] bias:  [-0.02644196]\n",
      "step:  2607 loss:  0.00143737 Weight:  [ 1.00367832] bias:  [-0.02633533]\n",
      "step:  2608 loss:  0.00142576 Weight:  [ 1.00366354] bias:  [-0.02622912]\n",
      "step:  2609 loss:  0.00141432 Weight:  [ 1.00364864] bias:  [-0.02612336]\n",
      "step:  2610 loss:  0.00140295 Weight:  [ 1.00363398] bias:  [-0.026018]\n",
      "step:  2611 loss:  0.00139162 Weight:  [ 1.00361931] bias:  [-0.02591307]\n",
      "step:  2612 loss:  0.00138041 Weight:  [ 1.00360477] bias:  [-0.02580857]\n",
      "step:  2613 loss:  0.00136934 Weight:  [ 1.00359023] bias:  [-0.02570449]\n",
      "step:  2614 loss:  0.00135829 Weight:  [ 1.00357568] bias:  [-0.02560084]\n",
      "step:  2615 loss:  0.00134739 Weight:  [ 1.00356126] bias:  [-0.02549759]\n",
      "step:  2616 loss:  0.00133651 Weight:  [ 1.00354695] bias:  [-0.02539476]\n",
      "step:  2617 loss:  0.00132575 Weight:  [ 1.00353265] bias:  [-0.02529235]\n",
      "step:  2618 loss:  0.00131508 Weight:  [ 1.00351834] bias:  [-0.02519035]\n",
      "step:  2619 loss:  0.00130449 Weight:  [ 1.00350416] bias:  [-0.02508876]\n",
      "step:  2620 loss:  0.00129403 Weight:  [ 1.00348997] bias:  [-0.02498758]\n",
      "step:  2621 loss:  0.00128361 Weight:  [ 1.0034759] bias:  [-0.0248868]\n",
      "step:  2622 loss:  0.00127325 Weight:  [ 1.00346196] bias:  [-0.02478643]\n",
      "step:  2623 loss:  0.00126301 Weight:  [ 1.00344801] bias:  [-0.02468647]\n",
      "step:  2624 loss:  0.00125283 Weight:  [ 1.00343406] bias:  [-0.02458692]\n",
      "step:  2625 loss:  0.00124277 Weight:  [ 1.00342023] bias:  [-0.02448776]\n",
      "step:  2626 loss:  0.00123276 Weight:  [ 1.00340641] bias:  [-0.024389]\n",
      "step:  2627 loss:  0.00122283 Weight:  [ 1.0033927] bias:  [-0.02429064]\n",
      "step:  2628 loss:  0.00121297 Weight:  [ 1.00337899] bias:  [-0.02419268]\n",
      "step:  2629 loss:  0.0012032 Weight:  [ 1.0033654] bias:  [-0.02409511]\n",
      "step:  2630 loss:  0.00119352 Weight:  [ 1.00335181] bias:  [-0.02399793]\n",
      "step:  2631 loss:  0.00118391 Weight:  [ 1.00333834] bias:  [-0.02390115]\n",
      "step:  2632 loss:  0.00117439 Weight:  [ 1.00332487] bias:  [-0.02380476]\n",
      "step:  2633 loss:  0.00116495 Weight:  [ 1.0033114] bias:  [-0.02370876]\n",
      "step:  2634 loss:  0.00115557 Weight:  [ 1.00329804] bias:  [-0.02361315]\n",
      "step:  2635 loss:  0.00114625 Weight:  [ 1.00328481] bias:  [-0.02351792]\n",
      "step:  2636 loss:  0.00113705 Weight:  [ 1.00327158] bias:  [-0.02342308]\n",
      "step:  2637 loss:  0.00112788 Weight:  [ 1.00325835] bias:  [-0.02332862]\n",
      "step:  2638 loss:  0.00111879 Weight:  [ 1.00324523] bias:  [-0.02323454]\n",
      "step:  2639 loss:  0.00110982 Weight:  [ 1.00323212] bias:  [-0.02314084]\n",
      "step:  2640 loss:  0.00110085 Weight:  [ 1.00321913] bias:  [-0.02304751]\n",
      "step:  2641 loss:  0.00109201 Weight:  [ 1.00320613] bias:  [-0.02295457]\n",
      "step:  2642 loss:  0.00108321 Weight:  [ 1.00319314] bias:  [-0.022862]\n",
      "step:  2643 loss:  0.00107446 Weight:  [ 1.00318027] bias:  [-0.0227698]\n",
      "step:  2644 loss:  0.00106585 Weight:  [ 1.00316751] bias:  [-0.02267797]\n",
      "step:  2645 loss:  0.00105725 Weight:  [ 1.00315475] bias:  [-0.02258651]\n",
      "step:  2646 loss:  0.00104875 Weight:  [ 1.003142] bias:  [-0.02249543]\n",
      "step:  2647 loss:  0.00104032 Weight:  [ 1.00312924] bias:  [-0.02240472]\n",
      "step:  2648 loss:  0.00103193 Weight:  [ 1.00311661] bias:  [-0.02231435]\n",
      "step:  2649 loss:  0.00102364 Weight:  [ 1.00310409] bias:  [-0.02222436]\n",
      "step:  2650 loss:  0.0010154 Weight:  [ 1.00309157] bias:  [-0.02213473]\n",
      "step:  2651 loss:  0.00100723 Weight:  [ 1.00307906] bias:  [-0.02204546]\n",
      "step:  2652 loss:  0.000999114 Weight:  [ 1.00306666] bias:  [-0.02195655]\n",
      "step:  2653 loss:  0.000991063 Weight:  [ 1.00305426] bias:  [-0.021868]\n",
      "step:  2654 loss:  0.000983095 Weight:  [ 1.00304198] bias:  [-0.02177981]\n",
      "step:  2655 loss:  0.00097518 Weight:  [ 1.0030297] bias:  [-0.02169197]\n",
      "step:  2656 loss:  0.000967328 Weight:  [ 1.00301754] bias:  [-0.02160448]\n",
      "step:  2657 loss:  0.000959534 Weight:  [ 1.00300539] bias:  [-0.02151735]\n",
      "step:  2658 loss:  0.000951806 Weight:  [ 1.00299323] bias:  [-0.02143058]\n",
      "step:  2659 loss:  0.000944133 Weight:  [ 1.00298119] bias:  [-0.02134415]\n",
      "step:  2660 loss:  0.000936556 Weight:  [ 1.00296915] bias:  [-0.02125807]\n",
      "step:  2661 loss:  0.000929013 Weight:  [ 1.00295722] bias:  [-0.02117234]\n",
      "step:  2662 loss:  0.000921541 Weight:  [ 1.0029453] bias:  [-0.02108696]\n",
      "step:  2663 loss:  0.000914134 Weight:  [ 1.00293338] bias:  [-0.02100192]\n",
      "step:  2664 loss:  0.000906772 Weight:  [ 1.00292158] bias:  [-0.02091723]\n",
      "step:  2665 loss:  0.000899478 Weight:  [ 1.00290978] bias:  [-0.02083288]\n",
      "step:  2666 loss:  0.000892209 Weight:  [ 1.00289798] bias:  [-0.02074886]\n",
      "step:  2667 loss:  0.000885047 Weight:  [ 1.0028863] bias:  [-0.02066518]\n",
      "step:  2668 loss:  0.000877912 Weight:  [ 1.00287473] bias:  [-0.02058183]\n",
      "step:  2669 loss:  0.00087084 Weight:  [ 1.00286317] bias:  [-0.02049883]\n",
      "step:  2670 loss:  0.000863816 Weight:  [ 1.00285161] bias:  [-0.02041617]\n",
      "step:  2671 loss:  0.000856879 Weight:  [ 1.00284004] bias:  [-0.02033384]\n",
      "step:  2672 loss:  0.000849976 Weight:  [ 1.0028286] bias:  [-0.02025183]\n",
      "step:  2673 loss:  0.000843133 Weight:  [ 1.00281715] bias:  [-0.02017015]\n",
      "step:  2674 loss:  0.000836364 Weight:  [ 1.00280583] bias:  [-0.0200888]\n",
      "step:  2675 loss:  0.000829611 Weight:  [ 1.0027945] bias:  [-0.02000779]\n",
      "step:  2676 loss:  0.000822931 Weight:  [ 1.0027833] bias:  [-0.0199271]\n",
      "step:  2677 loss:  0.000816322 Weight:  [ 1.00277209] bias:  [-0.01984674]\n",
      "step:  2678 loss:  0.000809758 Weight:  [ 1.00276089] bias:  [-0.01976671]\n",
      "step:  2679 loss:  0.000803239 Weight:  [ 1.00274968] bias:  [-0.01968699]\n",
      "step:  2680 loss:  0.000796769 Weight:  [ 1.0027386] bias:  [-0.01960759]\n",
      "step:  2681 loss:  0.000790352 Weight:  [ 1.00272751] bias:  [-0.01952852]\n",
      "step:  2682 loss:  0.000784003 Weight:  [ 1.00271654] bias:  [-0.01944976]\n",
      "step:  2683 loss:  0.000777665 Weight:  [ 1.00270569] bias:  [-0.01937131]\n",
      "step:  2684 loss:  0.000771425 Weight:  [ 1.00269473] bias:  [-0.0192932]\n",
      "step:  2685 loss:  0.000765228 Weight:  [ 1.00268388] bias:  [-0.0192154]\n",
      "step:  2686 loss:  0.00075904 Weight:  [ 1.00267303] bias:  [-0.0191379]\n",
      "step:  2687 loss:  0.000752941 Weight:  [ 1.0026623] bias:  [-0.01906072]\n",
      "step:  2688 loss:  0.000746867 Weight:  [ 1.00265157] bias:  [-0.01898386]\n",
      "step:  2689 loss:  0.000740878 Weight:  [ 1.00264084] bias:  [-0.0189073]\n",
      "step:  2690 loss:  0.000734895 Weight:  [ 1.00263023] bias:  [-0.01883106]\n",
      "step:  2691 loss:  0.00072899 Weight:  [ 1.00261962] bias:  [-0.01875512]\n",
      "step:  2692 loss:  0.000723144 Weight:  [ 1.00260901] bias:  [-0.01867949]\n",
      "step:  2693 loss:  0.000717312 Weight:  [ 1.00259852] bias:  [-0.01860416]\n",
      "step:  2694 loss:  0.000711546 Weight:  [ 1.00258803] bias:  [-0.01852914]\n",
      "step:  2695 loss:  0.000705823 Weight:  [ 1.00257754] bias:  [-0.01845442]\n",
      "step:  2696 loss:  0.00070013 Weight:  [ 1.00256717] bias:  [-0.01837999]\n",
      "step:  2697 loss:  0.000694495 Weight:  [ 1.0025568] bias:  [-0.01830587]\n",
      "step:  2698 loss:  0.000688901 Weight:  [ 1.00254643] bias:  [-0.01823205]\n",
      "step:  2699 loss:  0.000683337 Weight:  [ 1.00253618] bias:  [-0.01815851]\n",
      "step:  2700 loss:  0.000677844 Weight:  [ 1.00252604] bias:  [-0.01808527]\n",
      "step:  2701 loss:  0.0006724 Weight:  [ 1.00251579] bias:  [-0.01801234]\n",
      "step:  2702 loss:  0.000666991 Weight:  [ 1.00250566] bias:  [-0.0179397]\n",
      "step:  2703 loss:  0.000661626 Weight:  [ 1.00249553] bias:  [-0.01786735]\n",
      "step:  2704 loss:  0.000656306 Weight:  [ 1.00248551] bias:  [-0.01779529]\n",
      "step:  2705 loss:  0.000650991 Weight:  [ 1.0024755] bias:  [-0.01772352]\n",
      "step:  2706 loss:  0.00064579 Weight:  [ 1.00246549] bias:  [-0.01765206]\n",
      "step:  2707 loss:  0.000640546 Weight:  [ 1.00245559] bias:  [-0.01758086]\n",
      "step:  2708 loss:  0.000635415 Weight:  [ 1.0024457] bias:  [-0.01750997]\n",
      "step:  2709 loss:  0.00063028 Weight:  [ 1.0024358] bias:  [-0.01743935]\n",
      "step:  2710 loss:  0.000625231 Weight:  [ 1.00242591] bias:  [-0.01736903]\n",
      "step:  2711 loss:  0.000620194 Weight:  [ 1.00241613] bias:  [-0.01729898]\n",
      "step:  2712 loss:  0.000615211 Weight:  [ 1.00240636] bias:  [-0.01722921]\n",
      "step:  2713 loss:  0.000610245 Weight:  [ 1.0023967] bias:  [-0.01715972]\n",
      "step:  2714 loss:  0.000605334 Weight:  [ 1.00238705] bias:  [-0.01709052]\n",
      "step:  2715 loss:  0.000600456 Weight:  [ 1.00237739] bias:  [-0.01702159]\n",
      "step:  2716 loss:  0.000595649 Weight:  [ 1.00236785] bias:  [-0.01695294]\n",
      "step:  2717 loss:  0.000590857 Weight:  [ 1.00235832] bias:  [-0.01688458]\n",
      "step:  2718 loss:  0.000586082 Weight:  [ 1.00234878] bias:  [-0.01681649]\n",
      "step:  2719 loss:  0.000581376 Weight:  [ 1.00233924] bias:  [-0.01674867]\n",
      "step:  2720 loss:  0.000576697 Weight:  [ 1.00232983] bias:  [-0.01668112]\n",
      "step:  2721 loss:  0.000572052 Weight:  [ 1.00232041] bias:  [-0.01661384]\n",
      "step:  2722 loss:  0.000567428 Weight:  [ 1.00231111] bias:  [-0.01654683]\n",
      "step:  2723 loss:  0.000562857 Weight:  [ 1.00230181] bias:  [-0.0164801]\n",
      "step:  2724 loss:  0.000558326 Weight:  [ 1.00229251] bias:  [-0.01641364]\n",
      "step:  2725 loss:  0.000553851 Weight:  [ 1.00228322] bias:  [-0.01634744]\n",
      "step:  2726 loss:  0.000549363 Weight:  [ 1.00227404] bias:  [-0.01628151]\n",
      "step:  2727 loss:  0.000544969 Weight:  [ 1.00226486] bias:  [-0.01621585]\n",
      "step:  2728 loss:  0.000540568 Weight:  [ 1.0022558] bias:  [-0.01615044]\n",
      "step:  2729 loss:  0.000536228 Weight:  [ 1.00224662] bias:  [-0.01608532]\n",
      "step:  2730 loss:  0.000531907 Weight:  [ 1.00223756] bias:  [-0.01602044]\n",
      "step:  2731 loss:  0.000527608 Weight:  [ 1.00222862] bias:  [-0.01595583]\n",
      "step:  2732 loss:  0.000523385 Weight:  [ 1.00221956] bias:  [-0.01589148]\n",
      "step:  2733 loss:  0.000519179 Weight:  [ 1.00221062] bias:  [-0.01582739]\n",
      "step:  2734 loss:  0.000514991 Weight:  [ 1.00220168] bias:  [-0.01576356]\n",
      "step:  2735 loss:  0.000510842 Weight:  [ 1.00219285] bias:  [-0.01569999]\n",
      "step:  2736 loss:  0.000506725 Weight:  [ 1.00218403] bias:  [-0.01563667]\n",
      "step:  2737 loss:  0.000502647 Weight:  [ 1.00217521] bias:  [-0.01557362]\n",
      "step:  2738 loss:  0.000498609 Weight:  [ 1.00216639] bias:  [-0.01551081]\n",
      "step:  2739 loss:  0.000494592 Weight:  [ 1.00215769] bias:  [-0.01544826]\n",
      "step:  2740 loss:  0.000490598 Weight:  [ 1.00214899] bias:  [-0.01538596]\n",
      "step:  2741 loss:  0.00048667 Weight:  [ 1.00214028] bias:  [-0.01532391]\n",
      "step:  2742 loss:  0.00048274 Weight:  [ 1.0021317] bias:  [-0.01526211]\n",
      "step:  2743 loss:  0.000478861 Weight:  [ 1.00212312] bias:  [-0.01520056]\n",
      "step:  2744 loss:  0.000474996 Weight:  [ 1.00211453] bias:  [-0.01513926]\n",
      "step:  2745 loss:  0.000471164 Weight:  [ 1.00210607] bias:  [-0.01507821]\n",
      "step:  2746 loss:  0.000467383 Weight:  [ 1.00209749] bias:  [-0.0150174]\n",
      "step:  2747 loss:  0.000463628 Weight:  [ 1.00208902] bias:  [-0.01495684]\n",
      "step:  2748 loss:  0.0004599 Weight:  [ 1.00208056] bias:  [-0.01489651]\n",
      "step:  2749 loss:  0.000456191 Weight:  [ 1.00207222] bias:  [-0.01483644]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step:  2750 loss:  0.000452529 Weight:  [ 1.00206387] bias:  [-0.0147766]\n",
      "step:  2751 loss:  0.000448879 Weight:  [ 1.00205553] bias:  [-0.01471701]\n",
      "step:  2752 loss:  0.000445264 Weight:  [ 1.0020473] bias:  [-0.01465766]\n",
      "step:  2753 loss:  0.000441679 Weight:  [ 1.00203896] bias:  [-0.01459855]\n",
      "step:  2754 loss:  0.000438115 Weight:  [ 1.00203073] bias:  [-0.01453967]\n",
      "step:  2755 loss:  0.000434599 Weight:  [ 1.00202262] bias:  [-0.01448103]\n",
      "step:  2756 loss:  0.000431115 Weight:  [ 1.0020144] bias:  [-0.01442263]\n",
      "step:  2757 loss:  0.000427624 Weight:  [ 1.00200629] bias:  [-0.01436447]\n",
      "step:  2758 loss:  0.000424197 Weight:  [ 1.00199819] bias:  [-0.01430654]\n",
      "step:  2759 loss:  0.000420774 Weight:  [ 1.00199008] bias:  [-0.01424884]\n",
      "step:  2760 loss:  0.00041739 Weight:  [ 1.00198209] bias:  [-0.01419137]\n",
      "step:  2761 loss:  0.000414021 Weight:  [ 1.00197411] bias:  [-0.01413413]\n",
      "step:  2762 loss:  0.000410684 Weight:  [ 1.00196612] bias:  [-0.01407713]\n",
      "step:  2763 loss:  0.000407382 Weight:  [ 1.00195825] bias:  [-0.01402035]\n",
      "step:  2764 loss:  0.000404118 Weight:  [ 1.00195038] bias:  [-0.01396381]\n",
      "step:  2765 loss:  0.000400844 Weight:  [ 1.00194252] bias:  [-0.0139075]\n",
      "step:  2766 loss:  0.000397632 Weight:  [ 1.00193465] bias:  [-0.01385142]\n",
      "step:  2767 loss:  0.000394427 Weight:  [ 1.00192678] bias:  [-0.01379556]\n",
      "step:  2768 loss:  0.000391248 Weight:  [ 1.00191903] bias:  [-0.01373992]\n",
      "step:  2769 loss:  0.000388099 Weight:  [ 1.00191128] bias:  [-0.0136845]\n",
      "step:  2770 loss:  0.000384974 Weight:  [ 1.00190353] bias:  [-0.01362931]\n",
      "step:  2771 loss:  0.000381882 Weight:  [ 1.0018959] bias:  [-0.01357434]\n",
      "step:  2772 loss:  0.000378798 Weight:  [ 1.00188828] bias:  [-0.01351959]\n",
      "step:  2773 loss:  0.000375759 Weight:  [ 1.00188065] bias:  [-0.01346507]\n",
      "step:  2774 loss:  0.000372732 Weight:  [ 1.00187314] bias:  [-0.01341076]\n",
      "step:  2775 loss:  0.000369727 Weight:  [ 1.00186551] bias:  [-0.01335668]\n",
      "step:  2776 loss:  0.000366756 Weight:  [ 1.001858] bias:  [-0.01330282]\n",
      "step:  2777 loss:  0.000363793 Weight:  [ 1.00185049] bias:  [-0.01324916]\n",
      "step:  2778 loss:  0.00036087 Weight:  [ 1.00184309] bias:  [-0.01319572]\n",
      "step:  2779 loss:  0.000357959 Weight:  [ 1.00183558] bias:  [-0.01314251]\n",
      "step:  2780 loss:  0.000355092 Weight:  [ 1.00182819] bias:  [-0.0130895]\n",
      "step:  2781 loss:  0.000352231 Weight:  [ 1.0018208] bias:  [-0.01303671]\n",
      "step:  2782 loss:  0.000349385 Weight:  [ 1.00181353] bias:  [-0.01298413]\n",
      "step:  2783 loss:  0.000346572 Weight:  [ 1.00180626] bias:  [-0.01293177]\n",
      "step:  2784 loss:  0.000343791 Weight:  [ 1.00179899] bias:  [-0.01287962]\n",
      "step:  2785 loss:  0.00034103 Weight:  [ 1.00179172] bias:  [-0.01282769]\n",
      "step:  2786 loss:  0.000338279 Weight:  [ 1.00178444] bias:  [-0.01277596]\n",
      "step:  2787 loss:  0.000335539 Weight:  [ 1.00177729] bias:  [-0.01272444]\n",
      "step:  2788 loss:  0.000332841 Weight:  [ 1.00177014] bias:  [-0.01267312]\n",
      "step:  2789 loss:  0.000330164 Weight:  [ 1.00176299] bias:  [-0.01262202]\n",
      "step:  2790 loss:  0.000327518 Weight:  [ 1.00175583] bias:  [-0.01257113]\n",
      "step:  2791 loss:  0.00032487 Weight:  [ 1.0017488] bias:  [-0.01252043]\n",
      "step:  2792 loss:  0.000322262 Weight:  [ 1.00174177] bias:  [-0.01246994]\n",
      "step:  2793 loss:  0.00031967 Weight:  [ 1.00173473] bias:  [-0.01241965]\n",
      "step:  2794 loss:  0.0003171 Weight:  [ 1.0017277] bias:  [-0.01236957]\n",
      "step:  2795 loss:  0.000314555 Weight:  [ 1.00172067] bias:  [-0.0123197]\n",
      "step:  2796 loss:  0.000312013 Weight:  [ 1.00171375] bias:  [-0.01227001]\n",
      "step:  2797 loss:  0.000309503 Weight:  [ 1.00170684] bias:  [-0.01222052]\n",
      "step:  2798 loss:  0.000307022 Weight:  [ 1.00169992] bias:  [-0.01217124]\n",
      "step:  2799 loss:  0.000304548 Weight:  [ 1.00169313] bias:  [-0.01212215]\n",
      "step:  2800 loss:  0.000302092 Weight:  [ 1.00168633] bias:  [-0.01207326]\n",
      "step:  2801 loss:  0.000299647 Weight:  [ 1.00167954] bias:  [-0.01202458]\n",
      "step:  2802 loss:  0.000297235 Weight:  [ 1.00167274] bias:  [-0.01197609]\n",
      "step:  2803 loss:  0.000294849 Weight:  [ 1.00166595] bias:  [-0.01192779]\n",
      "step:  2804 loss:  0.00029249 Weight:  [ 1.00165927] bias:  [-0.01187969]\n",
      "step:  2805 loss:  0.000290128 Weight:  [ 1.0016526] bias:  [-0.01183178]\n",
      "step:  2806 loss:  0.000287786 Weight:  [ 1.00164592] bias:  [-0.01178406]\n",
      "step:  2807 loss:  0.000285489 Weight:  [ 1.00163925] bias:  [-0.01173654]\n",
      "step:  2808 loss:  0.000283177 Weight:  [ 1.00163269] bias:  [-0.01168921]\n",
      "step:  2809 loss:  0.000280896 Weight:  [ 1.00162601] bias:  [-0.01164207]\n",
      "step:  2810 loss:  0.000278627 Weight:  [ 1.00161946] bias:  [-0.01159512]\n",
      "step:  2811 loss:  0.000276391 Weight:  [ 1.0016129] bias:  [-0.01154835]\n",
      "step:  2812 loss:  0.000274166 Weight:  [ 1.00160646] bias:  [-0.01150177]\n",
      "step:  2813 loss:  0.000271967 Weight:  [ 1.00160003] bias:  [-0.01145539]\n",
      "step:  2814 loss:  0.000269774 Weight:  [ 1.00159359] bias:  [-0.01140919]\n",
      "step:  2815 loss:  0.000267608 Weight:  [ 1.00158715] bias:  [-0.01136319]\n",
      "step:  2816 loss:  0.000265444 Weight:  [ 1.00158072] bias:  [-0.01131737]\n",
      "step:  2817 loss:  0.000263311 Weight:  [ 1.00157428] bias:  [-0.01127173]\n",
      "step:  2818 loss:  0.0002612 Weight:  [ 1.00156796] bias:  [-0.01122627]\n",
      "step:  2819 loss:  0.000259074 Weight:  [ 1.00156164] bias:  [-0.01118099]\n",
      "step:  2820 loss:  0.000257004 Weight:  [ 1.00155532] bias:  [-0.0111359]\n",
      "step:  2821 loss:  0.000254934 Weight:  [ 1.00154912] bias:  [-0.01109098]\n",
      "step:  2822 loss:  0.000252879 Weight:  [ 1.00154293] bias:  [-0.01104626]\n",
      "step:  2823 loss:  0.000250847 Weight:  [ 1.00153661] bias:  [-0.01100171]\n",
      "step:  2824 loss:  0.000248825 Weight:  [ 1.00153041] bias:  [-0.01095734]\n",
      "step:  2825 loss:  0.00024682 Weight:  [ 1.00152421] bias:  [-0.01091315]\n",
      "step:  2826 loss:  0.000244835 Weight:  [ 1.00151813] bias:  [-0.01086914]\n",
      "step:  2827 loss:  0.00024287 Weight:  [ 1.00151193] bias:  [-0.01082531]\n",
      "step:  2828 loss:  0.000240916 Weight:  [ 1.00150585] bias:  [-0.01078164]\n",
      "step:  2829 loss:  0.000238975 Weight:  [ 1.00149977] bias:  [-0.01073816]\n",
      "step:  2830 loss:  0.000237041 Weight:  [ 1.00149381] bias:  [-0.01069485]\n",
      "step:  2831 loss:  0.000235146 Weight:  [ 1.00148773] bias:  [-0.01065173]\n",
      "step:  2832 loss:  0.00023325 Weight:  [ 1.00148177] bias:  [-0.01060877]\n",
      "step:  2833 loss:  0.000231375 Weight:  [ 1.00147581] bias:  [-0.01056599]\n",
      "step:  2834 loss:  0.000229513 Weight:  [ 1.00146985] bias:  [-0.01052339]\n",
      "step:  2835 loss:  0.000227647 Weight:  [ 1.00146389] bias:  [-0.01048095]\n",
      "step:  2836 loss:  0.000225828 Weight:  [ 1.00145793] bias:  [-0.01043868]\n",
      "step:  2837 loss:  0.00022401 Weight:  [ 1.00145209] bias:  [-0.01039658]\n",
      "step:  2838 loss:  0.000222196 Weight:  [ 1.00144625] bias:  [-0.01035465]\n",
      "step:  2839 loss:  0.000220411 Weight:  [ 1.00144041] bias:  [-0.01031289]\n",
      "step:  2840 loss:  0.000218643 Weight:  [ 1.00143456] bias:  [-0.01027129]\n",
      "step:  2841 loss:  0.000216881 Weight:  [ 1.00142884] bias:  [-0.01022987]\n",
      "step:  2842 loss:  0.000215129 Weight:  [ 1.00142312] bias:  [-0.01018861]\n",
      "step:  2843 loss:  0.000213392 Weight:  [ 1.0014174] bias:  [-0.01014752]\n",
      "step:  2844 loss:  0.000211691 Weight:  [ 1.00141156] bias:  [-0.01010661]\n",
      "step:  2845 loss:  0.000209971 Weight:  [ 1.00140595] bias:  [-0.01006584]\n",
      "step:  2846 loss:  0.000208285 Weight:  [ 1.00140023] bias:  [-0.01002525]\n",
      "step:  2847 loss:  0.000206615 Weight:  [ 1.00139463] bias:  [-0.00998482]\n",
      "step:  2848 loss:  0.000204954 Weight:  [ 1.00138903] bias:  [-0.00994455]\n",
      "step:  2849 loss:  0.000203294 Weight:  [ 1.00138342] bias:  [-0.00990445]\n",
      "step:  2850 loss:  0.000201659 Weight:  [ 1.00137782] bias:  [-0.00986451]\n",
      "step:  2851 loss:  0.000200047 Weight:  [ 1.00137222] bias:  [-0.00982473]\n",
      "step:  2852 loss:  0.00019844 Weight:  [ 1.00136673] bias:  [-0.00978511]\n",
      "step:  2853 loss:  0.00019684 Weight:  [ 1.00136113] bias:  [-0.00974565]\n",
      "step:  2854 loss:  0.000195247 Weight:  [ 1.00135565] bias:  [-0.00970635]\n",
      "step:  2855 loss:  0.00019368 Weight:  [ 1.00135016] bias:  [-0.0096672]\n",
      "step:  2856 loss:  0.000192123 Weight:  [ 1.0013448] bias:  [-0.0096282]\n",
      "step:  2857 loss:  0.000190571 Weight:  [ 1.00133932] bias:  [-0.00958937]\n",
      "step:  2858 loss:  0.000189045 Weight:  [ 1.00133395] bias:  [-0.0095507]\n",
      "step:  2859 loss:  0.000187521 Weight:  [ 1.00132859] bias:  [-0.00951218]\n",
      "step:  2860 loss:  0.000186008 Weight:  [ 1.00132322] bias:  [-0.00947382]\n",
      "step:  2861 loss:  0.000184514 Weight:  [ 1.00131786] bias:  [-0.00943561]\n",
      "step:  2862 loss:  0.000183032 Weight:  [ 1.00131261] bias:  [-0.00939756]\n",
      "step:  2863 loss:  0.000181555 Weight:  [ 1.00130725] bias:  [-0.00935966]\n",
      "step:  2864 loss:  0.000180102 Weight:  [ 1.001302] bias:  [-0.00932192]\n",
      "step:  2865 loss:  0.000178635 Weight:  [ 1.00129676] bias:  [-0.00928432]\n",
      "step:  2866 loss:  0.000177214 Weight:  [ 1.00129151] bias:  [-0.00924688]\n",
      "step:  2867 loss:  0.000175773 Weight:  [ 1.00128627] bias:  [-0.00920959]\n",
      "step:  2868 loss:  0.000174369 Weight:  [ 1.00128114] bias:  [-0.00917244]\n",
      "step:  2869 loss:  0.000172962 Weight:  [ 1.00127602] bias:  [-0.00913545]\n",
      "step:  2870 loss:  0.000171571 Weight:  [ 1.00127077] bias:  [-0.00909862]\n",
      "step:  2871 loss:  0.000170177 Weight:  [ 1.00126565] bias:  [-0.00906192]\n",
      "step:  2872 loss:  0.000168812 Weight:  [ 1.00126052] bias:  [-0.00902537]\n",
      "step:  2873 loss:  0.000167459 Weight:  [ 1.00125551] bias:  [-0.00898896]\n",
      "step:  2874 loss:  0.0001661 Weight:  [ 1.00125051] bias:  [-0.00895271]\n",
      "step:  2875 loss:  0.000164762 Weight:  [ 1.0012455] bias:  [-0.00891661]\n",
      "step:  2876 loss:  0.000163443 Weight:  [ 1.00124037] bias:  [-0.00888066]\n",
      "step:  2877 loss:  0.000162128 Weight:  [ 1.00123537] bias:  [-0.00884484]\n",
      "step:  2878 loss:  0.000160835 Weight:  [ 1.00123036] bias:  [-0.00880918]\n",
      "step:  2879 loss:  0.000159527 Weight:  [ 1.00122535] bias:  [-0.00877365]\n",
      "step:  2880 loss:  0.000158248 Weight:  [ 1.00122046] bias:  [-0.00873826]\n",
      "step:  2881 loss:  0.000156974 Weight:  [ 1.00121558] bias:  [-0.00870301]\n",
      "step:  2882 loss:  0.000155704 Weight:  [ 1.00121069] bias:  [-0.00866792]\n",
      "step:  2883 loss:  0.000154459 Weight:  [ 1.0012058] bias:  [-0.00863296]\n",
      "step:  2884 loss:  0.000153219 Weight:  [ 1.00120091] bias:  [-0.00859815]\n",
      "step:  2885 loss:  0.000151973 Weight:  [ 1.00119615] bias:  [-0.00856347]\n",
      "step:  2886 loss:  0.000150759 Weight:  [ 1.00119126] bias:  [-0.00852895]\n",
      "step:  2887 loss:  0.000149542 Weight:  [ 1.00118637] bias:  [-0.00849455]\n",
      "step:  2888 loss:  0.000148341 Weight:  [ 1.0011816] bias:  [-0.00846029]\n",
      "step:  2889 loss:  0.000147149 Weight:  [ 1.00117683] bias:  [-0.00842616]\n",
      "step:  2890 loss:  0.000145967 Weight:  [ 1.00117207] bias:  [-0.00839218]\n",
      "step:  2891 loss:  0.000144782 Weight:  [ 1.00116742] bias:  [-0.00835833]\n",
      "step:  2892 loss:  0.000143619 Weight:  [ 1.00116265] bias:  [-0.00832462]\n",
      "step:  2893 loss:  0.00014246 Weight:  [ 1.001158] bias:  [-0.00829104]\n",
      "step:  2894 loss:  0.00014131 Weight:  [ 1.00115335] bias:  [-0.0082576]\n",
      "step:  2895 loss:  0.000140178 Weight:  [ 1.0011487] bias:  [-0.0082243]\n",
      "step:  2896 loss:  0.000139046 Weight:  [ 1.00114405] bias:  [-0.00819113]\n",
      "step:  2897 loss:  0.000137929 Weight:  [ 1.0011394] bias:  [-0.00815809]\n",
      "step:  2898 loss:  0.000136832 Weight:  [ 1.00113487] bias:  [-0.00812519]\n",
      "step:  2899 loss:  0.00013572 Weight:  [ 1.00113034] bias:  [-0.00809242]\n",
      "step:  2900 loss:  0.000134617 Weight:  [ 1.00112581] bias:  [-0.00805979]\n",
      "step:  2901 loss:  0.000133545 Weight:  [ 1.00112116] bias:  [-0.00802729]\n",
      "step:  2902 loss:  0.00013247 Weight:  [ 1.00111663] bias:  [-0.00799492]\n",
      "step:  2903 loss:  0.00013141 Weight:  [ 1.0011121] bias:  [-0.00796268]\n",
      "step:  2904 loss:  0.000130348 Weight:  [ 1.00110769] bias:  [-0.00793056]\n",
      "step:  2905 loss:  0.000129295 Weight:  [ 1.00110316] bias:  [-0.00789858]\n",
      "step:  2906 loss:  0.000128262 Weight:  [ 1.00109875] bias:  [-0.00786672]\n",
      "step:  2907 loss:  0.000127216 Weight:  [ 1.00109434] bias:  [-0.007835]\n",
      "step:  2908 loss:  0.000126194 Weight:  [ 1.00108993] bias:  [-0.0078034]\n",
      "step:  2909 loss:  0.000125192 Weight:  [ 1.00108552] bias:  [-0.00777193]\n",
      "step:  2910 loss:  0.000124177 Weight:  [ 1.00108111] bias:  [-0.00774059]\n",
      "step:  2911 loss:  0.000123168 Weight:  [ 1.00107682] bias:  [-0.00770937]\n",
      "step:  2912 loss:  0.000122178 Weight:  [ 1.00107241] bias:  [-0.00767829]\n",
      "step:  2913 loss:  0.000121201 Weight:  [ 1.00106812] bias:  [-0.00764732]\n",
      "step:  2914 loss:  0.000120222 Weight:  [ 1.00106382] bias:  [-0.00761647]\n",
      "step:  2915 loss:  0.000119265 Weight:  [ 1.00105953] bias:  [-0.00758576]\n",
      "step:  2916 loss:  0.000118299 Weight:  [ 1.00105524] bias:  [-0.00755518]\n",
      "step:  2917 loss:  0.000117345 Weight:  [ 1.00105095] bias:  [-0.00752471]\n",
      "step:  2918 loss:  0.000116403 Weight:  [ 1.00104678] bias:  [-0.00749436]\n",
      "step:  2919 loss:  0.000115467 Weight:  [ 1.00104249] bias:  [-0.00746414]\n",
      "step:  2920 loss:  0.000114528 Weight:  [ 1.00103831] bias:  [-0.00743403]\n",
      "step:  2921 loss:  0.000113618 Weight:  [ 1.00103414] bias:  [-0.00740405]\n",
      "step:  2922 loss:  0.000112696 Weight:  [ 1.00102997] bias:  [-0.00737419]\n",
      "step:  2923 loss:  0.000111796 Weight:  [ 1.0010258] bias:  [-0.00734446]\n",
      "step:  2924 loss:  0.000110892 Weight:  [ 1.00102162] bias:  [-0.00731484]\n",
      "step:  2925 loss:  0.000109997 Weight:  [ 1.00101757] bias:  [-0.00728534]\n",
      "step:  2926 loss:  0.000109115 Weight:  [ 1.0010134] bias:  [-0.00725596]\n",
      "step:  2927 loss:  0.000108238 Weight:  [ 1.00100935] bias:  [-0.00722669]\n",
      "step:  2928 loss:  0.000107362 Weight:  [ 1.00100529] bias:  [-0.00719755]\n",
      "step:  2929 loss:  0.000106498 Weight:  [ 1.00100124] bias:  [-0.00716852]\n",
      "step:  2930 loss:  0.000105642 Weight:  [ 1.00099719] bias:  [-0.00713961]\n",
      "step:  2931 loss:  0.000104795 Weight:  [ 1.00099313] bias:  [-0.00711082]\n",
      "step:  2932 loss:  0.000103952 Weight:  [ 1.0009892] bias:  [-0.00708214]\n",
      "step:  2933 loss:  0.00010311 Weight:  [ 1.00098515] bias:  [-0.00705358]\n",
      "step:  2934 loss:  0.000102283 Weight:  [ 1.00098121] bias:  [-0.00702513]\n",
      "step:  2935 loss:  0.000101467 Weight:  [ 1.00097728] bias:  [-0.0069968]\n",
      "step:  2936 loss:  0.000100641 Weight:  [ 1.00097334] bias:  [-0.00696859]\n",
      "step:  2937 loss:  9.98289e-05 Weight:  [ 1.00096941] bias:  [-0.00694049]\n",
      "step:  2938 loss:  9.90233e-05 Weight:  [ 1.00096548] bias:  [-0.0069125]\n",
      "step:  2939 loss:  9.82343e-05 Weight:  [ 1.00096154] bias:  [-0.00688462]\n",
      "step:  2940 loss:  9.74354e-05 Weight:  [ 1.00095773] bias:  [-0.00685685]\n",
      "step:  2941 loss:  9.66518e-05 Weight:  [ 1.00095391] bias:  [-0.0068292]\n",
      "step:  2942 loss:  9.58738e-05 Weight:  [ 1.00094998] bias:  [-0.00680166]\n",
      "step:  2943 loss:  9.51014e-05 Weight:  [ 1.00094616] bias:  [-0.00677423]\n",
      "step:  2944 loss:  9.43394e-05 Weight:  [ 1.00094235] bias:  [-0.00674691]\n",
      "step:  2945 loss:  9.35744e-05 Weight:  [ 1.00093853] bias:  [-0.0067197]\n",
      "step:  2946 loss:  9.28236e-05 Weight:  [ 1.00093472] bias:  [-0.0066926]\n",
      "step:  2947 loss:  9.20743e-05 Weight:  [ 1.00093102] bias:  [-0.00666561]\n",
      "step:  2948 loss:  9.13405e-05 Weight:  [ 1.00092721] bias:  [-0.00663873]\n",
      "step:  2949 loss:  9.06023e-05 Weight:  [ 1.00092351] bias:  [-0.00661195]\n",
      "step:  2950 loss:  8.98781e-05 Weight:  [ 1.00091982] bias:  [-0.00658529]\n",
      "step:  2951 loss:  8.91513e-05 Weight:  [ 1.00091612] bias:  [-0.00655874]\n",
      "step:  2952 loss:  8.84348e-05 Weight:  [ 1.00091231] bias:  [-0.0065323]\n",
      "step:  2953 loss:  8.7715e-05 Weight:  [ 1.00090873] bias:  [-0.00650594]\n",
      "step:  2954 loss:  8.70193e-05 Weight:  [ 1.00090504] bias:  [-0.00647971]\n",
      "step:  2955 loss:  8.632e-05 Weight:  [ 1.00090134] bias:  [-0.00645358]\n",
      "step:  2956 loss:  8.56173e-05 Weight:  [ 1.00089777] bias:  [-0.00642755]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step:  2957 loss:  8.49302e-05 Weight:  [ 1.00089419] bias:  [-0.00640163]\n",
      "step:  2958 loss:  8.42416e-05 Weight:  [ 1.00089049] bias:  [-0.00637582]\n",
      "step:  2959 loss:  8.35672e-05 Weight:  [ 1.00088692] bias:  [-0.0063501]\n",
      "step:  2960 loss:  8.28921e-05 Weight:  [ 1.00088334] bias:  [-0.00632449]\n",
      "step:  2961 loss:  8.22236e-05 Weight:  [ 1.00087976] bias:  [-0.00629898]\n",
      "step:  2962 loss:  8.1566e-05 Weight:  [ 1.00087619] bias:  [-0.00627357]\n",
      "step:  2963 loss:  8.09166e-05 Weight:  [ 1.00087261] bias:  [-0.00624827]\n",
      "step:  2964 loss:  8.0257e-05 Weight:  [ 1.00086915] bias:  [-0.00622306]\n",
      "step:  2965 loss:  7.9617e-05 Weight:  [ 1.0008657] bias:  [-0.00619796]\n",
      "step:  2966 loss:  7.89751e-05 Weight:  [ 1.00086212] bias:  [-0.00617297]\n",
      "step:  2967 loss:  7.83344e-05 Weight:  [ 1.00085866] bias:  [-0.00614807]\n",
      "step:  2968 loss:  7.77043e-05 Weight:  [ 1.00085521] bias:  [-0.00612327]\n",
      "step:  2969 loss:  7.70806e-05 Weight:  [ 1.00085175] bias:  [-0.00609857]\n",
      "step:  2970 loss:  7.64539e-05 Weight:  [ 1.00084841] bias:  [-0.00607397]\n",
      "step:  2971 loss:  7.58423e-05 Weight:  [ 1.00084496] bias:  [-0.00604948]\n",
      "step:  2972 loss:  7.5236e-05 Weight:  [ 1.0008415] bias:  [-0.00602509]\n",
      "step:  2973 loss:  7.46274e-05 Weight:  [ 1.00083816] bias:  [-0.00600078]\n",
      "step:  2974 loss:  7.40323e-05 Weight:  [ 1.0008347] bias:  [-0.00597658]\n",
      "step:  2975 loss:  7.34282e-05 Weight:  [ 1.00083137] bias:  [-0.00595247]\n",
      "step:  2976 loss:  7.28382e-05 Weight:  [ 1.00082803] bias:  [-0.00592846]\n",
      "step:  2977 loss:  7.22557e-05 Weight:  [ 1.00082469] bias:  [-0.00590456]\n",
      "step:  2978 loss:  7.16801e-05 Weight:  [ 1.00082135] bias:  [-0.00588075]\n",
      "step:  2979 loss:  7.10969e-05 Weight:  [ 1.00081801] bias:  [-0.00585703]\n",
      "step:  2980 loss:  7.05212e-05 Weight:  [ 1.0008148] bias:  [-0.0058334]\n",
      "step:  2981 loss:  6.99546e-05 Weight:  [ 1.00081146] bias:  [-0.00580988]\n",
      "step:  2982 loss:  6.93947e-05 Weight:  [ 1.00080824] bias:  [-0.00578644]\n",
      "step:  2983 loss:  6.88301e-05 Weight:  [ 1.00080502] bias:  [-0.00576311]\n",
      "step:  2984 loss:  6.82762e-05 Weight:  [ 1.00080168] bias:  [-0.00573988]\n",
      "step:  2985 loss:  6.77266e-05 Weight:  [ 1.00079846] bias:  [-0.00571673]\n",
      "step:  2986 loss:  6.71862e-05 Weight:  [ 1.00079525] bias:  [-0.00569368]\n",
      "step:  2987 loss:  6.66493e-05 Weight:  [ 1.00079203] bias:  [-0.00567071]\n",
      "step:  2988 loss:  6.61078e-05 Weight:  [ 1.00078881] bias:  [-0.00564784]\n",
      "step:  2989 loss:  6.55749e-05 Weight:  [ 1.00078559] bias:  [-0.00562507]\n",
      "step:  2990 loss:  6.50521e-05 Weight:  [ 1.00078249] bias:  [-0.00560238]\n",
      "step:  2991 loss:  6.4518e-05 Weight:  [ 1.00077939] bias:  [-0.00557978]\n",
      "step:  2992 loss:  6.40001e-05 Weight:  [ 1.00077617] bias:  [-0.00555728]\n",
      "step:  2993 loss:  6.34938e-05 Weight:  [ 1.00077307] bias:  [-0.00553487]\n",
      "step:  2994 loss:  6.29749e-05 Weight:  [ 1.00076997] bias:  [-0.00551255]\n",
      "step:  2995 loss:  6.24751e-05 Weight:  [ 1.00076687] bias:  [-0.00549032]\n",
      "step:  2996 loss:  6.19656e-05 Weight:  [ 1.00076377] bias:  [-0.00546818]\n",
      "step:  2997 loss:  6.14703e-05 Weight:  [ 1.00076067] bias:  [-0.00544613]\n",
      "step:  2998 loss:  6.09712e-05 Weight:  [ 1.00075758] bias:  [-0.00542417]\n",
      "step:  2999 loss:  6.04829e-05 Weight:  [ 1.00075459] bias:  [-0.00540229]\n",
      "step:  3000 loss:  5.99925e-05 Weight:  [ 1.0007515] bias:  [-0.0053805]\n",
      "step:  3001 loss:  5.95124e-05 Weight:  [ 1.00074852] bias:  [-0.0053588]\n",
      "step:  3002 loss:  5.90376e-05 Weight:  [ 1.00074542] bias:  [-0.0053372]\n",
      "step:  3003 loss:  5.85631e-05 Weight:  [ 1.00074244] bias:  [-0.00531567]\n",
      "step:  3004 loss:  5.80878e-05 Weight:  [ 1.00073946] bias:  [-0.00529423]\n",
      "step:  3005 loss:  5.76262e-05 Weight:  [ 1.00073647] bias:  [-0.00527288]\n",
      "step:  3006 loss:  5.71543e-05 Weight:  [ 1.00073349] bias:  [-0.00525162]\n",
      "step:  3007 loss:  5.66924e-05 Weight:  [ 1.00073051] bias:  [-0.00523044]\n",
      "step:  3008 loss:  5.62367e-05 Weight:  [ 1.00072765] bias:  [-0.00520934]\n",
      "step:  3009 loss:  5.57923e-05 Weight:  [ 1.00072467] bias:  [-0.00518833]\n",
      "step:  3010 loss:  5.53436e-05 Weight:  [ 1.00072169] bias:  [-0.00516741]\n",
      "step:  3011 loss:  5.49e-05 Weight:  [ 1.00071883] bias:  [-0.00514657]\n",
      "step:  3012 loss:  5.44506e-05 Weight:  [ 1.00071597] bias:  [-0.00512582]\n",
      "step:  3013 loss:  5.40141e-05 Weight:  [ 1.00071311] bias:  [-0.00510515]\n",
      "step:  3014 loss:  5.35833e-05 Weight:  [ 1.00071013] bias:  [-0.00508457]\n",
      "step:  3015 loss:  5.31421e-05 Weight:  [ 1.00070727] bias:  [-0.00506406]\n",
      "step:  3016 loss:  5.2718e-05 Weight:  [ 1.00070441] bias:  [-0.00504363]\n",
      "step:  3017 loss:  5.22921e-05 Weight:  [ 1.00070167] bias:  [-0.00502329]\n",
      "step:  3018 loss:  5.18788e-05 Weight:  [ 1.0006988] bias:  [-0.00500304]\n",
      "step:  3019 loss:  5.14596e-05 Weight:  [ 1.00069594] bias:  [-0.00498287]\n",
      "step:  3020 loss:  5.10455e-05 Weight:  [ 1.0006932] bias:  [-0.00496277]\n",
      "step:  3021 loss:  5.06326e-05 Weight:  [ 1.00069034] bias:  [-0.00494276]\n",
      "step:  3022 loss:  5.02251e-05 Weight:  [ 1.0006876] bias:  [-0.00492282]\n",
      "step:  3023 loss:  4.98205e-05 Weight:  [ 1.00068486] bias:  [-0.00490297]\n",
      "step:  3024 loss:  4.94185e-05 Weight:  [ 1.000682] bias:  [-0.0048832]\n",
      "step:  3025 loss:  4.90229e-05 Weight:  [ 1.00067925] bias:  [-0.00486351]\n",
      "step:  3026 loss:  4.86226e-05 Weight:  [ 1.00067651] bias:  [-0.00484389]\n",
      "step:  3027 loss:  4.82374e-05 Weight:  [ 1.00067377] bias:  [-0.00482435]\n",
      "step:  3028 loss:  4.78438e-05 Weight:  [ 1.00067115] bias:  [-0.00480488]\n",
      "step:  3029 loss:  4.74661e-05 Weight:  [ 1.00066841] bias:  [-0.00478551]\n",
      "step:  3030 loss:  4.70824e-05 Weight:  [ 1.00066566] bias:  [-0.00476621]\n",
      "step:  3031 loss:  4.66986e-05 Weight:  [ 1.00066304] bias:  [-0.00474699]\n",
      "step:  3032 loss:  4.63187e-05 Weight:  [ 1.00066042] bias:  [-0.00472784]\n",
      "step:  3033 loss:  4.59472e-05 Weight:  [ 1.00065768] bias:  [-0.00470878]\n",
      "step:  3034 loss:  4.55771e-05 Weight:  [ 1.00065506] bias:  [-0.00468979]\n",
      "step:  3035 loss:  4.52138e-05 Weight:  [ 1.00065243] bias:  [-0.00467088]\n",
      "step:  3036 loss:  4.48473e-05 Weight:  [ 1.00064981] bias:  [-0.00465204]\n",
      "step:  3037 loss:  4.44909e-05 Weight:  [ 1.00064719] bias:  [-0.00463329]\n",
      "step:  3038 loss:  4.41332e-05 Weight:  [ 1.00064456] bias:  [-0.00461461]\n",
      "step:  3039 loss:  4.37776e-05 Weight:  [ 1.00064194] bias:  [-0.004596]\n",
      "step:  3040 loss:  4.34245e-05 Weight:  [ 1.00063932] bias:  [-0.00457747]\n",
      "step:  3041 loss:  4.30748e-05 Weight:  [ 1.00063682] bias:  [-0.004559]\n",
      "step:  3042 loss:  4.27343e-05 Weight:  [ 1.00063419] bias:  [-0.00454062]\n",
      "step:  3043 loss:  4.23863e-05 Weight:  [ 1.00063157] bias:  [-0.00452231]\n",
      "step:  3044 loss:  4.2041e-05 Weight:  [ 1.00062907] bias:  [-0.00450407]\n",
      "step:  3045 loss:  4.17023e-05 Weight:  [ 1.00062656] bias:  [-0.0044859]\n",
      "step:  3046 loss:  4.13672e-05 Weight:  [ 1.00062406] bias:  [-0.00446781]\n",
      "step:  3047 loss:  4.10356e-05 Weight:  [ 1.00062156] bias:  [-0.00444979]\n",
      "step:  3048 loss:  4.07086e-05 Weight:  [ 1.00061905] bias:  [-0.00443185]\n",
      "step:  3049 loss:  4.03777e-05 Weight:  [ 1.00061655] bias:  [-0.00441399]\n",
      "step:  3050 loss:  4.00556e-05 Weight:  [ 1.00061405] bias:  [-0.00439619]\n",
      "step:  3051 loss:  3.97312e-05 Weight:  [ 1.00061154] bias:  [-0.00437846]\n",
      "step:  3052 loss:  3.94101e-05 Weight:  [ 1.00060904] bias:  [-0.0043608]\n",
      "step:  3053 loss:  3.90911e-05 Weight:  [ 1.00060666] bias:  [-0.00434321]\n",
      "step:  3054 loss:  3.8779e-05 Weight:  [ 1.00060415] bias:  [-0.0043257]\n",
      "step:  3055 loss:  3.84668e-05 Weight:  [ 1.00060177] bias:  [-0.00430825]\n",
      "step:  3056 loss:  3.81542e-05 Weight:  [ 1.00059938] bias:  [-0.00429088]\n",
      "step:  3057 loss:  3.78514e-05 Weight:  [ 1.00059688] bias:  [-0.00427358]\n",
      "step:  3058 loss:  3.75489e-05 Weight:  [ 1.0005945] bias:  [-0.00425635]\n",
      "step:  3059 loss:  3.72443e-05 Weight:  [ 1.00059211] bias:  [-0.00423918]\n",
      "step:  3060 loss:  3.69456e-05 Weight:  [ 1.00058973] bias:  [-0.00422209]\n",
      "step:  3061 loss:  3.66485e-05 Weight:  [ 1.00058734] bias:  [-0.00420507]\n",
      "step:  3062 loss:  3.63552e-05 Weight:  [ 1.00058496] bias:  [-0.00418811]\n",
      "step:  3063 loss:  3.60563e-05 Weight:  [ 1.00058258] bias:  [-0.00417122]\n",
      "step:  3064 loss:  3.5769e-05 Weight:  [ 1.00058031] bias:  [-0.00415439]\n",
      "step:  3065 loss:  3.54813e-05 Weight:  [ 1.00057793] bias:  [-0.00413764]\n",
      "step:  3066 loss:  3.51935e-05 Weight:  [ 1.00057554] bias:  [-0.00412096]\n",
      "step:  3067 loss:  3.49141e-05 Weight:  [ 1.00057328] bias:  [-0.00410434]\n",
      "step:  3068 loss:  3.46272e-05 Weight:  [ 1.00057101] bias:  [-0.00408778]\n",
      "step:  3069 loss:  3.4356e-05 Weight:  [ 1.00056863] bias:  [-0.00407131]\n",
      "step:  3070 loss:  3.40734e-05 Weight:  [ 1.00056636] bias:  [-0.00405489]\n",
      "step:  3071 loss:  3.3802e-05 Weight:  [ 1.0005641] bias:  [-0.00403853]\n",
      "step:  3072 loss:  3.35286e-05 Weight:  [ 1.00056183] bias:  [-0.00402225]\n",
      "step:  3073 loss:  3.3256e-05 Weight:  [ 1.00055957] bias:  [-0.00400603]\n",
      "step:  3074 loss:  3.29924e-05 Weight:  [ 1.0005573] bias:  [-0.00398988]\n",
      "step:  3075 loss:  3.27258e-05 Weight:  [ 1.00055504] bias:  [-0.00397379]\n",
      "step:  3076 loss:  3.24617e-05 Weight:  [ 1.00055277] bias:  [-0.00395776]\n",
      "step:  3077 loss:  3.22003e-05 Weight:  [ 1.00055051] bias:  [-0.0039418]\n",
      "step:  3078 loss:  3.19415e-05 Weight:  [ 1.00054836] bias:  [-0.0039259]\n",
      "step:  3079 loss:  3.16821e-05 Weight:  [ 1.0005461] bias:  [-0.00391007]\n",
      "step:  3080 loss:  3.1429e-05 Weight:  [ 1.00054395] bias:  [-0.0038943]\n",
      "step:  3081 loss:  3.11813e-05 Weight:  [ 1.00054169] bias:  [-0.0038786]\n",
      "step:  3082 loss:  3.09259e-05 Weight:  [ 1.00053954] bias:  [-0.00386295]\n",
      "step:  3083 loss:  3.06762e-05 Weight:  [ 1.0005374] bias:  [-0.00384737]\n",
      "step:  3084 loss:  3.04327e-05 Weight:  [ 1.00053525] bias:  [-0.00383186]\n",
      "step:  3085 loss:  3.01835e-05 Weight:  [ 1.0005331] bias:  [-0.00381641]\n",
      "step:  3086 loss:  2.99411e-05 Weight:  [ 1.00053096] bias:  [-0.00380102]\n",
      "step:  3087 loss:  2.9702e-05 Weight:  [ 1.00052881] bias:  [-0.0037857]\n",
      "step:  3088 loss:  2.94589e-05 Weight:  [ 1.00052667] bias:  [-0.00377044]\n",
      "step:  3089 loss:  2.92204e-05 Weight:  [ 1.00052452] bias:  [-0.00375523]\n",
      "step:  3090 loss:  2.89885e-05 Weight:  [ 1.00052238] bias:  [-0.00374009]\n",
      "step:  3091 loss:  2.87573e-05 Weight:  [ 1.00052023] bias:  [-0.003725]\n",
      "step:  3092 loss:  2.85264e-05 Weight:  [ 1.0005182] bias:  [-0.00370998]\n",
      "step:  3093 loss:  2.82979e-05 Weight:  [ 1.00051606] bias:  [-0.00369502]\n",
      "step:  3094 loss:  2.80632e-05 Weight:  [ 1.00051403] bias:  [-0.00368011]\n",
      "step:  3095 loss:  2.78422e-05 Weight:  [ 1.000512] bias:  [-0.00366527]\n",
      "step:  3096 loss:  2.76214e-05 Weight:  [ 1.00050986] bias:  [-0.0036505]\n",
      "step:  3097 loss:  2.73936e-05 Weight:  [ 1.00050783] bias:  [-0.00363577]\n",
      "step:  3098 loss:  2.71771e-05 Weight:  [ 1.00050581] bias:  [-0.00362111]\n",
      "step:  3099 loss:  2.6957e-05 Weight:  [ 1.00050378] bias:  [-0.00360651]\n",
      "step:  3100 loss:  2.67362e-05 Weight:  [ 1.00050175] bias:  [-0.00359197]\n",
      "step:  3101 loss:  2.65255e-05 Weight:  [ 1.00049973] bias:  [-0.00357749]\n",
      "step:  3102 loss:  2.63122e-05 Weight:  [ 1.0004977] bias:  [-0.00356307]\n",
      "step:  3103 loss:  2.61018e-05 Weight:  [ 1.00049567] bias:  [-0.00354871]\n",
      "step:  3104 loss:  2.58907e-05 Weight:  [ 1.00049365] bias:  [-0.0035344]\n",
      "step:  3105 loss:  2.56797e-05 Weight:  [ 1.00049162] bias:  [-0.00352015]\n",
      "step:  3106 loss:  2.54759e-05 Weight:  [ 1.00048959] bias:  [-0.00350595]\n",
      "step:  3107 loss:  2.52711e-05 Weight:  [ 1.00048769] bias:  [-0.0034918]\n",
      "step:  3108 loss:  2.50689e-05 Weight:  [ 1.00048578] bias:  [-0.00347772]\n",
      "step:  3109 loss:  2.48629e-05 Weight:  [ 1.00048387] bias:  [-0.00346369]\n",
      "step:  3110 loss:  2.46648e-05 Weight:  [ 1.00048184] bias:  [-0.00344973]\n",
      "step:  3111 loss:  2.44645e-05 Weight:  [ 1.00047994] bias:  [-0.00343582]\n",
      "step:  3112 loss:  2.42653e-05 Weight:  [ 1.00047803] bias:  [-0.00342197]\n",
      "step:  3113 loss:  2.40726e-05 Weight:  [ 1.000476] bias:  [-0.00340818]\n",
      "step:  3114 loss:  2.38762e-05 Weight:  [ 1.0004741] bias:  [-0.00339443]\n",
      "step:  3115 loss:  2.36869e-05 Weight:  [ 1.00047219] bias:  [-0.00338074]\n",
      "step:  3116 loss:  2.34948e-05 Weight:  [ 1.00047028] bias:  [-0.00336711]\n",
      "step:  3117 loss:  2.33042e-05 Weight:  [ 1.00046837] bias:  [-0.00335352]\n",
      "step:  3118 loss:  2.31204e-05 Weight:  [ 1.00046647] bias:  [-0.00334]\n",
      "step:  3119 loss:  2.29328e-05 Weight:  [ 1.00046456] bias:  [-0.00332653]\n",
      "step:  3120 loss:  2.27468e-05 Weight:  [ 1.00046277] bias:  [-0.00331311]\n",
      "step:  3121 loss:  2.25637e-05 Weight:  [ 1.00046086] bias:  [-0.00329975]\n",
      "step:  3122 loss:  2.23821e-05 Weight:  [ 1.00045907] bias:  [-0.00328644]\n",
      "step:  3123 loss:  2.22045e-05 Weight:  [ 1.00045717] bias:  [-0.00327319]\n",
      "step:  3124 loss:  2.20272e-05 Weight:  [ 1.00045526] bias:  [-0.00325999]\n",
      "step:  3125 loss:  2.18499e-05 Weight:  [ 1.00045347] bias:  [-0.00324684]\n",
      "step:  3126 loss:  2.16699e-05 Weight:  [ 1.00045168] bias:  [-0.00323374]\n",
      "step:  3127 loss:  2.14958e-05 Weight:  [ 1.0004499] bias:  [-0.0032207]\n",
      "step:  3128 loss:  2.13271e-05 Weight:  [ 1.00044799] bias:  [-0.00320772]\n",
      "step:  3129 loss:  2.11519e-05 Weight:  [ 1.0004462] bias:  [-0.00319478]\n",
      "step:  3130 loss:  2.09844e-05 Weight:  [ 1.00044441] bias:  [-0.00318189]\n",
      "step:  3131 loss:  2.08161e-05 Weight:  [ 1.00044262] bias:  [-0.00316906]\n",
      "step:  3132 loss:  2.06445e-05 Weight:  [ 1.00044084] bias:  [-0.00315628]\n",
      "step:  3133 loss:  2.04759e-05 Weight:  [ 1.00043905] bias:  [-0.00314355]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step:  3134 loss:  2.03171e-05 Weight:  [ 1.00043726] bias:  [-0.00313087]\n",
      "step:  3135 loss:  2.01508e-05 Weight:  [ 1.00043559] bias:  [-0.00311824]\n",
      "step:  3136 loss:  1.99896e-05 Weight:  [ 1.0004338] bias:  [-0.00310567]\n",
      "step:  3137 loss:  1.98272e-05 Weight:  [ 1.00043201] bias:  [-0.00309315]\n",
      "step:  3138 loss:  1.9671e-05 Weight:  [ 1.00043023] bias:  [-0.00308067]\n",
      "step:  3139 loss:  1.95103e-05 Weight:  [ 1.00042856] bias:  [-0.00306824]\n",
      "step:  3140 loss:  1.93553e-05 Weight:  [ 1.00042677] bias:  [-0.00305587]\n",
      "step:  3141 loss:  1.91997e-05 Weight:  [ 1.0004251] bias:  [-0.00304354]\n",
      "step:  3142 loss:  1.9047e-05 Weight:  [ 1.00042331] bias:  [-0.00303127]\n",
      "step:  3143 loss:  1.88886e-05 Weight:  [ 1.00042164] bias:  [-0.00301904]\n",
      "step:  3144 loss:  1.87351e-05 Weight:  [ 1.00041997] bias:  [-0.00300686]\n",
      "step:  3145 loss:  1.85875e-05 Weight:  [ 1.00041831] bias:  [-0.00299473]\n",
      "step:  3146 loss:  1.84375e-05 Weight:  [ 1.00041664] bias:  [-0.00298266]\n",
      "step:  3147 loss:  1.82878e-05 Weight:  [ 1.00041497] bias:  [-0.00297063]\n",
      "step:  3148 loss:  1.81402e-05 Weight:  [ 1.0004133] bias:  [-0.00295866]\n",
      "step:  3149 loss:  1.79988e-05 Weight:  [ 1.00041163] bias:  [-0.00294673]\n",
      "step:  3150 loss:  1.78491e-05 Weight:  [ 1.00040996] bias:  [-0.00293485]\n",
      "step:  3151 loss:  1.77109e-05 Weight:  [ 1.00040829] bias:  [-0.00292302]\n",
      "step:  3152 loss:  1.75632e-05 Weight:  [ 1.00040662] bias:  [-0.00291124]\n",
      "step:  3153 loss:  1.74226e-05 Weight:  [ 1.00040495] bias:  [-0.0028995]\n",
      "step:  3154 loss:  1.72835e-05 Weight:  [ 1.00040329] bias:  [-0.0028878]\n",
      "step:  3155 loss:  1.71451e-05 Weight:  [ 1.00040174] bias:  [-0.00287615]\n",
      "step:  3156 loss:  1.70035e-05 Weight:  [ 1.00040007] bias:  [-0.00286455]\n",
      "step:  3157 loss:  1.68686e-05 Weight:  [ 1.00039852] bias:  [-0.002853]\n",
      "step:  3158 loss:  1.67323e-05 Weight:  [ 1.00039685] bias:  [-0.00284149]\n",
      "step:  3159 loss:  1.65948e-05 Weight:  [ 1.0003953] bias:  [-0.00283003]\n",
      "step:  3160 loss:  1.64642e-05 Weight:  [ 1.00039375] bias:  [-0.00281862]\n",
      "step:  3161 loss:  1.63316e-05 Weight:  [ 1.00039208] bias:  [-0.00280725]\n",
      "step:  3162 loss:  1.61973e-05 Weight:  [ 1.00039053] bias:  [-0.00279593]\n",
      "step:  3163 loss:  1.607e-05 Weight:  [ 1.00038898] bias:  [-0.00278466]\n",
      "step:  3164 loss:  1.59426e-05 Weight:  [ 1.00038743] bias:  [-0.00277343]\n",
      "step:  3165 loss:  1.58123e-05 Weight:  [ 1.00038588] bias:  [-0.00276225]\n",
      "step:  3166 loss:  1.56871e-05 Weight:  [ 1.00038421] bias:  [-0.00275112]\n",
      "step:  3167 loss:  1.55598e-05 Weight:  [ 1.00038266] bias:  [-0.00274002]\n",
      "step:  3168 loss:  1.54325e-05 Weight:  [ 1.00038111] bias:  [-0.00272897]\n",
      "step:  3169 loss:  1.53069e-05 Weight:  [ 1.00037968] bias:  [-0.00271795]\n",
      "step:  3170 loss:  1.51856e-05 Weight:  [ 1.00037813] bias:  [-0.002707]\n",
      "step:  3171 loss:  1.50659e-05 Weight:  [ 1.00037658] bias:  [-0.00269609]\n",
      "step:  3172 loss:  1.49442e-05 Weight:  [ 1.00037503] bias:  [-0.00268522]\n",
      "step:  3173 loss:  1.48203e-05 Weight:  [ 1.0003736] bias:  [-0.00267438]\n",
      "step:  3174 loss:  1.47061e-05 Weight:  [ 1.00037205] bias:  [-0.00266361]\n",
      "step:  3175 loss:  1.45842e-05 Weight:  [ 1.0003705] bias:  [-0.00265287]\n",
      "step:  3176 loss:  1.44662e-05 Weight:  [ 1.00036907] bias:  [-0.00264217]\n",
      "step:  3177 loss:  1.43494e-05 Weight:  [ 1.00036752] bias:  [-0.00263151]\n",
      "step:  3178 loss:  1.42375e-05 Weight:  [ 1.00036609] bias:  [-0.0026209]\n",
      "step:  3179 loss:  1.41235e-05 Weight:  [ 1.00036454] bias:  [-0.00261033]\n",
      "step:  3180 loss:  1.40072e-05 Weight:  [ 1.00036311] bias:  [-0.0025998]\n",
      "step:  3181 loss:  1.38961e-05 Weight:  [ 1.00036168] bias:  [-0.00258932]\n",
      "step:  3182 loss:  1.37818e-05 Weight:  [ 1.00036025] bias:  [-0.00257888]\n",
      "step:  3183 loss:  1.36743e-05 Weight:  [ 1.0003587] bias:  [-0.00256848]\n",
      "step:  3184 loss:  1.35621e-05 Weight:  [ 1.00035727] bias:  [-0.00255812]\n",
      "step:  3185 loss:  1.34544e-05 Weight:  [ 1.00035584] bias:  [-0.0025478]\n",
      "step:  3186 loss:  1.33416e-05 Weight:  [ 1.00035441] bias:  [-0.00253753]\n",
      "step:  3187 loss:  1.32358e-05 Weight:  [ 1.00035298] bias:  [-0.00252729]\n",
      "step:  3188 loss:  1.31292e-05 Weight:  [ 1.00035155] bias:  [-0.0025171]\n",
      "step:  3189 loss:  1.30253e-05 Weight:  [ 1.00035012] bias:  [-0.00250695]\n",
      "step:  3190 loss:  1.2918e-05 Weight:  [ 1.00034869] bias:  [-0.00249684]\n",
      "step:  3191 loss:  1.28152e-05 Weight:  [ 1.00034726] bias:  [-0.00248676]\n",
      "step:  3192 loss:  1.27095e-05 Weight:  [ 1.00034595] bias:  [-0.00247673]\n",
      "step:  3193 loss:  1.26117e-05 Weight:  [ 1.00034451] bias:  [-0.00246674]\n",
      "step:  3194 loss:  1.2506e-05 Weight:  [ 1.0003432] bias:  [-0.00245679]\n",
      "step:  3195 loss:  1.24104e-05 Weight:  [ 1.00034177] bias:  [-0.00244689]\n",
      "step:  3196 loss:  1.23079e-05 Weight:  [ 1.00034034] bias:  [-0.00243702]\n",
      "step:  3197 loss:  1.22123e-05 Weight:  [ 1.00033903] bias:  [-0.00242719]\n",
      "step:  3198 loss:  1.21115e-05 Weight:  [ 1.0003376] bias:  [-0.0024174]\n",
      "step:  3199 loss:  1.20128e-05 Weight:  [ 1.00033629] bias:  [-0.00240765]\n",
      "step:  3200 loss:  1.19145e-05 Weight:  [ 1.00033498] bias:  [-0.00239794]\n",
      "step:  3201 loss:  1.18216e-05 Weight:  [ 1.00033355] bias:  [-0.00238828]\n",
      "step:  3202 loss:  1.1728e-05 Weight:  [ 1.00033224] bias:  [-0.00237864]\n",
      "step:  3203 loss:  1.1633e-05 Weight:  [ 1.00033092] bias:  [-0.00236905]\n",
      "step:  3204 loss:  1.15388e-05 Weight:  [ 1.00032961] bias:  [-0.0023595]\n",
      "step:  3205 loss:  1.14444e-05 Weight:  [ 1.00032818] bias:  [-0.00235]\n",
      "step:  3206 loss:  1.13525e-05 Weight:  [ 1.00032687] bias:  [-0.00234052]\n",
      "step:  3207 loss:  1.12635e-05 Weight:  [ 1.00032556] bias:  [-0.00233108]\n",
      "step:  3208 loss:  1.11722e-05 Weight:  [ 1.00032425] bias:  [-0.00232167]\n",
      "step:  3209 loss:  1.10829e-05 Weight:  [ 1.00032294] bias:  [-0.00231231]\n",
      "step:  3210 loss:  1.09903e-05 Weight:  [ 1.00032163] bias:  [-0.00230298]\n",
      "step:  3211 loss:  1.09014e-05 Weight:  [ 1.00032032] bias:  [-0.00229369]\n",
      "step:  3212 loss:  1.08164e-05 Weight:  [ 1.000319] bias:  [-0.00228443]\n",
      "step:  3213 loss:  1.07294e-05 Weight:  [ 1.00031781] bias:  [-0.00227522]\n",
      "step:  3214 loss:  1.06399e-05 Weight:  [ 1.0003165] bias:  [-0.00226604]\n",
      "step:  3215 loss:  1.05572e-05 Weight:  [ 1.00031519] bias:  [-0.0022569]\n",
      "step:  3216 loss:  1.0469e-05 Weight:  [ 1.000314] bias:  [-0.0022478]\n",
      "step:  3217 loss:  1.0386e-05 Weight:  [ 1.00031269] bias:  [-0.00223873]\n",
      "step:  3218 loss:  1.03063e-05 Weight:  [ 1.00031137] bias:  [-0.00222971]\n",
      "step:  3219 loss:  1.02197e-05 Weight:  [ 1.00031018] bias:  [-0.00222071]\n",
      "step:  3220 loss:  1.01353e-05 Weight:  [ 1.00030899] bias:  [-0.00221175]\n",
      "step:  3221 loss:  1.00575e-05 Weight:  [ 1.00030768] bias:  [-0.00220284]\n",
      "step:  3222 loss:  9.97458e-06 Weight:  [ 1.00030649] bias:  [-0.00219396]\n",
      "step:  3223 loss:  9.89429e-06 Weight:  [ 1.00030529] bias:  [-0.00218511]\n",
      "step:  3224 loss:  9.81578e-06 Weight:  [ 1.00030398] bias:  [-0.00217631]\n",
      "step:  3225 loss:  9.73774e-06 Weight:  [ 1.00030279] bias:  [-0.00216754]\n",
      "step:  3226 loss:  9.65842e-06 Weight:  [ 1.00030148] bias:  [-0.0021588]\n",
      "step:  3227 loss:  9.57883e-06 Weight:  [ 1.00030029] bias:  [-0.00215009]\n",
      "step:  3228 loss:  9.50276e-06 Weight:  [ 1.0002991] bias:  [-0.00214141]\n",
      "step:  3229 loss:  9.42812e-06 Weight:  [ 1.0002979] bias:  [-0.00213278]\n",
      "step:  3230 loss:  9.35345e-06 Weight:  [ 1.00029671] bias:  [-0.00212418]\n",
      "step:  3231 loss:  9.27581e-06 Weight:  [ 1.00029552] bias:  [-0.00211562]\n",
      "step:  3232 loss:  9.20404e-06 Weight:  [ 1.00029433] bias:  [-0.00210709]\n",
      "step:  3233 loss:  9.12864e-06 Weight:  [ 1.00029314] bias:  [-0.0020986]\n",
      "step:  3234 loss:  9.05273e-06 Weight:  [ 1.00029194] bias:  [-0.00209013]\n",
      "step:  3235 loss:  8.97918e-06 Weight:  [ 1.00029075] bias:  [-0.0020817]\n",
      "step:  3236 loss:  8.90906e-06 Weight:  [ 1.00028956] bias:  [-0.00207331]\n",
      "step:  3237 loss:  8.83631e-06 Weight:  [ 1.00028837] bias:  [-0.00206495]\n",
      "step:  3238 loss:  8.76717e-06 Weight:  [ 1.00028729] bias:  [-0.00205661]\n",
      "step:  3239 loss:  8.6926e-06 Weight:  [ 1.0002861] bias:  [-0.00204832]\n",
      "step:  3240 loss:  8.6247e-06 Weight:  [ 1.00028491] bias:  [-0.00204006]\n",
      "step:  3241 loss:  8.55736e-06 Weight:  [ 1.00028372] bias:  [-0.00203183]\n",
      "step:  3242 loss:  8.48563e-06 Weight:  [ 1.00028265] bias:  [-0.00202363]\n",
      "step:  3243 loss:  8.41902e-06 Weight:  [ 1.00028157] bias:  [-0.00201546]\n",
      "step:  3244 loss:  8.35058e-06 Weight:  [ 1.00028038] bias:  [-0.00200734]\n",
      "step:  3245 loss:  8.28302e-06 Weight:  [ 1.00027931] bias:  [-0.00199925]\n",
      "step:  3246 loss:  8.21874e-06 Weight:  [ 1.00027812] bias:  [-0.0019912]\n",
      "step:  3247 loss:  8.15048e-06 Weight:  [ 1.00027704] bias:  [-0.00198317]\n",
      "step:  3248 loss:  8.08487e-06 Weight:  [ 1.00027585] bias:  [-0.00197517]\n",
      "step:  3249 loss:  8.01902e-06 Weight:  [ 1.00027478] bias:  [-0.0019672]\n",
      "step:  3250 loss:  7.95407e-06 Weight:  [ 1.0002737] bias:  [-0.00195927]\n",
      "step:  3251 loss:  7.89331e-06 Weight:  [ 1.00027251] bias:  [-0.00195138]\n",
      "step:  3252 loss:  7.82972e-06 Weight:  [ 1.00027144] bias:  [-0.0019435]\n",
      "step:  3253 loss:  7.76477e-06 Weight:  [ 1.00027037] bias:  [-0.00193566]\n",
      "step:  3254 loss:  7.70242e-06 Weight:  [ 1.00026929] bias:  [-0.00192786]\n",
      "step:  3255 loss:  7.63958e-06 Weight:  [ 1.00026822] bias:  [-0.00192008]\n",
      "step:  3256 loss:  7.57897e-06 Weight:  [ 1.00026715] bias:  [-0.00191234]\n",
      "step:  3257 loss:  7.51787e-06 Weight:  [ 1.00026608] bias:  [-0.00190464]\n",
      "step:  3258 loss:  7.45837e-06 Weight:  [ 1.000265] bias:  [-0.00189696]\n",
      "step:  3259 loss:  7.39851e-06 Weight:  [ 1.00026393] bias:  [-0.00188932]\n",
      "step:  3260 loss:  7.33919e-06 Weight:  [ 1.00026286] bias:  [-0.00188171]\n",
      "step:  3261 loss:  7.27864e-06 Weight:  [ 1.00026178] bias:  [-0.00187413]\n",
      "step:  3262 loss:  7.2208e-06 Weight:  [ 1.00026071] bias:  [-0.00186657]\n",
      "step:  3263 loss:  7.16351e-06 Weight:  [ 1.00025964] bias:  [-0.00185904]\n",
      "step:  3264 loss:  7.10472e-06 Weight:  [ 1.00025856] bias:  [-0.00185155]\n",
      "step:  3265 loss:  7.0482e-06 Weight:  [ 1.00025749] bias:  [-0.00184408]\n",
      "step:  3266 loss:  6.98998e-06 Weight:  [ 1.00025654] bias:  [-0.00183663]\n",
      "step:  3267 loss:  6.93408e-06 Weight:  [ 1.00025547] bias:  [-0.00182922]\n",
      "step:  3268 loss:  6.87952e-06 Weight:  [ 1.00025439] bias:  [-0.00182185]\n",
      "step:  3269 loss:  6.8251e-06 Weight:  [ 1.00025344] bias:  [-0.00181449]\n",
      "step:  3270 loss:  6.76864e-06 Weight:  [ 1.00025237] bias:  [-0.00180718]\n",
      "step:  3271 loss:  6.71356e-06 Weight:  [ 1.00025141] bias:  [-0.00179988]\n",
      "step:  3272 loss:  6.66033e-06 Weight:  [ 1.00025034] bias:  [-0.00179263]\n",
      "step:  3273 loss:  6.60509e-06 Weight:  [ 1.00024939] bias:  [-0.00178539]\n",
      "step:  3274 loss:  6.55335e-06 Weight:  [ 1.00024831] bias:  [-0.0017782]\n",
      "step:  3275 loss:  6.49936e-06 Weight:  [ 1.00024736] bias:  [-0.00177102]\n",
      "step:  3276 loss:  6.44821e-06 Weight:  [ 1.00024641] bias:  [-0.00176388]\n",
      "step:  3277 loss:  6.39498e-06 Weight:  [ 1.00024545] bias:  [-0.00175676]\n",
      "step:  3278 loss:  6.34504e-06 Weight:  [ 1.00024438] bias:  [-0.00174969]\n",
      "step:  3279 loss:  6.29186e-06 Weight:  [ 1.00024343] bias:  [-0.00174263]\n",
      "step:  3280 loss:  6.24263e-06 Weight:  [ 1.00024247] bias:  [-0.0017356]\n",
      "step:  3281 loss:  6.19193e-06 Weight:  [ 1.00024152] bias:  [-0.00172861]\n",
      "step:  3282 loss:  6.14072e-06 Weight:  [ 1.00024056] bias:  [-0.00172164]\n",
      "step:  3283 loss:  6.09328e-06 Weight:  [ 1.00023949] bias:  [-0.00171471]\n",
      "step:  3284 loss:  6.04362e-06 Weight:  [ 1.00023854] bias:  [-0.00170779]\n",
      "step:  3285 loss:  5.99451e-06 Weight:  [ 1.00023758] bias:  [-0.00170091]\n",
      "step:  3286 loss:  5.9453e-06 Weight:  [ 1.00023663] bias:  [-0.00169405]\n",
      "step:  3287 loss:  5.89955e-06 Weight:  [ 1.00023568] bias:  [-0.00168722]\n",
      "step:  3288 loss:  5.8519e-06 Weight:  [ 1.00023472] bias:  [-0.00168042]\n",
      "step:  3289 loss:  5.80421e-06 Weight:  [ 1.00023377] bias:  [-0.00167364]\n",
      "step:  3290 loss:  5.75782e-06 Weight:  [ 1.00023282] bias:  [-0.00166689]\n",
      "step:  3291 loss:  5.71068e-06 Weight:  [ 1.00023186] bias:  [-0.00166017]\n",
      "step:  3292 loss:  5.66484e-06 Weight:  [ 1.00023091] bias:  [-0.00165347]\n",
      "step:  3293 loss:  5.61906e-06 Weight:  [ 1.00023007] bias:  [-0.0016468]\n",
      "step:  3294 loss:  5.5738e-06 Weight:  [ 1.00022912] bias:  [-0.00164016]\n",
      "step:  3295 loss:  5.52896e-06 Weight:  [ 1.00022817] bias:  [-0.00163355]\n",
      "step:  3296 loss:  5.48498e-06 Weight:  [ 1.00022721] bias:  [-0.00162696]\n",
      "step:  3297 loss:  5.44084e-06 Weight:  [ 1.00022638] bias:  [-0.00162039]\n",
      "step:  3298 loss:  5.39817e-06 Weight:  [ 1.00022542] bias:  [-0.00161386]\n",
      "step:  3299 loss:  5.35544e-06 Weight:  [ 1.00022447] bias:  [-0.00160736]\n",
      "step:  3300 loss:  5.31277e-06 Weight:  [ 1.00022352] bias:  [-0.00160087]\n",
      "step:  3301 loss:  5.26819e-06 Weight:  [ 1.00022268] bias:  [-0.00159441]\n",
      "step:  3302 loss:  5.22594e-06 Weight:  [ 1.00022185] bias:  [-0.00158797]\n",
      "step:  3303 loss:  5.18358e-06 Weight:  [ 1.00022089] bias:  [-0.00158158]\n",
      "step:  3304 loss:  5.14308e-06 Weight:  [ 1.00021994] bias:  [-0.0015752]\n",
      "step:  3305 loss:  5.09997e-06 Weight:  [ 1.00021911] bias:  [-0.00156884]\n",
      "step:  3306 loss:  5.06046e-06 Weight:  [ 1.00021827] bias:  [-0.00156252]\n",
      "step:  3307 loss:  5.02108e-06 Weight:  [ 1.00021732] bias:  [-0.00155622]\n",
      "step:  3308 loss:  4.97902e-06 Weight:  [ 1.00021648] bias:  [-0.00154994]\n",
      "step:  3309 loss:  4.93917e-06 Weight:  [ 1.00021565] bias:  [-0.00154369]\n",
      "step:  3310 loss:  4.89749e-06 Weight:  [ 1.00021482] bias:  [-0.00153747]\n",
      "step:  3311 loss:  4.85917e-06 Weight:  [ 1.00021386] bias:  [-0.00153127]\n",
      "step:  3312 loss:  4.81902e-06 Weight:  [ 1.00021303] bias:  [-0.00152509]\n",
      "step:  3313 loss:  4.78235e-06 Weight:  [ 1.00021219] bias:  [-0.00151895]\n",
      "step:  3314 loss:  4.74216e-06 Weight:  [ 1.00021136] bias:  [-0.00151282]\n",
      "step:  3315 loss:  4.70513e-06 Weight:  [ 1.0002104] bias:  [-0.00150673]\n",
      "step:  3316 loss:  4.66676e-06 Weight:  [ 1.00020957] bias:  [-0.00150065]\n",
      "step:  3317 loss:  4.62904e-06 Weight:  [ 1.00020874] bias:  [-0.00149459]\n",
      "step:  3318 loss:  4.59349e-06 Weight:  [ 1.0002079] bias:  [-0.00148856]\n",
      "step:  3319 loss:  4.55467e-06 Weight:  [ 1.00020707] bias:  [-0.00148256]\n",
      "step:  3320 loss:  4.51674e-06 Weight:  [ 1.00020623] bias:  [-0.00147658]\n",
      "step:  3321 loss:  4.48193e-06 Weight:  [ 1.0002054] bias:  [-0.00147062]\n",
      "step:  3322 loss:  4.44623e-06 Weight:  [ 1.00020456] bias:  [-0.00146469]\n",
      "step:  3323 loss:  4.40941e-06 Weight:  [ 1.00020373] bias:  [-0.00145878]\n",
      "step:  3324 loss:  4.37382e-06 Weight:  [ 1.00020289] bias:  [-0.0014529]\n",
      "step:  3325 loss:  4.34149e-06 Weight:  [ 1.00020206] bias:  [-0.00144704]\n",
      "step:  3326 loss:  4.30589e-06 Weight:  [ 1.00020123] bias:  [-0.0014412]\n",
      "step:  3327 loss:  4.2704e-06 Weight:  [ 1.00020039] bias:  [-0.00143538]\n",
      "step:  3328 loss:  4.23476e-06 Weight:  [ 1.00019968] bias:  [-0.00142958]\n",
      "step:  3329 loss:  4.20159e-06 Weight:  [ 1.00019884] bias:  [-0.00142382]\n",
      "step:  3330 loss:  4.16732e-06 Weight:  [ 1.00019813] bias:  [-0.00141807]\n",
      "step:  3331 loss:  4.13489e-06 Weight:  [ 1.00019729] bias:  [-0.00141236]\n",
      "step:  3332 loss:  4.1015e-06 Weight:  [ 1.00019646] bias:  [-0.00140667]\n",
      "step:  3333 loss:  4.06779e-06 Weight:  [ 1.00019562] bias:  [-0.00140099]\n",
      "step:  3334 loss:  4.03422e-06 Weight:  [ 1.00019491] bias:  [-0.00139534]\n",
      "step:  3335 loss:  4.00275e-06 Weight:  [ 1.00019407] bias:  [-0.00138971]\n",
      "step:  3336 loss:  3.97105e-06 Weight:  [ 1.00019324] bias:  [-0.00138411]\n",
      "step:  3337 loss:  3.9385e-06 Weight:  [ 1.00019252] bias:  [-0.00137852]\n",
      "step:  3338 loss:  3.90837e-06 Weight:  [ 1.00019169] bias:  [-0.00137296]\n",
      "step:  3339 loss:  3.87449e-06 Weight:  [ 1.00019097] bias:  [-0.00136742]\n",
      "step:  3340 loss:  3.84276e-06 Weight:  [ 1.00019026] bias:  [-0.0013619]\n",
      "step:  3341 loss:  3.81321e-06 Weight:  [ 1.00018942] bias:  [-0.00135641]\n",
      "step:  3342 loss:  3.78249e-06 Weight:  [ 1.00018871] bias:  [-0.00135094]\n",
      "step:  3343 loss:  3.75157e-06 Weight:  [ 1.00018799] bias:  [-0.00134549]\n",
      "step:  3344 loss:  3.7207e-06 Weight:  [ 1.00018716] bias:  [-0.00134007]\n",
      "step:  3345 loss:  3.69149e-06 Weight:  [ 1.00018644] bias:  [-0.00133467]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step:  3346 loss:  3.66336e-06 Weight:  [ 1.00018561] bias:  [-0.00132929]\n",
      "step:  3347 loss:  3.6318e-06 Weight:  [ 1.00018489] bias:  [-0.00132392]\n",
      "step:  3348 loss:  3.60437e-06 Weight:  [ 1.00018418] bias:  [-0.00131858]\n",
      "step:  3349 loss:  3.57317e-06 Weight:  [ 1.00018346] bias:  [-0.00131327]\n",
      "step:  3350 loss:  3.54569e-06 Weight:  [ 1.00018275] bias:  [-0.00130797]\n",
      "step:  3351 loss:  3.51575e-06 Weight:  [ 1.00018203] bias:  [-0.0013027]\n",
      "step:  3352 loss:  3.48949e-06 Weight:  [ 1.0001812] bias:  [-0.00129746]\n",
      "step:  3353 loss:  3.46171e-06 Weight:  [ 1.00018048] bias:  [-0.00129223]\n",
      "step:  3354 loss:  3.43218e-06 Weight:  [ 1.00017977] bias:  [-0.00128701]\n",
      "step:  3355 loss:  3.40378e-06 Weight:  [ 1.00017905] bias:  [-0.00128182]\n",
      "step:  3356 loss:  3.37837e-06 Weight:  [ 1.00017834] bias:  [-0.00127665]\n",
      "step:  3357 loss:  3.35008e-06 Weight:  [ 1.00017762] bias:  [-0.00127151]\n",
      "step:  3358 loss:  3.32348e-06 Weight:  [ 1.00017691] bias:  [-0.00126638]\n",
      "step:  3359 loss:  3.29725e-06 Weight:  [ 1.00017619] bias:  [-0.00126128]\n",
      "step:  3360 loss:  3.27002e-06 Weight:  [ 1.00017548] bias:  [-0.00125619]\n",
      "step:  3361 loss:  3.24447e-06 Weight:  [ 1.00017476] bias:  [-0.00125113]\n",
      "step:  3362 loss:  3.21732e-06 Weight:  [ 1.00017405] bias:  [-0.00124608]\n",
      "step:  3363 loss:  3.19076e-06 Weight:  [ 1.00017333] bias:  [-0.00124106]\n",
      "step:  3364 loss:  3.16745e-06 Weight:  [ 1.00017262] bias:  [-0.00123605]\n",
      "step:  3365 loss:  3.14086e-06 Weight:  [ 1.0001719] bias:  [-0.00123107]\n",
      "step:  3366 loss:  3.11449e-06 Weight:  [ 1.0001713] bias:  [-0.0012261]\n",
      "step:  3367 loss:  3.08966e-06 Weight:  [ 1.00017059] bias:  [-0.00122116]\n",
      "step:  3368 loss:  3.06723e-06 Weight:  [ 1.00016987] bias:  [-0.00121624]\n",
      "step:  3369 loss:  3.04097e-06 Weight:  [ 1.00016916] bias:  [-0.00121133]\n",
      "step:  3370 loss:  3.01711e-06 Weight:  [ 1.00016844] bias:  [-0.00120644]\n",
      "step:  3371 loss:  2.99178e-06 Weight:  [ 1.00016785] bias:  [-0.00120158]\n",
      "step:  3372 loss:  2.96825e-06 Weight:  [ 1.00016713] bias:  [-0.00119673]\n",
      "step:  3373 loss:  2.94318e-06 Weight:  [ 1.00016654] bias:  [-0.0011919]\n",
      "step:  3374 loss:  2.91983e-06 Weight:  [ 1.00016582] bias:  [-0.0011871]\n",
      "step:  3375 loss:  2.89718e-06 Weight:  [ 1.0001651] bias:  [-0.00118231]\n",
      "step:  3376 loss:  2.8726e-06 Weight:  [ 1.00016451] bias:  [-0.00117754]\n",
      "step:  3377 loss:  2.84979e-06 Weight:  [ 1.00016379] bias:  [-0.00117279]\n",
      "step:  3378 loss:  2.82781e-06 Weight:  [ 1.0001632] bias:  [-0.00116806]\n",
      "step:  3379 loss:  2.80388e-06 Weight:  [ 1.00016248] bias:  [-0.00116335]\n",
      "step:  3380 loss:  2.78213e-06 Weight:  [ 1.00016189] bias:  [-0.00115866]\n",
      "step:  3381 loss:  2.76063e-06 Weight:  [ 1.00016117] bias:  [-0.00115399]\n",
      "step:  3382 loss:  2.73757e-06 Weight:  [ 1.00016057] bias:  [-0.00114934]\n",
      "step:  3383 loss:  2.71629e-06 Weight:  [ 1.00015986] bias:  [-0.00114471]\n",
      "step:  3384 loss:  2.69408e-06 Weight:  [ 1.00015926] bias:  [-0.00114009]\n",
      "step:  3385 loss:  2.67309e-06 Weight:  [ 1.00015855] bias:  [-0.0011355]\n",
      "step:  3386 loss:  2.6496e-06 Weight:  [ 1.00015795] bias:  [-0.00113091]\n",
      "step:  3387 loss:  2.62869e-06 Weight:  [ 1.00015736] bias:  [-0.00112635]\n",
      "step:  3388 loss:  2.60815e-06 Weight:  [ 1.00015664] bias:  [-0.00112181]\n",
      "step:  3389 loss:  2.5869e-06 Weight:  [ 1.00015604] bias:  [-0.00111729]\n",
      "step:  3390 loss:  2.56506e-06 Weight:  [ 1.00015545] bias:  [-0.00111278]\n",
      "step:  3391 loss:  2.54527e-06 Weight:  [ 1.00015485] bias:  [-0.00110829]\n",
      "step:  3392 loss:  2.52583e-06 Weight:  [ 1.00015414] bias:  [-0.00110383]\n",
      "step:  3393 loss:  2.50548e-06 Weight:  [ 1.00015354] bias:  [-0.00109938]\n",
      "step:  3394 loss:  2.48423e-06 Weight:  [ 1.00015295] bias:  [-0.00109494]\n",
      "step:  3395 loss:  2.46532e-06 Weight:  [ 1.00015235] bias:  [-0.00109052]\n",
      "step:  3396 loss:  2.44621e-06 Weight:  [ 1.00015163] bias:  [-0.00108613]\n",
      "step:  3397 loss:  2.42517e-06 Weight:  [ 1.00015104] bias:  [-0.00108174]\n",
      "step:  3398 loss:  2.40626e-06 Weight:  [ 1.00015044] bias:  [-0.00107738]\n",
      "step:  3399 loss:  2.38631e-06 Weight:  [ 1.00014985] bias:  [-0.00107303]\n",
      "step:  3400 loss:  2.36678e-06 Weight:  [ 1.00014925] bias:  [-0.0010687]\n",
      "step:  3401 loss:  2.34682e-06 Weight:  [ 1.00014865] bias:  [-0.00106439]\n",
      "step:  3402 loss:  2.32945e-06 Weight:  [ 1.00014806] bias:  [-0.00106009]\n",
      "step:  3403 loss:  2.30888e-06 Weight:  [ 1.00014746] bias:  [-0.00105582]\n",
      "step:  3404 loss:  2.29141e-06 Weight:  [ 1.00014687] bias:  [-0.00105156]\n",
      "step:  3405 loss:  2.2728e-06 Weight:  [ 1.00014627] bias:  [-0.00104732]\n",
      "step:  3406 loss:  2.25558e-06 Weight:  [ 1.00014567] bias:  [-0.00104309]\n",
      "step:  3407 loss:  2.23696e-06 Weight:  [ 1.00014508] bias:  [-0.00103888]\n",
      "step:  3408 loss:  2.21872e-06 Weight:  [ 1.00014448] bias:  [-0.00103469]\n",
      "step:  3409 loss:  2.2012e-06 Weight:  [ 1.00014389] bias:  [-0.00103052]\n",
      "step:  3410 loss:  2.18285e-06 Weight:  [ 1.00014341] bias:  [-0.00102635]\n",
      "step:  3411 loss:  2.16617e-06 Weight:  [ 1.00014281] bias:  [-0.00102222]\n",
      "step:  3412 loss:  2.1488e-06 Weight:  [ 1.00014222] bias:  [-0.0010181]\n",
      "step:  3413 loss:  2.12954e-06 Weight:  [ 1.00014162] bias:  [-0.00101399]\n",
      "step:  3414 loss:  2.11393e-06 Weight:  [ 1.00014102] bias:  [-0.0010099]\n",
      "step:  3415 loss:  2.09721e-06 Weight:  [ 1.00014043] bias:  [-0.00100583]\n",
      "step:  3416 loss:  2.07877e-06 Weight:  [ 1.00013995] bias:  [-0.00100176]\n",
      "step:  3417 loss:  2.06432e-06 Weight:  [ 1.00013936] bias:  [-0.00099773]\n",
      "step:  3418 loss:  2.04637e-06 Weight:  [ 1.00013876] bias:  [-0.00099371]\n",
      "step:  3419 loss:  2.0303e-06 Weight:  [ 1.00013816] bias:  [-0.0009897]\n",
      "step:  3420 loss:  2.0137e-06 Weight:  [ 1.00013769] bias:  [-0.0009857]\n",
      "step:  3421 loss:  1.99687e-06 Weight:  [ 1.00013709] bias:  [-0.00098172]\n",
      "step:  3422 loss:  1.98259e-06 Weight:  [ 1.00013649] bias:  [-0.00097776]\n",
      "step:  3423 loss:  1.96528e-06 Weight:  [ 1.00013602] bias:  [-0.00097381]\n",
      "step:  3424 loss:  1.94923e-06 Weight:  [ 1.00013542] bias:  [-0.00096989]\n",
      "step:  3425 loss:  1.9338e-06 Weight:  [ 1.00013494] bias:  [-0.00096597]\n",
      "step:  3426 loss:  1.91853e-06 Weight:  [ 1.00013435] bias:  [-0.00096208]\n",
      "step:  3427 loss:  1.90285e-06 Weight:  [ 1.00013387] bias:  [-0.00095819]\n",
      "step:  3428 loss:  1.88732e-06 Weight:  [ 1.00013328] bias:  [-0.00095433]\n",
      "step:  3429 loss:  1.87247e-06 Weight:  [ 1.0001328] bias:  [-0.00095048]\n",
      "step:  3430 loss:  1.85632e-06 Weight:  [ 1.0001322] bias:  [-0.00094665]\n",
      "step:  3431 loss:  1.84169e-06 Weight:  [ 1.00013173] bias:  [-0.00094283]\n",
      "step:  3432 loss:  1.82667e-06 Weight:  [ 1.00013113] bias:  [-0.00093903]\n",
      "step:  3433 loss:  1.81255e-06 Weight:  [ 1.00013065] bias:  [-0.00093524]\n",
      "step:  3434 loss:  1.79775e-06 Weight:  [ 1.00013018] bias:  [-0.00093147]\n",
      "step:  3435 loss:  1.78308e-06 Weight:  [ 1.00012958] bias:  [-0.00092772]\n",
      "step:  3436 loss:  1.76948e-06 Weight:  [ 1.0001291] bias:  [-0.00092398]\n",
      "step:  3437 loss:  1.75502e-06 Weight:  [ 1.00012851] bias:  [-0.00092025]\n",
      "step:  3438 loss:  1.74149e-06 Weight:  [ 1.00012803] bias:  [-0.00091654]\n",
      "step:  3439 loss:  1.72636e-06 Weight:  [ 1.00012755] bias:  [-0.00091284]\n",
      "step:  3440 loss:  1.71377e-06 Weight:  [ 1.00012696] bias:  [-0.00090917]\n",
      "step:  3441 loss:  1.69921e-06 Weight:  [ 1.00012648] bias:  [-0.0009055]\n",
      "step:  3442 loss:  1.68611e-06 Weight:  [ 1.00012589] bias:  [-0.00090186]\n",
      "step:  3443 loss:  1.67166e-06 Weight:  [ 1.00012541] bias:  [-0.00089821]\n",
      "step:  3444 loss:  1.65844e-06 Weight:  [ 1.00012493] bias:  [-0.00089458]\n",
      "step:  3445 loss:  1.64515e-06 Weight:  [ 1.00012445] bias:  [-0.00089097]\n",
      "step:  3446 loss:  1.63213e-06 Weight:  [ 1.00012398] bias:  [-0.00088738]\n",
      "step:  3447 loss:  1.61953e-06 Weight:  [ 1.00012338] bias:  [-0.00088381]\n",
      "step:  3448 loss:  1.6056e-06 Weight:  [ 1.0001229] bias:  [-0.00088024]\n",
      "step:  3449 loss:  1.59326e-06 Weight:  [ 1.00012243] bias:  [-0.00087668]\n",
      "step:  3450 loss:  1.58054e-06 Weight:  [ 1.00012195] bias:  [-0.00087314]\n",
      "step:  3451 loss:  1.56695e-06 Weight:  [ 1.00012147] bias:  [-0.00086962]\n",
      "step:  3452 loss:  1.55457e-06 Weight:  [ 1.000121] bias:  [-0.00086611]\n",
      "step:  3453 loss:  1.54279e-06 Weight:  [ 1.00012052] bias:  [-0.00086262]\n",
      "step:  3454 loss:  1.52923e-06 Weight:  [ 1.00012004] bias:  [-0.00085914]\n",
      "step:  3455 loss:  1.51793e-06 Weight:  [ 1.00011957] bias:  [-0.00085568]\n",
      "step:  3456 loss:  1.50569e-06 Weight:  [ 1.00011909] bias:  [-0.00085224]\n",
      "step:  3457 loss:  1.49322e-06 Weight:  [ 1.00011861] bias:  [-0.0008488]\n",
      "step:  3458 loss:  1.48131e-06 Weight:  [ 1.00011814] bias:  [-0.00084539]\n",
      "step:  3459 loss:  1.47034e-06 Weight:  [ 1.00011754] bias:  [-0.00084199]\n",
      "step:  3460 loss:  1.45702e-06 Weight:  [ 1.00011718] bias:  [-0.00083858]\n",
      "step:  3461 loss:  1.44648e-06 Weight:  [ 1.00011671] bias:  [-0.00083521]\n",
      "step:  3462 loss:  1.43394e-06 Weight:  [ 1.00011623] bias:  [-0.00083184]\n",
      "step:  3463 loss:  1.42317e-06 Weight:  [ 1.00011575] bias:  [-0.00082849]\n",
      "step:  3464 loss:  1.41095e-06 Weight:  [ 1.00011528] bias:  [-0.00082515]\n",
      "step:  3465 loss:  1.40041e-06 Weight:  [ 1.0001148] bias:  [-0.00082183]\n",
      "step:  3466 loss:  1.38844e-06 Weight:  [ 1.00011432] bias:  [-0.00081851]\n",
      "step:  3467 loss:  1.37798e-06 Weight:  [ 1.00011384] bias:  [-0.00081521]\n",
      "step:  3468 loss:  1.36605e-06 Weight:  [ 1.00011337] bias:  [-0.00081192]\n",
      "step:  3469 loss:  1.3559e-06 Weight:  [ 1.00011289] bias:  [-0.00080864]\n",
      "step:  3470 loss:  1.34446e-06 Weight:  [ 1.00011253] bias:  [-0.00080538]\n",
      "step:  3471 loss:  1.33406e-06 Weight:  [ 1.00011206] bias:  [-0.00080213]\n",
      "step:  3472 loss:  1.32293e-06 Weight:  [ 1.00011158] bias:  [-0.0007989]\n",
      "step:  3473 loss:  1.31176e-06 Weight:  [ 1.0001111] bias:  [-0.00079568]\n",
      "step:  3474 loss:  1.30165e-06 Weight:  [ 1.00011063] bias:  [-0.00079247]\n",
      "step:  3475 loss:  1.29105e-06 Weight:  [ 1.00011027] bias:  [-0.00078927]\n",
      "step:  3476 loss:  1.27986e-06 Weight:  [ 1.00010979] bias:  [-0.00078609]\n",
      "step:  3477 loss:  1.27025e-06 Weight:  [ 1.00010931] bias:  [-0.00078292]\n",
      "step:  3478 loss:  1.25999e-06 Weight:  [ 1.00010896] bias:  [-0.00077976]\n",
      "step:  3479 loss:  1.24928e-06 Weight:  [ 1.00010848] bias:  [-0.00077661]\n",
      "step:  3480 loss:  1.2406e-06 Weight:  [ 1.000108] bias:  [-0.00077349]\n",
      "step:  3481 loss:  1.22943e-06 Weight:  [ 1.00010765] bias:  [-0.00077036]\n",
      "step:  3482 loss:  1.22017e-06 Weight:  [ 1.00010717] bias:  [-0.00076726]\n",
      "step:  3483 loss:  1.20973e-06 Weight:  [ 1.00010681] bias:  [-0.00076416]\n",
      "step:  3484 loss:  1.20083e-06 Weight:  [ 1.00010633] bias:  [-0.00076109]\n",
      "step:  3485 loss:  1.19093e-06 Weight:  [ 1.00010586] bias:  [-0.00075802]\n",
      "step:  3486 loss:  1.18104e-06 Weight:  [ 1.0001055] bias:  [-0.00075497]\n",
      "step:  3487 loss:  1.17128e-06 Weight:  [ 1.00010502] bias:  [-0.00075192]\n",
      "step:  3488 loss:  1.16258e-06 Weight:  [ 1.00010455] bias:  [-0.0007489]\n",
      "step:  3489 loss:  1.15305e-06 Weight:  [ 1.00010419] bias:  [-0.00074587]\n",
      "step:  3490 loss:  1.14329e-06 Weight:  [ 1.00010371] bias:  [-0.00074287]\n",
      "step:  3491 loss:  1.1349e-06 Weight:  [ 1.00010335] bias:  [-0.00073987]\n",
      "step:  3492 loss:  1.12522e-06 Weight:  [ 1.000103] bias:  [-0.00073688]\n",
      "step:  3493 loss:  1.11598e-06 Weight:  [ 1.00010252] bias:  [-0.00073391]\n",
      "step:  3494 loss:  1.10697e-06 Weight:  [ 1.00010216] bias:  [-0.00073095]\n",
      "step:  3495 loss:  1.09865e-06 Weight:  [ 1.00010169] bias:  [-0.00072802]\n",
      "step:  3496 loss:  1.09001e-06 Weight:  [ 1.00010121] bias:  [-0.00072508]\n",
      "step:  3497 loss:  1.0807e-06 Weight:  [ 1.00010085] bias:  [-0.00072215]\n",
      "step:  3498 loss:  1.07271e-06 Weight:  [ 1.00010037] bias:  [-0.00071924]\n",
      "step:  3499 loss:  1.06327e-06 Weight:  [ 1.00010002] bias:  [-0.00071634]\n",
      "step:  3500 loss:  1.05547e-06 Weight:  [ 1.00009966] bias:  [-0.00071344]\n",
      "step:  3501 loss:  1.04622e-06 Weight:  [ 1.0000993] bias:  [-0.00071057]\n",
      "step:  3502 loss:  1.03763e-06 Weight:  [ 1.00009882] bias:  [-0.00070771]\n",
      "step:  3503 loss:  1.02943e-06 Weight:  [ 1.00009847] bias:  [-0.00070486]\n",
      "step:  3504 loss:  1.02101e-06 Weight:  [ 1.00009811] bias:  [-0.00070201]\n",
      "step:  3505 loss:  1.01345e-06 Weight:  [ 1.00009763] bias:  [-0.00069919]\n",
      "step:  3506 loss:  1.00534e-06 Weight:  [ 1.00009727] bias:  [-0.00069637]\n",
      "step:  3507 loss:  9.97181e-07 Weight:  [ 1.00009692] bias:  [-0.00069356]\n",
      "step:  3508 loss:  9.89059e-07 Weight:  [ 1.00009644] bias:  [-0.00069077]\n",
      "step:  3509 loss:  9.80905e-07 Weight:  [ 1.00009608] bias:  [-0.00068798]\n",
      "step:  3510 loss:  9.73896e-07 Weight:  [ 1.00009573] bias:  [-0.00068521]\n",
      "step:  3511 loss:  9.65895e-07 Weight:  [ 1.00009525] bias:  [-0.00068245]\n",
      "step:  3512 loss:  9.57242e-07 Weight:  [ 1.00009489] bias:  [-0.00067969]\n",
      "step:  3513 loss:  9.4961e-07 Weight:  [ 1.00009453] bias:  [-0.00067694]\n",
      "step:  3514 loss:  9.41536e-07 Weight:  [ 1.00009418] bias:  [-0.00067421]\n",
      "step:  3515 loss:  9.34853e-07 Weight:  [ 1.00009382] bias:  [-0.00067149]\n",
      "step:  3516 loss:  9.27084e-07 Weight:  [ 1.00009346] bias:  [-0.00066879]\n",
      "step:  3517 loss:  9.19763e-07 Weight:  [ 1.00009298] bias:  [-0.0006661]\n",
      "step:  3518 loss:  9.12803e-07 Weight:  [ 1.00009263] bias:  [-0.00066341]\n",
      "step:  3519 loss:  9.04189e-07 Weight:  [ 1.00009227] bias:  [-0.00066073]\n",
      "step:  3520 loss:  8.97614e-07 Weight:  [ 1.00009191] bias:  [-0.00065806]\n",
      "step:  3521 loss:  8.90322e-07 Weight:  [ 1.00009155] bias:  [-0.00065541]\n",
      "step:  3522 loss:  8.83279e-07 Weight:  [ 1.0000912] bias:  [-0.00065277]\n",
      "step:  3523 loss:  8.76379e-07 Weight:  [ 1.00009084] bias:  [-0.00065014]\n",
      "step:  3524 loss:  8.68395e-07 Weight:  [ 1.00009048] bias:  [-0.00064752]\n",
      "step:  3525 loss:  8.62336e-07 Weight:  [ 1.00009012] bias:  [-0.00064491]\n",
      "step:  3526 loss:  8.55088e-07 Weight:  [ 1.00008976] bias:  [-0.00064232]\n",
      "step:  3527 loss:  8.47353e-07 Weight:  [ 1.00008941] bias:  [-0.00063973]\n",
      "step:  3528 loss:  8.41434e-07 Weight:  [ 1.00008905] bias:  [-0.00063715]\n",
      "step:  3529 loss:  8.34724e-07 Weight:  [ 1.00008869] bias:  [-0.00063459]\n",
      "step:  3530 loss:  8.28391e-07 Weight:  [ 1.00008821] bias:  [-0.00063204]\n",
      "step:  3531 loss:  8.20788e-07 Weight:  [ 1.00008798] bias:  [-0.00062948]\n",
      "step:  3532 loss:  8.14215e-07 Weight:  [ 1.00008762] bias:  [-0.00062695]\n",
      "step:  3533 loss:  8.08828e-07 Weight:  [ 1.00008726] bias:  [-0.00062443]\n",
      "step:  3534 loss:  8.01539e-07 Weight:  [ 1.0000869] bias:  [-0.00062191]\n",
      "step:  3535 loss:  7.94787e-07 Weight:  [ 1.00008655] bias:  [-0.00061941]\n",
      "step:  3536 loss:  7.89527e-07 Weight:  [ 1.00008619] bias:  [-0.00061692]\n",
      "step:  3537 loss:  7.8259e-07 Weight:  [ 1.00008583] bias:  [-0.00061443]\n",
      "step:  3538 loss:  7.76327e-07 Weight:  [ 1.00008547] bias:  [-0.00061196]\n",
      "step:  3539 loss:  7.69443e-07 Weight:  [ 1.00008512] bias:  [-0.00060949]\n",
      "step:  3540 loss:  7.63363e-07 Weight:  [ 1.00008476] bias:  [-0.00060703]\n",
      "step:  3541 loss:  7.57315e-07 Weight:  [ 1.0000844] bias:  [-0.00060457]\n",
      "step:  3542 loss:  7.51319e-07 Weight:  [ 1.00008416] bias:  [-0.00060213]\n",
      "step:  3543 loss:  7.45554e-07 Weight:  [ 1.0000838] bias:  [-0.00059971]\n",
      "step:  3544 loss:  7.39447e-07 Weight:  [ 1.00008345] bias:  [-0.00059729]\n",
      "step:  3545 loss:  7.33774e-07 Weight:  [ 1.00008309] bias:  [-0.00059489]\n",
      "step:  3546 loss:  7.27056e-07 Weight:  [ 1.00008273] bias:  [-0.00059249]\n",
      "step:  3547 loss:  7.22188e-07 Weight:  [ 1.00008237] bias:  [-0.0005901]\n",
      "step:  3548 loss:  7.15781e-07 Weight:  [ 1.00008214] bias:  [-0.00058771]\n",
      "step:  3549 loss:  7.1006e-07 Weight:  [ 1.00008178] bias:  [-0.00058535]\n",
      "step:  3550 loss:  7.04447e-07 Weight:  [ 1.00008142] bias:  [-0.00058299]\n",
      "step:  3551 loss:  6.99034e-07 Weight:  [ 1.00008106] bias:  [-0.00058063]\n",
      "step:  3552 loss:  6.92863e-07 Weight:  [ 1.00008082] bias:  [-0.00057829]\n",
      "step:  3553 loss:  6.8828e-07 Weight:  [ 1.00008047] bias:  [-0.00057596]\n",
      "step:  3554 loss:  6.82179e-07 Weight:  [ 1.00008011] bias:  [-0.00057364]\n",
      "step:  3555 loss:  6.76043e-07 Weight:  [ 1.00007987] bias:  [-0.00057132]\n",
      "step:  3556 loss:  6.71336e-07 Weight:  [ 1.00007951] bias:  [-0.00056903]\n",
      "step:  3557 loss:  6.65548e-07 Weight:  [ 1.00007915] bias:  [-0.00056673]\n",
      "step:  3558 loss:  6.60518e-07 Weight:  [ 1.0000788] bias:  [-0.00056445]\n",
      "step:  3559 loss:  6.54635e-07 Weight:  [ 1.00007856] bias:  [-0.00056217]\n",
      "step:  3560 loss:  6.503e-07 Weight:  [ 1.0000782] bias:  [-0.00055991]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step:  3561 loss:  6.44462e-07 Weight:  [ 1.00007784] bias:  [-0.00055765]\n",
      "step:  3562 loss:  6.38924e-07 Weight:  [ 1.00007761] bias:  [-0.0005554]\n",
      "step:  3563 loss:  6.34738e-07 Weight:  [ 1.00007725] bias:  [-0.00055316]\n",
      "step:  3564 loss:  6.29027e-07 Weight:  [ 1.00007689] bias:  [-0.00055093]\n",
      "step:  3565 loss:  6.23652e-07 Weight:  [ 1.00007665] bias:  [-0.0005487]\n",
      "step:  3566 loss:  6.19303e-07 Weight:  [ 1.00007629] bias:  [-0.00054649]\n",
      "step:  3567 loss:  6.1393e-07 Weight:  [ 1.00007606] bias:  [-0.00054428]\n",
      "step:  3568 loss:  6.09111e-07 Weight:  [ 1.0000757] bias:  [-0.00054209]\n",
      "step:  3569 loss:  6.04643e-07 Weight:  [ 1.00007534] bias:  [-0.00053991]\n",
      "step:  3570 loss:  5.99427e-07 Weight:  [ 1.0000751] bias:  [-0.00053772]\n",
      "step:  3571 loss:  5.94498e-07 Weight:  [ 1.00007474] bias:  [-0.00053555]\n",
      "step:  3572 loss:  5.89352e-07 Weight:  [ 1.00007451] bias:  [-0.00053338]\n",
      "step:  3573 loss:  5.84824e-07 Weight:  [ 1.00007415] bias:  [-0.00053124]\n",
      "step:  3574 loss:  5.80391e-07 Weight:  [ 1.00007391] bias:  [-0.00052909]\n",
      "step:  3575 loss:  5.75442e-07 Weight:  [ 1.00007355] bias:  [-0.00052696]\n",
      "step:  3576 loss:  5.70152e-07 Weight:  [ 1.00007331] bias:  [-0.00052482]\n",
      "step:  3577 loss:  5.66741e-07 Weight:  [ 1.00007296] bias:  [-0.00052271]\n",
      "step:  3578 loss:  5.61542e-07 Weight:  [ 1.00007272] bias:  [-0.0005206]\n",
      "step:  3579 loss:  5.56928e-07 Weight:  [ 1.00007248] bias:  [-0.0005185]\n",
      "step:  3580 loss:  5.52446e-07 Weight:  [ 1.00007212] bias:  [-0.00051641]\n",
      "step:  3581 loss:  5.48504e-07 Weight:  [ 1.00007176] bias:  [-0.00051433]\n",
      "step:  3582 loss:  5.43792e-07 Weight:  [ 1.00007153] bias:  [-0.00051225]\n",
      "step:  3583 loss:  5.3961e-07 Weight:  [ 1.00007129] bias:  [-0.00051018]\n",
      "step:  3584 loss:  5.35376e-07 Weight:  [ 1.00007093] bias:  [-0.00050813]\n",
      "step:  3585 loss:  5.30685e-07 Weight:  [ 1.00007069] bias:  [-0.00050607]\n",
      "step:  3586 loss:  5.2603e-07 Weight:  [ 1.00007045] bias:  [-0.00050403]\n",
      "step:  3587 loss:  5.22118e-07 Weight:  [ 1.0000701] bias:  [-0.000502]\n",
      "step:  3588 loss:  5.18584e-07 Weight:  [ 1.00006986] bias:  [-0.00049998]\n",
      "step:  3589 loss:  5.13849e-07 Weight:  [ 1.00006962] bias:  [-0.00049796]\n",
      "step:  3590 loss:  5.09925e-07 Weight:  [ 1.00006926] bias:  [-0.00049596]\n",
      "step:  3591 loss:  5.05509e-07 Weight:  [ 1.00006902] bias:  [-0.00049396]\n",
      "step:  3592 loss:  5.01758e-07 Weight:  [ 1.00006878] bias:  [-0.00049197]\n",
      "step:  3593 loss:  4.97182e-07 Weight:  [ 1.00006843] bias:  [-0.00048999]\n",
      "step:  3594 loss:  4.93654e-07 Weight:  [ 1.00006819] bias:  [-0.00048802]\n",
      "step:  3595 loss:  4.89561e-07 Weight:  [ 1.00006795] bias:  [-0.00048605]\n",
      "step:  3596 loss:  4.85248e-07 Weight:  [ 1.00006771] bias:  [-0.00048409]\n",
      "step:  3597 loss:  4.81416e-07 Weight:  [ 1.00006735] bias:  [-0.00048215]\n",
      "step:  3598 loss:  4.77444e-07 Weight:  [ 1.00006711] bias:  [-0.0004802]\n",
      "step:  3599 loss:  4.73814e-07 Weight:  [ 1.00006688] bias:  [-0.00047827]\n",
      "step:  3600 loss:  4.69683e-07 Weight:  [ 1.00006664] bias:  [-0.00047635]\n",
      "step:  3601 loss:  4.67106e-07 Weight:  [ 1.00006628] bias:  [-0.00047444]\n",
      "step:  3602 loss:  4.63237e-07 Weight:  [ 1.00006592] bias:  [-0.00047253]\n",
      "step:  3603 loss:  4.59158e-07 Weight:  [ 1.00006568] bias:  [-0.00047062]\n",
      "step:  3604 loss:  4.55911e-07 Weight:  [ 1.00006545] bias:  [-0.00046872]\n",
      "step:  3605 loss:  4.51905e-07 Weight:  [ 1.00006521] bias:  [-0.00046683]\n",
      "step:  3606 loss:  4.47768e-07 Weight:  [ 1.00006497] bias:  [-0.00046494]\n",
      "step:  3607 loss:  4.43798e-07 Weight:  [ 1.00006473] bias:  [-0.00046307]\n",
      "step:  3608 loss:  4.40733e-07 Weight:  [ 1.00006449] bias:  [-0.0004612]\n",
      "step:  3609 loss:  4.36844e-07 Weight:  [ 1.00006413] bias:  [-0.00045935]\n",
      "step:  3610 loss:  4.33352e-07 Weight:  [ 1.0000639] bias:  [-0.00045749]\n",
      "step:  3611 loss:  4.30077e-07 Weight:  [ 1.00006366] bias:  [-0.00045565]\n",
      "step:  3612 loss:  4.26912e-07 Weight:  [ 1.00006342] bias:  [-0.00045381]\n",
      "step:  3613 loss:  4.23134e-07 Weight:  [ 1.00006318] bias:  [-0.00045198]\n",
      "step:  3614 loss:  4.19815e-07 Weight:  [ 1.00006294] bias:  [-0.00045016]\n",
      "step:  3615 loss:  4.16563e-07 Weight:  [ 1.00006258] bias:  [-0.00044836]\n",
      "step:  3616 loss:  4.13302e-07 Weight:  [ 1.00006235] bias:  [-0.00044655]\n",
      "step:  3617 loss:  4.10171e-07 Weight:  [ 1.00006211] bias:  [-0.00044474]\n",
      "step:  3618 loss:  4.06618e-07 Weight:  [ 1.00006187] bias:  [-0.00044295]\n",
      "step:  3619 loss:  4.039e-07 Weight:  [ 1.00006163] bias:  [-0.00044117]\n",
      "step:  3620 loss:  3.99693e-07 Weight:  [ 1.00006139] bias:  [-0.00043939]\n",
      "step:  3621 loss:  3.96828e-07 Weight:  [ 1.00006115] bias:  [-0.00043762]\n",
      "step:  3622 loss:  3.93333e-07 Weight:  [ 1.00006092] bias:  [-0.00043585]\n",
      "step:  3623 loss:  3.90706e-07 Weight:  [ 1.00006068] bias:  [-0.0004341]\n",
      "step:  3624 loss:  3.87494e-07 Weight:  [ 1.00006044] bias:  [-0.00043236]\n",
      "step:  3625 loss:  3.84582e-07 Weight:  [ 1.0000602] bias:  [-0.00043062]\n",
      "step:  3626 loss:  3.80686e-07 Weight:  [ 1.00005996] bias:  [-0.00042889]\n",
      "step:  3627 loss:  3.78218e-07 Weight:  [ 1.00005972] bias:  [-0.00042716]\n",
      "step:  3628 loss:  3.75059e-07 Weight:  [ 1.00005949] bias:  [-0.00042544]\n",
      "step:  3629 loss:  3.72194e-07 Weight:  [ 1.00005925] bias:  [-0.00042374]\n",
      "step:  3630 loss:  3.69133e-07 Weight:  [ 1.00005901] bias:  [-0.00042203]\n",
      "step:  3631 loss:  3.66051e-07 Weight:  [ 1.00005877] bias:  [-0.00042034]\n",
      "step:  3632 loss:  3.63015e-07 Weight:  [ 1.00005853] bias:  [-0.00041865]\n",
      "step:  3633 loss:  3.60443e-07 Weight:  [ 1.00005829] bias:  [-0.00041696]\n",
      "step:  3634 loss:  3.57358e-07 Weight:  [ 1.00005805] bias:  [-0.00041529]\n",
      "step:  3635 loss:  3.55084e-07 Weight:  [ 1.00005782] bias:  [-0.00041362]\n",
      "step:  3636 loss:  3.51453e-07 Weight:  [ 1.00005758] bias:  [-0.00041196]\n",
      "step:  3637 loss:  3.48994e-07 Weight:  [ 1.00005734] bias:  [-0.0004103]\n",
      "step:  3638 loss:  3.46042e-07 Weight:  [ 1.0000571] bias:  [-0.00040865]\n",
      "step:  3639 loss:  3.43804e-07 Weight:  [ 1.00005686] bias:  [-0.00040701]\n",
      "step:  3640 loss:  3.40303e-07 Weight:  [ 1.00005662] bias:  [-0.00040537]\n",
      "step:  3641 loss:  3.37854e-07 Weight:  [ 1.00005639] bias:  [-0.00040373]\n",
      "step:  3642 loss:  3.35049e-07 Weight:  [ 1.00005615] bias:  [-0.0004021]\n",
      "step:  3643 loss:  3.32421e-07 Weight:  [ 1.00005591] bias:  [-0.00040048]\n",
      "step:  3644 loss:  3.2961e-07 Weight:  [ 1.00005567] bias:  [-0.00039886]\n",
      "step:  3645 loss:  3.27269e-07 Weight:  [ 1.00005543] bias:  [-0.00039725]\n",
      "step:  3646 loss:  3.23898e-07 Weight:  [ 1.00005531] bias:  [-0.00039564]\n",
      "step:  3647 loss:  3.21678e-07 Weight:  [ 1.00005507] bias:  [-0.00039405]\n",
      "step:  3648 loss:  3.19582e-07 Weight:  [ 1.00005484] bias:  [-0.00039247]\n",
      "step:  3649 loss:  3.16457e-07 Weight:  [ 1.0000546] bias:  [-0.00039088]\n",
      "step:  3650 loss:  3.13759e-07 Weight:  [ 1.00005436] bias:  [-0.00038931]\n",
      "step:  3651 loss:  3.11687e-07 Weight:  [ 1.00005412] bias:  [-0.00038774]\n",
      "step:  3652 loss:  3.08971e-07 Weight:  [ 1.00005388] bias:  [-0.00038617]\n",
      "step:  3653 loss:  3.06468e-07 Weight:  [ 1.00005376] bias:  [-0.0003846]\n",
      "step:  3654 loss:  3.0435e-07 Weight:  [ 1.00005352] bias:  [-0.00038306]\n",
      "step:  3655 loss:  3.01321e-07 Weight:  [ 1.00005329] bias:  [-0.00038151]\n",
      "step:  3656 loss:  2.99375e-07 Weight:  [ 1.00005305] bias:  [-0.00037998]\n",
      "step:  3657 loss:  2.96998e-07 Weight:  [ 1.00005281] bias:  [-0.00037844]\n",
      "step:  3658 loss:  2.94391e-07 Weight:  [ 1.00005269] bias:  [-0.00037691]\n",
      "step:  3659 loss:  2.92015e-07 Weight:  [ 1.00005245] bias:  [-0.0003754]\n",
      "step:  3660 loss:  2.89568e-07 Weight:  [ 1.00005221] bias:  [-0.00037388]\n",
      "step:  3661 loss:  2.87538e-07 Weight:  [ 1.00005198] bias:  [-0.00037238]\n",
      "step:  3662 loss:  2.8525e-07 Weight:  [ 1.00005174] bias:  [-0.00037087]\n",
      "step:  3663 loss:  2.82585e-07 Weight:  [ 1.00005162] bias:  [-0.00036937]\n",
      "step:  3664 loss:  2.80428e-07 Weight:  [ 1.00005138] bias:  [-0.00036788]\n",
      "step:  3665 loss:  2.7829e-07 Weight:  [ 1.00005114] bias:  [-0.0003664]\n",
      "step:  3666 loss:  2.76034e-07 Weight:  [ 1.0000509] bias:  [-0.00036492]\n",
      "step:  3667 loss:  2.73675e-07 Weight:  [ 1.00005078] bias:  [-0.00036344]\n",
      "step:  3668 loss:  2.71331e-07 Weight:  [ 1.00005054] bias:  [-0.00036197]\n",
      "step:  3669 loss:  2.69048e-07 Weight:  [ 1.00005043] bias:  [-0.00036051]\n",
      "step:  3670 loss:  2.6734e-07 Weight:  [ 1.00005019] bias:  [-0.00035906]\n",
      "step:  3671 loss:  2.64699e-07 Weight:  [ 1.00004995] bias:  [-0.00035762]\n",
      "step:  3672 loss:  2.6301e-07 Weight:  [ 1.00004971] bias:  [-0.00035617]\n",
      "step:  3673 loss:  2.60916e-07 Weight:  [ 1.00004947] bias:  [-0.00035474]\n",
      "step:  3674 loss:  2.58856e-07 Weight:  [ 1.00004935] bias:  [-0.0003533]\n",
      "step:  3675 loss:  2.57063e-07 Weight:  [ 1.00004911] bias:  [-0.00035188]\n",
      "step:  3676 loss:  2.54662e-07 Weight:  [ 1.00004888] bias:  [-0.00035045]\n",
      "step:  3677 loss:  2.52582e-07 Weight:  [ 1.00004876] bias:  [-0.00034904]\n",
      "step:  3678 loss:  2.50296e-07 Weight:  [ 1.00004852] bias:  [-0.00034763]\n",
      "step:  3679 loss:  2.48315e-07 Weight:  [ 1.0000484] bias:  [-0.00034622]\n",
      "step:  3680 loss:  2.46594e-07 Weight:  [ 1.00004816] bias:  [-0.00034483]\n",
      "step:  3681 loss:  2.44098e-07 Weight:  [ 1.00004804] bias:  [-0.00034344]\n",
      "step:  3682 loss:  2.42542e-07 Weight:  [ 1.0000478] bias:  [-0.00034206]\n",
      "step:  3683 loss:  2.40229e-07 Weight:  [ 1.00004756] bias:  [-0.00034068]\n",
      "step:  3684 loss:  2.38879e-07 Weight:  [ 1.00004733] bias:  [-0.00033931]\n",
      "step:  3685 loss:  2.36482e-07 Weight:  [ 1.00004721] bias:  [-0.00033793]\n",
      "step:  3686 loss:  2.35006e-07 Weight:  [ 1.00004697] bias:  [-0.00033657]\n",
      "step:  3687 loss:  2.32711e-07 Weight:  [ 1.00004685] bias:  [-0.00033521]\n",
      "step:  3688 loss:  2.31103e-07 Weight:  [ 1.00004661] bias:  [-0.00033386]\n",
      "step:  3689 loss:  2.29288e-07 Weight:  [ 1.00004637] bias:  [-0.00033251]\n",
      "step:  3690 loss:  2.27077e-07 Weight:  [ 1.00004625] bias:  [-0.00033117]\n",
      "step:  3691 loss:  2.25525e-07 Weight:  [ 1.00004601] bias:  [-0.00032983]\n",
      "step:  3692 loss:  2.2337e-07 Weight:  [ 1.0000459] bias:  [-0.00032849]\n",
      "step:  3693 loss:  2.21954e-07 Weight:  [ 1.00004566] bias:  [-0.00032717]\n",
      "step:  3694 loss:  2.20209e-07 Weight:  [ 1.00004554] bias:  [-0.00032585]\n",
      "step:  3695 loss:  2.18302e-07 Weight:  [ 1.0000453] bias:  [-0.00032454]\n",
      "step:  3696 loss:  2.16714e-07 Weight:  [ 1.00004506] bias:  [-0.00032323]\n",
      "step:  3697 loss:  2.14603e-07 Weight:  [ 1.00004494] bias:  [-0.00032191]\n",
      "step:  3698 loss:  2.12616e-07 Weight:  [ 1.00004482] bias:  [-0.00032061]\n",
      "step:  3699 loss:  2.11407e-07 Weight:  [ 1.00004458] bias:  [-0.00031932]\n",
      "step:  3700 loss:  2.09677e-07 Weight:  [ 1.00004447] bias:  [-0.00031803]\n",
      "step:  3701 loss:  2.08127e-07 Weight:  [ 1.00004423] bias:  [-0.00031676]\n",
      "step:  3702 loss:  2.06434e-07 Weight:  [ 1.00004411] bias:  [-0.00031548]\n",
      "step:  3703 loss:  2.04331e-07 Weight:  [ 1.00004387] bias:  [-0.00031421]\n",
      "step:  3704 loss:  2.03223e-07 Weight:  [ 1.00004363] bias:  [-0.00031294]\n",
      "step:  3705 loss:  2.01573e-07 Weight:  [ 1.00004351] bias:  [-0.00031168]\n",
      "step:  3706 loss:  1.99548e-07 Weight:  [ 1.00004339] bias:  [-0.00031042]\n",
      "step:  3707 loss:  1.98411e-07 Weight:  [ 1.00004315] bias:  [-0.00030917]\n",
      "step:  3708 loss:  1.96469e-07 Weight:  [ 1.00004303] bias:  [-0.00030792]\n",
      "step:  3709 loss:  1.94806e-07 Weight:  [ 1.0000428] bias:  [-0.00030668]\n",
      "step:  3710 loss:  1.93258e-07 Weight:  [ 1.00004268] bias:  [-0.00030544]\n",
      "step:  3711 loss:  1.92059e-07 Weight:  [ 1.00004244] bias:  [-0.00030421]\n",
      "step:  3712 loss:  1.90182e-07 Weight:  [ 1.00004232] bias:  [-0.00030298]\n",
      "step:  3713 loss:  1.88675e-07 Weight:  [ 1.0000422] bias:  [-0.00030175]\n",
      "step:  3714 loss:  1.87287e-07 Weight:  [ 1.00004196] bias:  [-0.00030054]\n",
      "step:  3715 loss:  1.85783e-07 Weight:  [ 1.00004184] bias:  [-0.00029933]\n",
      "step:  3716 loss:  1.84232e-07 Weight:  [ 1.0000416] bias:  [-0.00029813]\n",
      "step:  3717 loss:  1.82413e-07 Weight:  [ 1.00004148] bias:  [-0.00029692]\n",
      "step:  3718 loss:  1.81427e-07 Weight:  [ 1.00004125] bias:  [-0.00029573]\n",
      "step:  3719 loss:  1.79604e-07 Weight:  [ 1.00004113] bias:  [-0.00029453]\n",
      "step:  3720 loss:  1.78094e-07 Weight:  [ 1.00004101] bias:  [-0.00029334]\n",
      "step:  3721 loss:  1.76667e-07 Weight:  [ 1.00004089] bias:  [-0.00029216]\n",
      "step:  3722 loss:  1.75761e-07 Weight:  [ 1.00004065] bias:  [-0.00029099]\n",
      "step:  3723 loss:  1.73979e-07 Weight:  [ 1.00004041] bias:  [-0.00028982]\n",
      "step:  3724 loss:  1.72738e-07 Weight:  [ 1.00004029] bias:  [-0.00028864]\n",
      "step:  3725 loss:  1.71012e-07 Weight:  [ 1.00004017] bias:  [-0.00028747]\n",
      "step:  3726 loss:  1.69962e-07 Weight:  [ 1.00003994] bias:  [-0.00028632]\n",
      "step:  3727 loss:  1.68761e-07 Weight:  [ 1.00003982] bias:  [-0.00028516]\n",
      "step:  3728 loss:  1.67299e-07 Weight:  [ 1.0000397] bias:  [-0.00028401]\n",
      "step:  3729 loss:  1.65651e-07 Weight:  [ 1.00003958] bias:  [-0.00028287]\n",
      "step:  3730 loss:  1.64354e-07 Weight:  [ 1.00003934] bias:  [-0.00028173]\n",
      "step:  3731 loss:  1.63247e-07 Weight:  [ 1.00003922] bias:  [-0.0002806]\n",
      "step:  3732 loss:  1.61837e-07 Weight:  [ 1.00003898] bias:  [-0.00027947]\n",
      "step:  3733 loss:  1.60667e-07 Weight:  [ 1.00003886] bias:  [-0.00027834]\n",
      "step:  3734 loss:  1.59317e-07 Weight:  [ 1.00003874] bias:  [-0.00027721]\n",
      "step:  3735 loss:  1.57796e-07 Weight:  [ 1.00003862] bias:  [-0.0002761]\n",
      "step:  3736 loss:  1.56441e-07 Weight:  [ 1.0000385] bias:  [-0.00027499]\n",
      "step:  3737 loss:  1.55664e-07 Weight:  [ 1.00003827] bias:  [-0.00027389]\n",
      "step:  3738 loss:  1.54196e-07 Weight:  [ 1.00003815] bias:  [-0.00027279]\n",
      "step:  3739 loss:  1.52972e-07 Weight:  [ 1.00003791] bias:  [-0.00027169]\n",
      "step:  3740 loss:  1.51897e-07 Weight:  [ 1.00003779] bias:  [-0.00027059]\n",
      "step:  3741 loss:  1.50354e-07 Weight:  [ 1.00003767] bias:  [-0.0002695]\n",
      "step:  3742 loss:  1.49164e-07 Weight:  [ 1.00003755] bias:  [-0.00026841]\n",
      "step:  3743 loss:  1.48403e-07 Weight:  [ 1.00003731] bias:  [-0.00026734]\n",
      "step:  3744 loss:  1.46914e-07 Weight:  [ 1.00003719] bias:  [-0.00026626]\n",
      "step:  3745 loss:  1.45849e-07 Weight:  [ 1.00003707] bias:  [-0.00026519]\n",
      "step:  3746 loss:  1.44705e-07 Weight:  [ 1.00003684] bias:  [-0.00026412]\n",
      "step:  3747 loss:  1.43367e-07 Weight:  [ 1.00003672] bias:  [-0.00026305]\n",
      "step:  3748 loss:  1.42103e-07 Weight:  [ 1.0000366] bias:  [-0.00026199]\n",
      "step:  3749 loss:  1.41054e-07 Weight:  [ 1.00003648] bias:  [-0.00026093]\n",
      "step:  3750 loss:  1.3964e-07 Weight:  [ 1.00003636] bias:  [-0.00025988]\n",
      "step:  3751 loss:  1.38534e-07 Weight:  [ 1.00003624] bias:  [-0.00025883]\n",
      "step:  3752 loss:  1.3793e-07 Weight:  [ 1.000036] bias:  [-0.0002578]\n",
      "step:  3753 loss:  1.36877e-07 Weight:  [ 1.00003588] bias:  [-0.00025676]\n",
      "step:  3754 loss:  1.35564e-07 Weight:  [ 1.00003564] bias:  [-0.00025573]\n",
      "step:  3755 loss:  1.34564e-07 Weight:  [ 1.00003552] bias:  [-0.00025469]\n",
      "step:  3756 loss:  1.33225e-07 Weight:  [ 1.00003541] bias:  [-0.00025366]\n",
      "step:  3757 loss:  1.32171e-07 Weight:  [ 1.00003529] bias:  [-0.00025264]\n",
      "step:  3758 loss:  1.3123e-07 Weight:  [ 1.00003517] bias:  [-0.00025161]\n",
      "step:  3759 loss:  1.30188e-07 Weight:  [ 1.00003505] bias:  [-0.0002506]\n",
      "step:  3760 loss:  1.29147e-07 Weight:  [ 1.00003493] bias:  [-0.00024959]\n",
      "step:  3761 loss:  1.27915e-07 Weight:  [ 1.00003469] bias:  [-0.00024859]\n",
      "step:  3762 loss:  1.26994e-07 Weight:  [ 1.00003457] bias:  [-0.00024759]\n",
      "step:  3763 loss:  1.25694e-07 Weight:  [ 1.00003445] bias:  [-0.00024658]\n",
      "step:  3764 loss:  1.2467e-07 Weight:  [ 1.00003433] bias:  [-0.00024559]\n",
      "step:  3765 loss:  1.23798e-07 Weight:  [ 1.00003421] bias:  [-0.0002446]\n",
      "step:  3766 loss:  1.23318e-07 Weight:  [ 1.00003397] bias:  [-0.00024362]\n",
      "step:  3767 loss:  1.22147e-07 Weight:  [ 1.00003386] bias:  [-0.00024264]\n",
      "step:  3768 loss:  1.21268e-07 Weight:  [ 1.00003374] bias:  [-0.00024166]\n",
      "step:  3769 loss:  1.2029e-07 Weight:  [ 1.00003362] bias:  [-0.00024068]\n",
      "step:  3770 loss:  1.1905e-07 Weight:  [ 1.0000335] bias:  [-0.00023971]\n",
      "step:  3771 loss:  1.1812e-07 Weight:  [ 1.00003338] bias:  [-0.00023875]\n",
      "step:  3772 loss:  1.17297e-07 Weight:  [ 1.00003326] bias:  [-0.00023779]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step:  3773 loss:  1.16453e-07 Weight:  [ 1.00003302] bias:  [-0.00023684]\n",
      "step:  3774 loss:  1.15341e-07 Weight:  [ 1.0000329] bias:  [-0.00023588]\n",
      "step:  3775 loss:  1.14411e-07 Weight:  [ 1.00003278] bias:  [-0.00023492]\n",
      "step:  3776 loss:  1.13601e-07 Weight:  [ 1.00003266] bias:  [-0.00023397]\n",
      "step:  3777 loss:  1.1251e-07 Weight:  [ 1.00003254] bias:  [-0.00023303]\n",
      "step:  3778 loss:  1.11629e-07 Weight:  [ 1.00003242] bias:  [-0.00023209]\n",
      "step:  3779 loss:  1.10829e-07 Weight:  [ 1.00003231] bias:  [-0.00023116]\n",
      "step:  3780 loss:  1.10145e-07 Weight:  [ 1.00003219] bias:  [-0.00023023]\n",
      "step:  3781 loss:  1.09275e-07 Weight:  [ 1.00003195] bias:  [-0.00022931]\n",
      "step:  3782 loss:  1.08336e-07 Weight:  [ 1.00003183] bias:  [-0.00022838]\n",
      "step:  3783 loss:  1.07459e-07 Weight:  [ 1.00003171] bias:  [-0.00022745]\n",
      "step:  3784 loss:  1.06398e-07 Weight:  [ 1.00003159] bias:  [-0.00022653]\n",
      "step:  3785 loss:  1.05211e-07 Weight:  [ 1.00003147] bias:  [-0.00022561]\n",
      "step:  3786 loss:  1.04471e-07 Weight:  [ 1.00003135] bias:  [-0.00022469]\n",
      "step:  3787 loss:  1.03719e-07 Weight:  [ 1.00003123] bias:  [-0.00022378]\n",
      "step:  3788 loss:  1.0296e-07 Weight:  [ 1.00003111] bias:  [-0.00022288]\n",
      "step:  3789 loss:  1.01983e-07 Weight:  [ 1.00003099] bias:  [-0.00022198]\n",
      "step:  3790 loss:  1.01171e-07 Weight:  [ 1.00003088] bias:  [-0.00022108]\n",
      "step:  3791 loss:  1.00469e-07 Weight:  [ 1.00003076] bias:  [-0.00022018]\n",
      "step:  3792 loss:  9.95732e-08 Weight:  [ 1.00003064] bias:  [-0.00021929]\n",
      "step:  3793 loss:  9.87824e-08 Weight:  [ 1.00003052] bias:  [-0.00021841]\n",
      "step:  3794 loss:  9.80517e-08 Weight:  [ 1.0000304] bias:  [-0.00021753]\n",
      "step:  3795 loss:  9.73839e-08 Weight:  [ 1.00003028] bias:  [-0.00021665]\n",
      "step:  3796 loss:  9.66713e-08 Weight:  [ 1.00003016] bias:  [-0.00021578]\n",
      "step:  3797 loss:  9.57623e-08 Weight:  [ 1.00003004] bias:  [-0.00021492]\n",
      "step:  3798 loss:  9.51027e-08 Weight:  [ 1.00002992] bias:  [-0.00021406]\n",
      "step:  3799 loss:  9.44216e-08 Weight:  [ 1.0000298] bias:  [-0.0002132]\n",
      "step:  3800 loss:  9.31574e-08 Weight:  [ 1.00002968] bias:  [-0.00021234]\n",
      "step:  3801 loss:  9.24961e-08 Weight:  [ 1.00002956] bias:  [-0.00021148]\n",
      "step:  3802 loss:  9.18837e-08 Weight:  [ 1.00002944] bias:  [-0.00021063]\n",
      "step:  3803 loss:  9.12372e-08 Weight:  [ 1.00002933] bias:  [-0.00020978]\n",
      "step:  3804 loss:  9.06049e-08 Weight:  [ 1.00002921] bias:  [-0.00020894]\n",
      "step:  3805 loss:  8.96886e-08 Weight:  [ 1.00002909] bias:  [-0.0002081]\n",
      "step:  3806 loss:  8.90487e-08 Weight:  [ 1.00002897] bias:  [-0.00020726]\n",
      "step:  3807 loss:  8.84487e-08 Weight:  [ 1.00002885] bias:  [-0.00020643]\n",
      "step:  3808 loss:  8.76647e-08 Weight:  [ 1.00002873] bias:  [-0.0002056]\n",
      "step:  3809 loss:  8.66421e-08 Weight:  [ 1.00002861] bias:  [-0.00020477]\n",
      "step:  3810 loss:  8.61134e-08 Weight:  [ 1.00002849] bias:  [-0.00020394]\n",
      "step:  3811 loss:  8.5544e-08 Weight:  [ 1.00002837] bias:  [-0.00020312]\n",
      "step:  3812 loss:  8.49315e-08 Weight:  [ 1.00002825] bias:  [-0.0002023]\n",
      "step:  3813 loss:  8.41636e-08 Weight:  [ 1.00002813] bias:  [-0.00020149]\n",
      "step:  3814 loss:  8.35803e-08 Weight:  [ 1.00002801] bias:  [-0.00020068]\n",
      "step:  3815 loss:  8.29834e-08 Weight:  [ 1.00002789] bias:  [-0.00019987]\n",
      "step:  3816 loss:  8.1869e-08 Weight:  [ 1.00002778] bias:  [-0.00019906]\n",
      "step:  3817 loss:  8.12893e-08 Weight:  [ 1.00002766] bias:  [-0.00019825]\n",
      "step:  3818 loss:  8.0812e-08 Weight:  [ 1.00002754] bias:  [-0.00019745]\n",
      "step:  3819 loss:  8.02255e-08 Weight:  [ 1.00002742] bias:  [-0.00019665]\n",
      "step:  3820 loss:  7.96658e-08 Weight:  [ 1.0000273] bias:  [-0.00019585]\n",
      "step:  3821 loss:  7.88813e-08 Weight:  [ 1.00002718] bias:  [-0.00019506]\n",
      "step:  3822 loss:  7.8019e-08 Weight:  [ 1.00002718] bias:  [-0.00019426]\n",
      "step:  3823 loss:  7.74686e-08 Weight:  [ 1.00002706] bias:  [-0.00019348]\n",
      "step:  3824 loss:  7.70226e-08 Weight:  [ 1.00002694] bias:  [-0.0001927]\n",
      "step:  3825 loss:  7.62173e-08 Weight:  [ 1.00002682] bias:  [-0.00019193]\n",
      "step:  3826 loss:  7.56921e-08 Weight:  [ 1.0000267] bias:  [-0.00019116]\n",
      "step:  3827 loss:  7.52843e-08 Weight:  [ 1.00002658] bias:  [-0.00019039]\n",
      "step:  3828 loss:  7.4531e-08 Weight:  [ 1.00002646] bias:  [-0.00018962]\n",
      "step:  3829 loss:  7.39522e-08 Weight:  [ 1.00002635] bias:  [-0.00018885]\n",
      "step:  3830 loss:  7.3255e-08 Weight:  [ 1.00002623] bias:  [-0.00018809]\n",
      "step:  3831 loss:  7.28401e-08 Weight:  [ 1.00002611] bias:  [-0.00018733]\n",
      "step:  3832 loss:  7.23169e-08 Weight:  [ 1.00002599] bias:  [-0.00018657]\n",
      "step:  3833 loss:  7.13892e-08 Weight:  [ 1.00002599] bias:  [-0.0001858]\n",
      "step:  3834 loss:  7.08229e-08 Weight:  [ 1.00002587] bias:  [-0.00018506]\n",
      "step:  3835 loss:  7.03037e-08 Weight:  [ 1.00002575] bias:  [-0.00018431]\n",
      "step:  3836 loss:  6.99108e-08 Weight:  [ 1.00002563] bias:  [-0.00018357]\n",
      "step:  3837 loss:  6.9506e-08 Weight:  [ 1.00002551] bias:  [-0.00018283]\n",
      "step:  3838 loss:  6.87346e-08 Weight:  [ 1.00002539] bias:  [-0.00018209]\n",
      "step:  3839 loss:  6.81449e-08 Weight:  [ 1.00002527] bias:  [-0.00018135]\n",
      "step:  3840 loss:  6.76252e-08 Weight:  [ 1.00002515] bias:  [-0.00018062]\n",
      "step:  3841 loss:  6.72407e-08 Weight:  [ 1.00002515] bias:  [-0.00017988]\n",
      "step:  3842 loss:  6.64164e-08 Weight:  [ 1.00002503] bias:  [-0.00017916]\n",
      "step:  3843 loss:  6.58364e-08 Weight:  [ 1.00002491] bias:  [-0.00017843]\n",
      "step:  3844 loss:  6.53661e-08 Weight:  [ 1.0000248] bias:  [-0.00017771]\n",
      "step:  3845 loss:  6.50056e-08 Weight:  [ 1.00002468] bias:  [-0.000177]\n",
      "step:  3846 loss:  6.43559e-08 Weight:  [ 1.00002468] bias:  [-0.00017628]\n",
      "step:  3847 loss:  6.39079e-08 Weight:  [ 1.00002456] bias:  [-0.00017557]\n",
      "step:  3848 loss:  6.34635e-08 Weight:  [ 1.00002444] bias:  [-0.00017487]\n",
      "step:  3849 loss:  6.28961e-08 Weight:  [ 1.00002432] bias:  [-0.00017417]\n",
      "step:  3850 loss:  6.23398e-08 Weight:  [ 1.0000242] bias:  [-0.00017346]\n",
      "step:  3851 loss:  6.19034e-08 Weight:  [ 1.00002408] bias:  [-0.00017276]\n",
      "step:  3852 loss:  6.13131e-08 Weight:  [ 1.00002396] bias:  [-0.00017206]\n",
      "step:  3853 loss:  6.0907e-08 Weight:  [ 1.00002384] bias:  [-0.00017136]\n",
      "step:  3854 loss:  6.02804e-08 Weight:  [ 1.00002384] bias:  [-0.00017066]\n",
      "step:  3855 loss:  5.989e-08 Weight:  [ 1.00002372] bias:  [-0.00016997]\n",
      "step:  3856 loss:  5.95326e-08 Weight:  [ 1.0000236] bias:  [-0.00016928]\n",
      "step:  3857 loss:  5.88676e-08 Weight:  [ 1.0000236] bias:  [-0.00016859]\n",
      "step:  3858 loss:  5.84743e-08 Weight:  [ 1.00002348] bias:  [-0.00016792]\n",
      "step:  3859 loss:  5.79294e-08 Weight:  [ 1.00002337] bias:  [-0.00016725]\n",
      "step:  3860 loss:  5.75244e-08 Weight:  [ 1.00002325] bias:  [-0.00016657]\n",
      "step:  3861 loss:  5.7011e-08 Weight:  [ 1.00002325] bias:  [-0.0001659]\n",
      "step:  3862 loss:  5.66361e-08 Weight:  [ 1.00002313] bias:  [-0.00016523]\n",
      "step:  3863 loss:  5.61168e-08 Weight:  [ 1.00002301] bias:  [-0.00016457]\n",
      "step:  3864 loss:  5.54858e-08 Weight:  [ 1.00002289] bias:  [-0.00016391]\n",
      "step:  3865 loss:  5.51701e-08 Weight:  [ 1.00002277] bias:  [-0.00016325]\n",
      "step:  3866 loss:  5.49066e-08 Weight:  [ 1.00002265] bias:  [-0.00016259]\n",
      "step:  3867 loss:  5.45512e-08 Weight:  [ 1.00002253] bias:  [-0.00016193]\n",
      "step:  3868 loss:  5.3807e-08 Weight:  [ 1.00002253] bias:  [-0.00016126]\n",
      "step:  3869 loss:  5.34276e-08 Weight:  [ 1.00002241] bias:  [-0.00016062]\n",
      "step:  3870 loss:  5.31966e-08 Weight:  [ 1.00002229] bias:  [-0.00015997]\n",
      "step:  3871 loss:  5.25747e-08 Weight:  [ 1.00002229] bias:  [-0.00015932]\n",
      "step:  3872 loss:  5.2202e-08 Weight:  [ 1.00002217] bias:  [-0.00015868]\n",
      "step:  3873 loss:  5.18083e-08 Weight:  [ 1.00002205] bias:  [-0.00015804]\n",
      "step:  3874 loss:  5.12031e-08 Weight:  [ 1.00002193] bias:  [-0.0001574]\n",
      "step:  3875 loss:  5.0916e-08 Weight:  [ 1.00002193] bias:  [-0.00015676]\n",
      "step:  3876 loss:  5.06548e-08 Weight:  [ 1.00002182] bias:  [-0.00015613]\n",
      "step:  3877 loss:  5.01376e-08 Weight:  [ 1.0000217] bias:  [-0.0001555]\n",
      "step:  3878 loss:  4.97709e-08 Weight:  [ 1.00002158] bias:  [-0.00015488]\n",
      "step:  3879 loss:  4.93872e-08 Weight:  [ 1.00002146] bias:  [-0.00015425]\n",
      "step:  3880 loss:  4.88067e-08 Weight:  [ 1.00002146] bias:  [-0.00015362]\n",
      "step:  3881 loss:  4.84707e-08 Weight:  [ 1.00002134] bias:  [-0.000153]\n",
      "step:  3882 loss:  4.82668e-08 Weight:  [ 1.00002122] bias:  [-0.00015238]\n",
      "step:  3883 loss:  4.75935e-08 Weight:  [ 1.00002122] bias:  [-0.00015176]\n",
      "step:  3884 loss:  4.73258e-08 Weight:  [ 1.0000211] bias:  [-0.00015115]\n",
      "step:  3885 loss:  4.70106e-08 Weight:  [ 1.00002098] bias:  [-0.00015054]\n",
      "step:  3886 loss:  4.65413e-08 Weight:  [ 1.00002098] bias:  [-0.00014992]\n",
      "step:  3887 loss:  4.62645e-08 Weight:  [ 1.00002086] bias:  [-0.00014933]\n",
      "step:  3888 loss:  4.57967e-08 Weight:  [ 1.00002074] bias:  [-0.00014872]\n",
      "step:  3889 loss:  4.55596e-08 Weight:  [ 1.00002062] bias:  [-0.00014812]\n",
      "step:  3890 loss:  4.51357e-08 Weight:  [ 1.00002062] bias:  [-0.00014752]\n",
      "step:  3891 loss:  4.46356e-08 Weight:  [ 1.0000205] bias:  [-0.00014693]\n",
      "step:  3892 loss:  4.43927e-08 Weight:  [ 1.00002038] bias:  [-0.00014633]\n",
      "step:  3893 loss:  4.41765e-08 Weight:  [ 1.00002038] bias:  [-0.00014574]\n",
      "step:  3894 loss:  4.36609e-08 Weight:  [ 1.00002027] bias:  [-0.00014515]\n",
      "step:  3895 loss:  4.32908e-08 Weight:  [ 1.00002015] bias:  [-0.00014457]\n",
      "step:  3896 loss:  4.27793e-08 Weight:  [ 1.00002015] bias:  [-0.00014398]\n",
      "step:  3897 loss:  4.25667e-08 Weight:  [ 1.00002003] bias:  [-0.0001434]\n",
      "step:  3898 loss:  4.23298e-08 Weight:  [ 1.00001991] bias:  [-0.00014282]\n",
      "step:  3899 loss:  4.19544e-08 Weight:  [ 1.00001991] bias:  [-0.00014224]\n",
      "step:  3900 loss:  4.17059e-08 Weight:  [ 1.00001979] bias:  [-0.00014167]\n",
      "step:  3901 loss:  4.12152e-08 Weight:  [ 1.00001967] bias:  [-0.0001411]\n",
      "step:  3902 loss:  4.0905e-08 Weight:  [ 1.00001967] bias:  [-0.00014053]\n",
      "step:  3903 loss:  4.06321e-08 Weight:  [ 1.00001955] bias:  [-0.00013997]\n",
      "step:  3904 loss:  4.01928e-08 Weight:  [ 1.00001955] bias:  [-0.0001394]\n",
      "step:  3905 loss:  3.99664e-08 Weight:  [ 1.00001943] bias:  [-0.00013885]\n",
      "step:  3906 loss:  3.95541e-08 Weight:  [ 1.00001931] bias:  [-0.00013829]\n",
      "step:  3907 loss:  3.92403e-08 Weight:  [ 1.00001919] bias:  [-0.00013773]\n",
      "step:  3908 loss:  3.90347e-08 Weight:  [ 1.00001907] bias:  [-0.00013717]\n",
      "step:  3909 loss:  3.86332e-08 Weight:  [ 1.00001907] bias:  [-0.00013661]\n",
      "step:  3910 loss:  3.84055e-08 Weight:  [ 1.00001895] bias:  [-0.00013606]\n",
      "step:  3911 loss:  3.80622e-08 Weight:  [ 1.00001895] bias:  [-0.0001355]\n",
      "step:  3912 loss:  3.78058e-08 Weight:  [ 1.00001884] bias:  [-0.00013496]\n",
      "step:  3913 loss:  3.74054e-08 Weight:  [ 1.00001884] bias:  [-0.00013441]\n",
      "step:  3914 loss:  3.72329e-08 Weight:  [ 1.00001872] bias:  [-0.00013388]\n",
      "step:  3915 loss:  3.69276e-08 Weight:  [ 1.0000186] bias:  [-0.00013334]\n",
      "step:  3916 loss:  3.65469e-08 Weight:  [ 1.0000186] bias:  [-0.0001328]\n",
      "step:  3917 loss:  3.63539e-08 Weight:  [ 1.00001848] bias:  [-0.00013227]\n",
      "step:  3918 loss:  3.59513e-08 Weight:  [ 1.00001848] bias:  [-0.00013173]\n",
      "step:  3919 loss:  3.57603e-08 Weight:  [ 1.00001836] bias:  [-0.00013121]\n",
      "step:  3920 loss:  3.52258e-08 Weight:  [ 1.00001824] bias:  [-0.00013068]\n",
      "step:  3921 loss:  3.50306e-08 Weight:  [ 1.00001812] bias:  [-0.00013015]\n",
      "step:  3922 loss:  3.49287e-08 Weight:  [ 1.00001812] bias:  [-0.00012963]\n",
      "step:  3923 loss:  3.45471e-08 Weight:  [ 1.000018] bias:  [-0.0001291]\n",
      "step:  3924 loss:  3.44329e-08 Weight:  [ 1.00001788] bias:  [-0.00012858]\n",
      "step:  3925 loss:  3.40123e-08 Weight:  [ 1.00001788] bias:  [-0.00012805]\n",
      "step:  3926 loss:  3.38229e-08 Weight:  [ 1.00001776] bias:  [-0.00012754]\n",
      "step:  3927 loss:  3.33583e-08 Weight:  [ 1.00001776] bias:  [-0.00012702]\n",
      "step:  3928 loss:  3.31938e-08 Weight:  [ 1.00001764] bias:  [-0.00012651]\n",
      "step:  3929 loss:  3.28248e-08 Weight:  [ 1.00001764] bias:  [-0.000126]\n",
      "step:  3930 loss:  3.25998e-08 Weight:  [ 1.00001752] bias:  [-0.00012549]\n",
      "step:  3931 loss:  3.23036e-08 Weight:  [ 1.00001752] bias:  [-0.00012499]\n",
      "step:  3932 loss:  3.21343e-08 Weight:  [ 1.0000174] bias:  [-0.00012449]\n",
      "step:  3933 loss:  3.17019e-08 Weight:  [ 1.0000174] bias:  [-0.00012398]\n",
      "step:  3934 loss:  3.15432e-08 Weight:  [ 1.00001729] bias:  [-0.00012349]\n",
      "step:  3935 loss:  3.13925e-08 Weight:  [ 1.00001717] bias:  [-0.000123]\n",
      "step:  3936 loss:  3.11129e-08 Weight:  [ 1.00001717] bias:  [-0.0001225]\n",
      "step:  3937 loss:  3.09571e-08 Weight:  [ 1.00001705] bias:  [-0.00012202]\n",
      "step:  3938 loss:  3.06785e-08 Weight:  [ 1.00001693] bias:  [-0.00012152]\n",
      "step:  3939 loss:  3.05098e-08 Weight:  [ 1.00001681] bias:  [-0.00012103]\n",
      "step:  3940 loss:  3.00923e-08 Weight:  [ 1.00001681] bias:  [-0.00012053]\n",
      "step:  3941 loss:  2.99331e-08 Weight:  [ 1.00001669] bias:  [-0.00012005]\n",
      "step:  3942 loss:  2.9664e-08 Weight:  [ 1.00001669] bias:  [-0.00011956]\n",
      "step:  3943 loss:  2.94613e-08 Weight:  [ 1.00001657] bias:  [-0.00011908]\n",
      "step:  3944 loss:  2.91358e-08 Weight:  [ 1.00001657] bias:  [-0.00011859]\n",
      "step:  3945 loss:  2.89924e-08 Weight:  [ 1.00001645] bias:  [-0.00011811]\n",
      "step:  3946 loss:  2.85841e-08 Weight:  [ 1.00001645] bias:  [-0.00011763]\n",
      "step:  3947 loss:  2.84304e-08 Weight:  [ 1.00001633] bias:  [-0.00011716]\n",
      "step:  3948 loss:  2.8128e-08 Weight:  [ 1.00001633] bias:  [-0.00011668]\n",
      "step:  3949 loss:  2.79894e-08 Weight:  [ 1.00001621] bias:  [-0.00011621]\n",
      "step:  3950 loss:  2.77263e-08 Weight:  [ 1.00001621] bias:  [-0.00011574]\n",
      "step:  3951 loss:  2.75901e-08 Weight:  [ 1.00001609] bias:  [-0.00011528]\n",
      "step:  3952 loss:  2.73842e-08 Weight:  [ 1.00001609] bias:  [-0.00011481]\n",
      "step:  3953 loss:  2.725e-08 Weight:  [ 1.00001597] bias:  [-0.00011436]\n",
      "step:  3954 loss:  2.69507e-08 Weight:  [ 1.00001585] bias:  [-0.0001139]\n",
      "step:  3955 loss:  2.67232e-08 Weight:  [ 1.00001585] bias:  [-0.00011344]\n",
      "step:  3956 loss:  2.64403e-08 Weight:  [ 1.00001574] bias:  [-0.00011298]\n",
      "step:  3957 loss:  2.62962e-08 Weight:  [ 1.00001574] bias:  [-0.00011252]\n",
      "step:  3958 loss:  2.60077e-08 Weight:  [ 1.00001562] bias:  [-0.00011207]\n",
      "step:  3959 loss:  2.57572e-08 Weight:  [ 1.00001562] bias:  [-0.00011161]\n",
      "step:  3960 loss:  2.56222e-08 Weight:  [ 1.0000155] bias:  [-0.00011116]\n",
      "step:  3961 loss:  2.52504e-08 Weight:  [ 1.0000155] bias:  [-0.00011071]\n",
      "step:  3962 loss:  2.51361e-08 Weight:  [ 1.00001538] bias:  [-0.00011026]\n",
      "step:  3963 loss:  2.48521e-08 Weight:  [ 1.00001538] bias:  [-0.00010982]\n",
      "step:  3964 loss:  2.4775e-08 Weight:  [ 1.00001526] bias:  [-0.00010938]\n",
      "step:  3965 loss:  2.45576e-08 Weight:  [ 1.00001526] bias:  [-0.00010893]\n",
      "step:  3966 loss:  2.44402e-08 Weight:  [ 1.00001514] bias:  [-0.0001085]\n",
      "step:  3967 loss:  2.42132e-08 Weight:  [ 1.00001514] bias:  [-0.00010806]\n",
      "step:  3968 loss:  2.4097e-08 Weight:  [ 1.00001502] bias:  [-0.00010763]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step:  3969 loss:  2.38349e-08 Weight:  [ 1.00001502] bias:  [-0.00010719]\n",
      "step:  3970 loss:  2.37626e-08 Weight:  [ 1.0000149] bias:  [-0.00010677]\n",
      "step:  3971 loss:  2.34178e-08 Weight:  [ 1.0000149] bias:  [-0.00010634]\n",
      "step:  3972 loss:  2.31463e-08 Weight:  [ 1.00001478] bias:  [-0.00010591]\n",
      "step:  3973 loss:  2.30736e-08 Weight:  [ 1.00001466] bias:  [-0.00010549]\n",
      "step:  3974 loss:  2.28166e-08 Weight:  [ 1.00001466] bias:  [-0.00010505]\n",
      "step:  3975 loss:  2.2739e-08 Weight:  [ 1.00001466] bias:  [-0.00010463]\n",
      "step:  3976 loss:  2.24893e-08 Weight:  [ 1.00001454] bias:  [-0.00010421]\n",
      "step:  3977 loss:  2.231e-08 Weight:  [ 1.00001454] bias:  [-0.00010379]\n",
      "step:  3978 loss:  2.20624e-08 Weight:  [ 1.00001442] bias:  [-0.00010337]\n",
      "step:  3979 loss:  2.19976e-08 Weight:  [ 1.00001431] bias:  [-0.00010295]\n",
      "step:  3980 loss:  2.17955e-08 Weight:  [ 1.00001431] bias:  [-0.00010253]\n",
      "step:  3981 loss:  2.15318e-08 Weight:  [ 1.00001431] bias:  [-0.00010211]\n",
      "step:  3982 loss:  2.14384e-08 Weight:  [ 1.00001419] bias:  [-0.0001017]\n",
      "step:  3983 loss:  2.12685e-08 Weight:  [ 1.00001419] bias:  [-0.00010129]\n",
      "step:  3984 loss:  2.11765e-08 Weight:  [ 1.00001407] bias:  [-0.00010089]\n",
      "step:  3985 loss:  2.09425e-08 Weight:  [ 1.00001407] bias:  [-0.00010048]\n",
      "step:  3986 loss:  2.08522e-08 Weight:  [ 1.00001395] bias:  [-0.00010008]\n",
      "step:  3987 loss:  2.05791e-08 Weight:  [ 1.00001395] bias:  [ -9.96721137e-05]\n",
      "step:  3988 loss:  2.03241e-08 Weight:  [ 1.00001395] bias:  [ -9.92713321e-05]\n",
      "step:  3989 loss:  2.0237e-08 Weight:  [ 1.00001383] bias:  [ -9.88816391e-05]\n",
      "step:  3990 loss:  2.00304e-08 Weight:  [ 1.00001383] bias:  [ -9.84824073e-05]\n",
      "step:  3991 loss:  1.99593e-08 Weight:  [ 1.00001371] bias:  [ -9.80923578e-05]\n",
      "step:  3992 loss:  1.9773e-08 Weight:  [ 1.00001371] bias:  [ -9.76953888e-05]\n",
      "step:  3993 loss:  1.97338e-08 Weight:  [ 1.00001359] bias:  [ -9.73085553e-05]\n",
      "step:  3994 loss:  1.94133e-08 Weight:  [ 1.00001359] bias:  [ -9.69155226e-05]\n",
      "step:  3995 loss:  1.91828e-08 Weight:  [ 1.00001347] bias:  [ -9.65278523e-05]\n",
      "step:  3996 loss:  1.91385e-08 Weight:  [ 1.00001335] bias:  [ -9.61395854e-05]\n",
      "step:  3997 loss:  1.89612e-08 Weight:  [ 1.00001335] bias:  [ -9.57427401e-05]\n",
      "step:  3998 loss:  1.89173e-08 Weight:  [ 1.00001335] bias:  [ -9.53560229e-05]\n",
      "step:  3999 loss:  1.86992e-08 Weight:  [ 1.00001323] bias:  [ -9.49745518e-05]\n",
      "step:  4000 loss:  1.85117e-08 Weight:  [ 1.00001323] bias:  [ -9.45862848e-05]\n",
      "step:  4001 loss:  1.84643e-08 Weight:  [ 1.00001311] bias:  [ -9.42082697e-05]\n",
      "step:  4002 loss:  1.82633e-08 Weight:  [ 1.00001311] bias:  [ -9.38228695e-05]\n",
      "step:  4003 loss:  1.82233e-08 Weight:  [ 1.00001299] bias:  [ -9.34475975e-05]\n",
      "step:  4004 loss:  1.79336e-08 Weight:  [ 1.00001299] bias:  [ -9.30658862e-05]\n",
      "step:  4005 loss:  1.77112e-08 Weight:  [ 1.00001299] bias:  [ -9.26895445e-05]\n",
      "step:  4006 loss:  1.76829e-08 Weight:  [ 1.00001287] bias:  [ -9.23232146e-05]\n",
      "step:  4007 loss:  1.75077e-08 Weight:  [ 1.00001287] bias:  [ -9.19482991e-05]\n",
      "step:  4008 loss:  1.74796e-08 Weight:  [ 1.00001276] bias:  [ -9.15834025e-05]\n",
      "step:  4009 loss:  1.72606e-08 Weight:  [ 1.00001276] bias:  [ -9.12096802e-05]\n",
      "step:  4010 loss:  1.70869e-08 Weight:  [ 1.00001276] bias:  [ -9.08403672e-05]\n",
      "step:  4011 loss:  1.70309e-08 Weight:  [ 1.00001264] bias:  [ -9.04801200e-05]\n",
      "step:  4012 loss:  1.67935e-08 Weight:  [ 1.00001264] bias:  [ -9.01124804e-05]\n",
      "step:  4013 loss:  1.67216e-08 Weight:  [ 1.00001252] bias:  [ -8.97558057e-05]\n",
      "step:  4014 loss:  1.65123e-08 Weight:  [ 1.00001252] bias:  [ -8.93936449e-05]\n",
      "step:  4015 loss:  1.63525e-08 Weight:  [ 1.0000124] bias:  [ -8.90357769e-05]\n",
      "step:  4016 loss:  1.63315e-08 Weight:  [ 1.0000124] bias:  [ -8.86748094e-05]\n",
      "step:  4017 loss:  1.61717e-08 Weight:  [ 1.0000124] bias:  [ -8.83181347e-05]\n",
      "step:  4018 loss:  1.59708e-08 Weight:  [ 1.00001228] bias:  [ -8.79667059e-05]\n",
      "step:  4019 loss:  1.59773e-08 Weight:  [ 1.00001216] bias:  [ -8.76131307e-05]\n",
      "step:  4020 loss:  1.5792e-08 Weight:  [ 1.00001216] bias:  [ -8.72521632e-05]\n",
      "step:  4021 loss:  1.57621e-08 Weight:  [ 1.00001204] bias:  [ -8.69012147e-05]\n",
      "step:  4022 loss:  1.54944e-08 Weight:  [ 1.00001204] bias:  [ -8.65438269e-05]\n",
      "step:  4023 loss:  1.53383e-08 Weight:  [ 1.00001204] bias:  [ -8.61907320e-05]\n",
      "step:  4024 loss:  1.53248e-08 Weight:  [ 1.00001192] bias:  [ -8.58457424e-05]\n",
      "step:  4025 loss:  1.51229e-08 Weight:  [ 1.00001192] bias:  [ -8.54919272e-05]\n",
      "step:  4026 loss:  1.49685e-08 Weight:  [ 1.00001192] bias:  [ -8.51424047e-05]\n",
      "step:  4027 loss:  1.49134e-08 Weight:  [ 1.0000118] bias:  [ -8.48019408e-05]\n",
      "step:  4028 loss:  1.48051e-08 Weight:  [ 1.0000118] bias:  [ -8.44534952e-05]\n",
      "step:  4029 loss:  1.46111e-08 Weight:  [ 1.0000118] bias:  [ -8.41102883e-05]\n",
      "step:  4030 loss:  1.45994e-08 Weight:  [ 1.00001168] bias:  [ -8.37768603e-05]\n",
      "step:  4031 loss:  1.43631e-08 Weight:  [ 1.00001168] bias:  [ -8.34349703e-05]\n",
      "step:  4032 loss:  1.4336e-08 Weight:  [ 1.00001156] bias:  [ -8.31030920e-05]\n",
      "step:  4033 loss:  1.41839e-08 Weight:  [ 1.00001156] bias:  [ -8.27647746e-05]\n",
      "step:  4034 loss:  1.40177e-08 Weight:  [ 1.00001156] bias:  [ -8.24296803e-05]\n",
      "step:  4035 loss:  1.39929e-08 Weight:  [ 1.00001144] bias:  [ -8.21045978e-05]\n",
      "step:  4036 loss:  1.38505e-08 Weight:  [ 1.00001144] bias:  [ -8.17723630e-05]\n",
      "step:  4037 loss:  1.37122e-08 Weight:  [ 1.00001144] bias:  [ -8.14442974e-05]\n",
      "step:  4038 loss:  1.37032e-08 Weight:  [ 1.00001132] bias:  [ -8.11243372e-05]\n",
      "step:  4039 loss:  1.35695e-08 Weight:  [ 1.00001132] bias:  [ -8.07971082e-05]\n",
      "step:  4040 loss:  1.34327e-08 Weight:  [ 1.00001121] bias:  [ -8.04740484e-05]\n",
      "step:  4041 loss:  1.34189e-08 Weight:  [ 1.00001121] bias:  [ -8.01483693e-05]\n",
      "step:  4042 loss:  1.32433e-08 Weight:  [ 1.00001109] bias:  [ -7.98278124e-05]\n",
      "step:  4043 loss:  1.30509e-08 Weight:  [ 1.00001109] bias:  [ -7.94996304e-05]\n",
      "step:  4044 loss:  1.30066e-08 Weight:  [ 1.00001109] bias:  [ -7.91803905e-05]\n",
      "step:  4045 loss:  1.28648e-08 Weight:  [ 1.00001097] bias:  [ -7.88654361e-05]\n",
      "step:  4046 loss:  1.27771e-08 Weight:  [ 1.00001097] bias:  [ -7.85422599e-05]\n",
      "step:  4047 loss:  1.27348e-08 Weight:  [ 1.00001085] bias:  [ -7.82280258e-05]\n",
      "step:  4048 loss:  1.25892e-08 Weight:  [ 1.00001085] bias:  [ -7.79040129e-05]\n",
      "step:  4049 loss:  1.24694e-08 Weight:  [ 1.00001085] bias:  [ -7.75839362e-05]\n",
      "step:  4050 loss:  1.24271e-08 Weight:  [ 1.00001073] bias:  [ -7.72728017e-05]\n",
      "step:  4051 loss:  1.22274e-08 Weight:  [ 1.00001073] bias:  [ -7.69542748e-05]\n",
      "step:  4052 loss:  1.21099e-08 Weight:  [ 1.00001073] bias:  [ -7.66396843e-05]\n",
      "step:  4053 loss:  1.20679e-08 Weight:  [ 1.00001061] bias:  [ -7.63340286e-05]\n",
      "step:  4054 loss:  1.19371e-08 Weight:  [ 1.00001061] bias:  [ -7.60218172e-05]\n",
      "step:  4055 loss:  1.18136e-08 Weight:  [ 1.00001061] bias:  [ -7.57136586e-05]\n",
      "step:  4056 loss:  1.18105e-08 Weight:  [ 1.00001049] bias:  [ -7.54134890e-05]\n",
      "step:  4057 loss:  1.16886e-08 Weight:  [ 1.00001049] bias:  [ -7.51060506e-05]\n",
      "step:  4058 loss:  1.15745e-08 Weight:  [ 1.00001049] bias:  [ -7.48008752e-05]\n",
      "step:  4059 loss:  1.15596e-08 Weight:  [ 1.00001037] bias:  [ -7.45055950e-05]\n",
      "step:  4060 loss:  1.1438e-08 Weight:  [ 1.00001037] bias:  [ -7.42030388e-05]\n",
      "step:  4061 loss:  1.13253e-08 Weight:  [ 1.00001037] bias:  [ -7.39027528e-05]\n",
      "step:  4062 loss:  1.13245e-08 Weight:  [ 1.00001025] bias:  [ -7.36121219e-05]\n",
      "step:  4063 loss:  1.12071e-08 Weight:  [ 1.00001025] bias:  [ -7.33146953e-05]\n",
      "step:  4064 loss:  1.10956e-08 Weight:  [ 1.00001025] bias:  [ -7.30195316e-05]\n",
      "step:  4065 loss:  1.09491e-08 Weight:  [ 1.00001013] bias:  [ -7.27292572e-05]\n",
      "step:  4066 loss:  1.08867e-08 Weight:  [ 1.00001013] bias:  [ -7.24352867e-05]\n",
      "step:  4067 loss:  1.07764e-08 Weight:  [ 1.00001013] bias:  [ -7.21452525e-05]\n",
      "step:  4068 loss:  1.06552e-08 Weight:  [ 1.00001001] bias:  [ -7.18593874e-05]\n",
      "step:  4069 loss:  1.06295e-08 Weight:  [ 1.00001001] bias:  [ -7.15680435e-05]\n",
      "step:  4070 loss:  1.05081e-08 Weight:  [ 1.00001001] bias:  [ -7.12808687e-05]\n",
      "step:  4071 loss:  1.04005e-08 Weight:  [ 1.00000989] bias:  [ -7.09976302e-05]\n",
      "step:  4072 loss:  1.04379e-08 Weight:  [ 1.00000989] bias:  [ -7.07099753e-05]\n",
      "step:  4073 loss:  1.03176e-08 Weight:  [ 1.00000989] bias:  [ -7.04264967e-05]\n",
      "step:  4074 loss:  1.01958e-08 Weight:  [ 1.00000978] bias:  [ -7.01458775e-05]\n",
      "step:  4075 loss:  1.00222e-08 Weight:  [ 1.00000978] bias:  [ -6.98577496e-05]\n",
      "step:  4076 loss:  1.00291e-08 Weight:  [ 1.00000978] bias:  [ -6.95773706e-05]\n",
      "step:  4077 loss:  9.92376e-09 Weight:  [ 1.00000966] bias:  [ -6.93009279e-05]\n",
      "step:  4078 loss:  9.81239e-09 Weight:  [ 1.00000966] bias:  [ -6.90178058e-05]\n",
      "step:  4079 loss:  9.7868e-09 Weight:  [ 1.00000954] bias:  [ -6.87433858e-05]\n",
      "step:  4080 loss:  9.71047e-09 Weight:  [ 1.00000954] bias:  [ -6.84607439e-05]\n",
      "step:  4081 loss:  9.61333e-09 Weight:  [ 1.00000954] bias:  [ -6.81802412e-05]\n",
      "step:  4082 loss:  9.61018e-09 Weight:  [ 1.00000942] bias:  [ -6.79094010e-05]\n",
      "step:  4083 loss:  9.53387e-09 Weight:  [ 1.00000942] bias:  [ -6.76303316e-05]\n",
      "step:  4084 loss:  9.43773e-09 Weight:  [ 1.00000942] bias:  [ -6.73534087e-05]\n",
      "step:  4085 loss:  9.33456e-09 Weight:  [ 1.00000942] bias:  [ -6.70804220e-05]\n",
      "step:  4086 loss:  9.3125e-09 Weight:  [ 1.0000093] bias:  [ -6.68161374e-05]\n",
      "step:  4087 loss:  9.20196e-09 Weight:  [ 1.0000093] bias:  [ -6.65451735e-05]\n",
      "step:  4088 loss:  9.11234e-09 Weight:  [ 1.0000093] bias:  [ -6.62762395e-05]\n",
      "step:  4089 loss:  9.11916e-09 Weight:  [ 1.00000918] bias:  [ -6.60168371e-05]\n",
      "step:  4090 loss:  8.96854e-09 Weight:  [ 1.00000918] bias:  [ -6.57495693e-05]\n",
      "step:  4091 loss:  8.87988e-09 Weight:  [ 1.00000918] bias:  [ -6.54843316e-05]\n",
      "step:  4092 loss:  8.7763e-09 Weight:  [ 1.00000918] bias:  [ -6.52231465e-05]\n",
      "step:  4093 loss:  8.78784e-09 Weight:  [ 1.00000906] bias:  [ -6.49697104e-05]\n",
      "step:  4094 loss:  8.71799e-09 Weight:  [ 1.00000906] bias:  [ -6.47079287e-05]\n",
      "step:  4095 loss:  8.60589e-09 Weight:  [ 1.00000906] bias:  [ -6.44490065e-05]\n",
      "step:  4096 loss:  8.50845e-09 Weight:  [ 1.00000894] bias:  [ -6.41940205e-05]\n",
      "step:  4097 loss:  8.50639e-09 Weight:  [ 1.00000894] bias:  [ -6.39328355e-05]\n",
      "step:  4098 loss:  8.41502e-09 Weight:  [ 1.00000894] bias:  [ -6.36754630e-05]\n",
      "step:  4099 loss:  8.32939e-09 Weight:  [ 1.00000894] bias:  [ -6.34201133e-05]\n",
      "step:  4100 loss:  8.23987e-09 Weight:  [ 1.00000882] bias:  [ -6.31685834e-05]\n",
      "step:  4101 loss:  8.19193e-09 Weight:  [ 1.00000882] bias:  [ -6.29149072e-05]\n",
      "step:  4102 loss:  8.11372e-09 Weight:  [ 1.00000882] bias:  [ -6.26631372e-05]\n",
      "step:  4103 loss:  8.01953e-09 Weight:  [ 1.0000087] bias:  [ -6.24153035e-05]\n",
      "step:  4104 loss:  8.03098e-09 Weight:  [ 1.0000087] bias:  [ -6.21643703e-05]\n",
      "step:  4105 loss:  7.93252e-09 Weight:  [ 1.00000858] bias:  [ -6.19174898e-05]\n",
      "step:  4106 loss:  7.88193e-09 Weight:  [ 1.00000858] bias:  [ -6.16603575e-05]\n",
      "step:  4107 loss:  7.78724e-09 Weight:  [ 1.00000858] bias:  [ -6.14071541e-05]\n",
      "step:  4108 loss:  7.80076e-09 Weight:  [ 1.00000858] bias:  [ -6.11615833e-05]\n",
      "step:  4109 loss:  7.71425e-09 Weight:  [ 1.00000846] bias:  [ -6.09198250e-05]\n",
      "step:  4110 loss:  7.66342e-09 Weight:  [ 1.00000846] bias:  [ -6.06678150e-05]\n",
      "step:  4111 loss:  7.5466e-09 Weight:  [ 1.00000846] bias:  [ -6.04205743e-05]\n",
      "step:  4112 loss:  7.56155e-09 Weight:  [ 1.00000834] bias:  [ -6.01809625e-05]\n",
      "step:  4113 loss:  7.4734e-09 Weight:  [ 1.00000834] bias:  [ -5.99344385e-05]\n",
      "step:  4114 loss:  7.39871e-09 Weight:  [ 1.00000834] bias:  [ -5.96898208e-05]\n",
      "step:  4115 loss:  7.30744e-09 Weight:  [ 1.00000834] bias:  [ -5.94491357e-05]\n",
      "step:  4116 loss:  7.32372e-09 Weight:  [ 1.00000823] bias:  [ -5.92160832e-05]\n",
      "step:  4117 loss:  7.1894e-09 Weight:  [ 1.00000823] bias:  [ -5.89751617e-05]\n",
      "step:  4118 loss:  7.11605e-09 Weight:  [ 1.00000823] bias:  [ -5.87361465e-05]\n",
      "step:  4119 loss:  7.03314e-09 Weight:  [ 1.00000823] bias:  [ -5.85009475e-05]\n",
      "step:  4120 loss:  7.04891e-09 Weight:  [ 1.00000811] bias:  [ -5.82733774e-05]\n",
      "step:  4121 loss:  6.96483e-09 Weight:  [ 1.00000811] bias:  [ -5.80354354e-05]\n",
      "step:  4122 loss:  6.89263e-09 Weight:  [ 1.00000811] bias:  [ -5.77993997e-05]\n",
      "step:  4123 loss:  6.81089e-09 Weight:  [ 1.00000811] bias:  [ -5.75671802e-05]\n",
      "step:  4124 loss:  6.82601e-09 Weight:  [ 1.00000799] bias:  [ -5.73425896e-05]\n",
      "step:  4125 loss:  6.79377e-09 Weight:  [ 1.00000799] bias:  [ -5.71087003e-05]\n",
      "step:  4126 loss:  6.72254e-09 Weight:  [ 1.00000799] bias:  [ -5.68767173e-05]\n",
      "step:  4127 loss:  6.64177e-09 Weight:  [ 1.00000799] bias:  [ -5.66485505e-05]\n",
      "step:  4128 loss:  6.65786e-09 Weight:  [ 1.00000787] bias:  [ -5.64280126e-05]\n",
      "step:  4129 loss:  6.53479e-09 Weight:  [ 1.00000787] bias:  [ -5.61994893e-05]\n",
      "step:  4130 loss:  6.46485e-09 Weight:  [ 1.00000787] bias:  [ -5.59728724e-05]\n",
      "step:  4131 loss:  6.39585e-09 Weight:  [ 1.00000787] bias:  [ -5.57498315e-05]\n",
      "step:  4132 loss:  6.41134e-09 Weight:  [ 1.00000775] bias:  [ -5.55344195e-05]\n",
      "step:  4133 loss:  6.33339e-09 Weight:  [ 1.00000775] bias:  [ -5.53119753e-05]\n",
      "step:  4134 loss:  6.26467e-09 Weight:  [ 1.00000775] bias:  [ -5.50914374e-05]\n",
      "step:  4135 loss:  6.1964e-09 Weight:  [ 1.00000775] bias:  [ -5.48728058e-05]\n",
      "step:  4136 loss:  6.12556e-09 Weight:  [ 1.00000763] bias:  [ -5.46578704e-05]\n",
      "step:  4137 loss:  6.16841e-09 Weight:  [ 1.00000763] bias:  [ -5.44381692e-05]\n",
      "step:  4138 loss:  6.09124e-09 Weight:  [ 1.00000763] bias:  [ -5.42222806e-05]\n",
      "step:  4139 loss:  6.02964e-09 Weight:  [ 1.00000751] bias:  [ -5.40081819e-05]\n",
      "step:  4140 loss:  6.00171e-09 Weight:  [ 1.00000751] bias:  [ -5.37847845e-05]\n",
      "step:  4141 loss:  6.01371e-09 Weight:  [ 1.00000751] bias:  [ -5.35690160e-05]\n",
      "step:  4142 loss:  5.94297e-09 Weight:  [ 1.00000739] bias:  [ -5.33569437e-05]\n",
      "step:  4143 loss:  5.88452e-09 Weight:  [ 1.00000739] bias:  [ -5.31358091e-05]\n",
      "step:  4144 loss:  5.80815e-09 Weight:  [ 1.00000739] bias:  [ -5.29184908e-05]\n",
      "step:  4145 loss:  5.82679e-09 Weight:  [ 1.00000727] bias:  [ -5.27086813e-05]\n",
      "step:  4146 loss:  5.71892e-09 Weight:  [ 1.00000727] bias:  [ -5.24890966e-05]\n",
      "step:  4147 loss:  5.64876e-09 Weight:  [ 1.00000727] bias:  [ -5.22732080e-05]\n",
      "step:  4148 loss:  5.58365e-09 Weight:  [ 1.00000727] bias:  [ -5.20592257e-05]\n",
      "step:  4149 loss:  5.51883e-09 Weight:  [ 1.00000727] bias:  [ -5.18488196e-05]\n",
      "step:  4150 loss:  5.53719e-09 Weight:  [ 1.00000715] bias:  [ -5.16459259e-05]\n",
      "step:  4151 loss:  5.50884e-09 Weight:  [ 1.00000715] bias:  [ -5.14337335e-05]\n",
      "step:  4152 loss:  5.44988e-09 Weight:  [ 1.00000715] bias:  [ -5.12233273e-05]\n",
      "step:  4153 loss:  5.38578e-09 Weight:  [ 1.00000715] bias:  [ -5.10165009e-05]\n",
      "step:  4154 loss:  5.4049e-09 Weight:  [ 1.00000703] bias:  [ -5.08171834e-05]\n",
      "step:  4155 loss:  5.34181e-09 Weight:  [ 1.00000703] bias:  [ -5.06055876e-05]\n",
      "step:  4156 loss:  5.27733e-09 Weight:  [ 1.00000703] bias:  [ -5.03975680e-05]\n",
      "step:  4157 loss:  5.21972e-09 Weight:  [ 1.00000703] bias:  [ -5.01913346e-05]\n",
      "step:  4158 loss:  5.17271e-09 Weight:  [ 1.00000703] bias:  [ -4.99879643e-05]\n",
      "step:  4159 loss:  5.19119e-09 Weight:  [ 1.00000691] bias:  [ -4.97921028e-05]\n",
      "step:  4160 loss:  5.08726e-09 Weight:  [ 1.00000691] bias:  [ -4.95881359e-05]\n",
      "step:  4161 loss:  5.03053e-09 Weight:  [ 1.00000691] bias:  [ -4.93859552e-05]\n",
      "step:  4162 loss:  4.9742e-09 Weight:  [ 1.00000691] bias:  [ -4.91855644e-05]\n",
      "step:  4163 loss:  4.91348e-09 Weight:  [ 1.00000691] bias:  [ -4.89887498e-05]\n",
      "step:  4164 loss:  4.93159e-09 Weight:  [ 1.00000679] bias:  [ -4.87994439e-05]\n",
      "step:  4165 loss:  4.88753e-09 Weight:  [ 1.00000679] bias:  [ -4.86021527e-05]\n",
      "step:  4166 loss:  4.83542e-09 Weight:  [ 1.00000679] bias:  [ -4.84065313e-05]\n",
      "step:  4167 loss:  4.78032e-09 Weight:  [ 1.00000679] bias:  [ -4.82126961e-05]\n",
      "step:  4168 loss:  4.71789e-09 Weight:  [ 1.00000679] bias:  [ -4.80225572e-05]\n",
      "step:  4169 loss:  4.74547e-09 Weight:  [ 1.00000668] bias:  [ -4.78396905e-05]\n",
      "step:  4170 loss:  4.72101e-09 Weight:  [ 1.00000668] bias:  [ -4.76474052e-05]\n",
      "step:  4171 loss:  4.66992e-09 Weight:  [ 1.00000668] bias:  [ -4.74567896e-05]\n",
      "step:  4172 loss:  4.61591e-09 Weight:  [ 1.00000668] bias:  [ -4.72679603e-05]\n",
      "step:  4173 loss:  4.56267e-09 Weight:  [ 1.00000668] bias:  [ -4.70825908e-05]\n",
      "step:  4174 loss:  4.58309e-09 Weight:  [ 1.00000656] bias:  [ -4.69047300e-05]\n",
      "step:  4175 loss:  4.55781e-09 Weight:  [ 1.00000656] bias:  [ -4.67174505e-05]\n",
      "step:  4176 loss:  4.51253e-09 Weight:  [ 1.00000656] bias:  [ -4.65317207e-05]\n",
      "step:  4177 loss:  4.4596e-09 Weight:  [ 1.00000656] bias:  [ -4.63477809e-05]\n",
      "step:  4178 loss:  4.40726e-09 Weight:  [ 1.00000644] bias:  [ -4.61672971e-05]\n",
      "step:  4179 loss:  4.4435e-09 Weight:  [ 1.00000644] bias:  [ -4.59826406e-05]\n",
      "step:  4180 loss:  4.38598e-09 Weight:  [ 1.00000632] bias:  [ -4.58015602e-05]\n",
      "step:  4181 loss:  4.29732e-09 Weight:  [ 1.00000632] bias:  [ -4.56105881e-05]\n",
      "step:  4182 loss:  4.25296e-09 Weight:  [ 1.00000632] bias:  [ -4.54211659e-05]\n",
      "step:  4183 loss:  4.21507e-09 Weight:  [ 1.00000632] bias:  [ -4.52344830e-05]\n",
      "step:  4184 loss:  4.22976e-09 Weight:  [ 1.00000632] bias:  [ -4.50553125e-05]\n",
      "step:  4185 loss:  4.17891e-09 Weight:  [ 1.0000062] bias:  [ -4.48795981e-05]\n",
      "step:  4186 loss:  4.12439e-09 Weight:  [ 1.0000062] bias:  [ -4.46916056e-05]\n",
      "step:  4187 loss:  4.08093e-09 Weight:  [ 1.0000062] bias:  [ -4.45051628e-05]\n",
      "step:  4188 loss:  4.04366e-09 Weight:  [ 1.0000062] bias:  [ -4.43214631e-05]\n",
      "step:  4189 loss:  4.06521e-09 Weight:  [ 1.0000062] bias:  [ -4.41450320e-05]\n",
      "step:  4190 loss:  4.01494e-09 Weight:  [ 1.0000062] bias:  [ -4.39703908e-05]\n",
      "step:  4191 loss:  3.96531e-09 Weight:  [ 1.00000608] bias:  [ -4.37992057e-05]\n",
      "step:  4192 loss:  3.95444e-09 Weight:  [ 1.00000608] bias:  [ -4.36166956e-05]\n",
      "step:  4193 loss:  3.90379e-09 Weight:  [ 1.00000608] bias:  [ -4.34376416e-05]\n",
      "step:  4194 loss:  3.85419e-09 Weight:  [ 1.00000608] bias:  [ -4.32603774e-05]\n",
      "step:  4195 loss:  3.87696e-09 Weight:  [ 1.00000596] bias:  [ -4.30903856e-05]\n",
      "step:  4196 loss:  3.80631e-09 Weight:  [ 1.00000596] bias:  [ -4.29114516e-05]\n",
      "step:  4197 loss:  3.76451e-09 Weight:  [ 1.00000596] bias:  [ -4.27340674e-05]\n",
      "step:  4198 loss:  3.71569e-09 Weight:  [ 1.00000596] bias:  [ -4.25584731e-05]\n",
      "step:  4199 loss:  3.68797e-09 Weight:  [ 1.00000596] bias:  [ -4.23853817e-05]\n",
      "step:  4200 loss:  3.70969e-09 Weight:  [ 1.00000584] bias:  [ -4.22195626e-05]\n",
      "step:  4201 loss:  3.65993e-09 Weight:  [ 1.00000584] bias:  [ -4.20448014e-05]\n",
      "step:  4202 loss:  3.61139e-09 Weight:  [ 1.00000584] bias:  [ -4.18734962e-05]\n",
      "step:  4203 loss:  3.58834e-09 Weight:  [ 1.00000584] bias:  [ -4.17029078e-05]\n",
      "step:  4204 loss:  3.54113e-09 Weight:  [ 1.00000584] bias:  [ -4.15357754e-05]\n",
      "step:  4205 loss:  3.49394e-09 Weight:  [ 1.00000584] bias:  [ -4.13704329e-05]\n",
      "step:  4206 loss:  3.53687e-09 Weight:  [ 1.00000572] bias:  [ -4.12114059e-05]\n",
      "step:  4207 loss:  3.50392e-09 Weight:  [ 1.00000572] bias:  [ -4.10436769e-05]\n",
      "step:  4208 loss:  3.4612e-09 Weight:  [ 1.00000572] bias:  [ -4.08776177e-05]\n",
      "step:  4209 loss:  3.42166e-09 Weight:  [ 1.00000572] bias:  [ -4.07131083e-05]\n",
      "step:  4210 loss:  3.39529e-09 Weight:  [ 1.00000572] bias:  [ -4.05511018e-05]\n",
      "step:  4211 loss:  3.34921e-09 Weight:  [ 1.00000572] bias:  [ -4.03908853e-05]\n",
      "step:  4212 loss:  3.37732e-09 Weight:  [ 1.0000056] bias:  [ -4.02378209e-05]\n",
      "step:  4213 loss:  3.36329e-09 Weight:  [ 1.0000056] bias:  [ -4.00751014e-05]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step:  4214 loss:  3.31745e-09 Weight:  [ 1.0000056] bias:  [ -3.99141682e-05]\n",
      "step:  4215 loss:  3.28295e-09 Weight:  [ 1.0000056] bias:  [ -3.97546646e-05]\n",
      "step:  4216 loss:  3.25024e-09 Weight:  [ 1.0000056] bias:  [ -3.95979041e-05]\n",
      "step:  4217 loss:  3.21207e-09 Weight:  [ 1.0000056] bias:  [ -3.94426934e-05]\n",
      "step:  4218 loss:  3.241e-09 Weight:  [ 1.00000548] bias:  [ -3.92946349e-05]\n",
      "step:  4219 loss:  3.20887e-09 Weight:  [ 1.00000548] bias:  [ -3.91383510e-05]\n",
      "step:  4220 loss:  3.16826e-09 Weight:  [ 1.00000548] bias:  [ -3.89837369e-05]\n",
      "step:  4221 loss:  3.14955e-09 Weight:  [ 1.00000548] bias:  [ -3.88297194e-05]\n",
      "step:  4222 loss:  3.1057e-09 Weight:  [ 1.00000536] bias:  [ -3.86791580e-05]\n",
      "step:  4223 loss:  3.0347e-09 Weight:  [ 1.00000536] bias:  [ -3.85185849e-05]\n",
      "step:  4224 loss:  3.07166e-09 Weight:  [ 1.00000536] bias:  [ -3.83643273e-05]\n",
      "step:  4225 loss:  3.03204e-09 Weight:  [ 1.00000536] bias:  [ -3.82134094e-05]\n",
      "step:  4226 loss:  2.99515e-09 Weight:  [ 1.00000536] bias:  [ -3.80640413e-05]\n",
      "step:  4227 loss:  2.97426e-09 Weight:  [ 1.00000536] bias:  [ -3.79153862e-05]\n",
      "step:  4228 loss:  2.9378e-09 Weight:  [ 1.00000536] bias:  [ -3.77682809e-05]\n",
      "step:  4229 loss:  2.90005e-09 Weight:  [ 1.00000525] bias:  [ -3.76245152e-05]\n",
      "step:  4230 loss:  2.95086e-09 Weight:  [ 1.00000525] bias:  [ -3.74749070e-05]\n",
      "step:  4231 loss:  2.91195e-09 Weight:  [ 1.00000525] bias:  [ -3.73269686e-05]\n",
      "step:  4232 loss:  2.88767e-09 Weight:  [ 1.00000513] bias:  [ -3.71815331e-05]\n",
      "step:  4233 loss:  2.85327e-09 Weight:  [ 1.00000513] bias:  [ -3.70234629e-05]\n",
      "step:  4234 loss:  2.81712e-09 Weight:  [ 1.00000513] bias:  [ -3.68669425e-05]\n",
      "step:  4235 loss:  2.78952e-09 Weight:  [ 1.00000513] bias:  [ -3.67130451e-05]\n",
      "step:  4236 loss:  2.80745e-09 Weight:  [ 1.00000513] bias:  [ -3.65664164e-05]\n",
      "step:  4237 loss:  2.77204e-09 Weight:  [ 1.00000513] bias:  [ -3.64213374e-05]\n",
      "step:  4238 loss:  2.75194e-09 Weight:  [ 1.00000513] bias:  [ -3.62786450e-05]\n",
      "step:  4239 loss:  2.71442e-09 Weight:  [ 1.00000501] bias:  [ -3.61376187e-05]\n",
      "step:  4240 loss:  2.66854e-09 Weight:  [ 1.00000501] bias:  [ -3.59855076e-05]\n",
      "step:  4241 loss:  2.63083e-09 Weight:  [ 1.00000501] bias:  [ -3.58367361e-05]\n",
      "step:  4242 loss:  2.59609e-09 Weight:  [ 1.00000501] bias:  [ -3.56895143e-05]\n",
      "step:  4243 loss:  2.62832e-09 Weight:  [ 1.00000501] bias:  [ -3.55487282e-05]\n",
      "step:  4244 loss:  2.59399e-09 Weight:  [ 1.00000501] bias:  [ -3.54094918e-05]\n",
      "step:  4245 loss:  2.57521e-09 Weight:  [ 1.00000489] bias:  [ -3.52726383e-05]\n",
      "step:  4246 loss:  2.54302e-09 Weight:  [ 1.00000489] bias:  [ -3.51264862e-05]\n",
      "step:  4247 loss:  2.50894e-09 Weight:  [ 1.00000489] bias:  [ -3.49818838e-05]\n",
      "step:  4248 loss:  2.48379e-09 Weight:  [ 1.00000489] bias:  [ -3.48399044e-05]\n",
      "step:  4249 loss:  2.45371e-09 Weight:  [ 1.00000489] bias:  [ -3.46993584e-05]\n",
      "step:  4250 loss:  2.4889e-09 Weight:  [ 1.00000477] bias:  [ -3.45651279e-05]\n",
      "step:  4251 loss:  2.46587e-09 Weight:  [ 1.00000477] bias:  [ -3.44220753e-05]\n",
      "step:  4252 loss:  2.43588e-09 Weight:  [ 1.00000477] bias:  [ -3.42804560e-05]\n",
      "step:  4253 loss:  2.41937e-09 Weight:  [ 1.00000477] bias:  [ -3.41394298e-05]\n",
      "step:  4254 loss:  2.38387e-09 Weight:  [ 1.00000477] bias:  [ -3.40000734e-05]\n",
      "step:  4255 loss:  2.36165e-09 Weight:  [ 1.00000477] bias:  [ -3.38632199e-05]\n",
      "step:  4256 loss:  2.3324e-09 Weight:  [ 1.00000477] bias:  [ -3.37277997e-05]\n",
      "step:  4257 loss:  2.35528e-09 Weight:  [ 1.00000465] bias:  [ -3.35995319e-05]\n",
      "step:  4258 loss:  2.34787e-09 Weight:  [ 1.00000465] bias:  [ -3.34614888e-05]\n",
      "step:  4259 loss:  2.31528e-09 Weight:  [ 1.00000465] bias:  [ -3.33249955e-05]\n",
      "step:  4260 loss:  2.30267e-09 Weight:  [ 1.00000465] bias:  [ -3.31889787e-05]\n",
      "step:  4261 loss:  2.26817e-09 Weight:  [ 1.00000465] bias:  [ -3.30546281e-05]\n",
      "step:  4262 loss:  2.2498e-09 Weight:  [ 1.00000465] bias:  [ -3.29226641e-05]\n",
      "step:  4263 loss:  2.21799e-09 Weight:  [ 1.00000465] bias:  [ -3.27922498e-05]\n",
      "step:  4264 loss:  2.25749e-09 Weight:  [ 1.00000453] bias:  [ -3.26680347e-05]\n",
      "step:  4265 loss:  2.22519e-09 Weight:  [ 1.00000453] bias:  [ -3.25345209e-05]\n",
      "step:  4266 loss:  2.1906e-09 Weight:  [ 1.00000453] bias:  [ -3.24043431e-05]\n",
      "step:  4267 loss:  2.17835e-09 Weight:  [ 1.00000453] bias:  [ -3.22746419e-05]\n",
      "step:  4268 loss:  2.14716e-09 Weight:  [ 1.00000453] bias:  [ -3.21464904e-05]\n",
      "step:  4269 loss:  2.12959e-09 Weight:  [ 1.00000453] bias:  [ -3.20207255e-05]\n",
      "step:  4270 loss:  2.10206e-09 Weight:  [ 1.00000453] bias:  [ -3.18963903e-05]\n",
      "step:  4271 loss:  2.14189e-09 Weight:  [ 1.00000441] bias:  [ -3.17782542e-05]\n",
      "step:  4272 loss:  2.08307e-09 Weight:  [ 1.00000441] bias:  [ -3.16498663e-05]\n",
      "step:  4273 loss:  2.06019e-09 Weight:  [ 1.00000441] bias:  [ -3.15241014e-05]\n",
      "step:  4274 loss:  2.03295e-09 Weight:  [ 1.00000441] bias:  [ -3.13997662e-05]\n",
      "step:  4275 loss:  2.02111e-09 Weight:  [ 1.00000441] bias:  [ -3.12759075e-05]\n",
      "step:  4276 loss:  1.99421e-09 Weight:  [ 1.00000441] bias:  [ -3.11534786e-05]\n",
      "step:  4277 loss:  1.97794e-09 Weight:  [ 1.00000441] bias:  [ -3.10334362e-05]\n",
      "step:  4278 loss:  1.95138e-09 Weight:  [ 1.00000441] bias:  [ -3.09148236e-05]\n",
      "step:  4279 loss:  1.98979e-09 Weight:  [ 1.00000429] bias:  [ -3.08024100e-05]\n",
      "step:  4280 loss:  1.95448e-09 Weight:  [ 1.00000429] bias:  [ -3.06773582e-05]\n",
      "step:  4281 loss:  1.93744e-09 Weight:  [ 1.00000429] bias:  [ -3.05546928e-05]\n",
      "step:  4282 loss:  1.91102e-09 Weight:  [ 1.00000429] bias:  [ -3.04334571e-05]\n",
      "step:  4283 loss:  1.89955e-09 Weight:  [ 1.00000429] bias:  [ -3.03126981e-05]\n",
      "step:  4284 loss:  1.87348e-09 Weight:  [ 1.00000429] bias:  [ -3.01933687e-05]\n",
      "step:  4285 loss:  1.85758e-09 Weight:  [ 1.00000429] bias:  [ -3.00764241e-05]\n",
      "step:  4286 loss:  1.83184e-09 Weight:  [ 1.00000429] bias:  [ -2.99609110e-05]\n",
      "step:  4287 loss:  1.8688e-09 Weight:  [ 1.00000417] bias:  [ -2.98515952e-05]\n",
      "step:  4288 loss:  1.85014e-09 Weight:  [ 1.00000417] bias:  [ -2.97315510e-05]\n",
      "step:  4289 loss:  1.83347e-09 Weight:  [ 1.00000417] bias:  [ -2.96138915e-05]\n",
      "step:  4290 loss:  1.80788e-09 Weight:  [ 1.00000417] bias:  [ -2.94976617e-05]\n",
      "step:  4291 loss:  1.79678e-09 Weight:  [ 1.00000417] bias:  [ -2.93819103e-05]\n",
      "step:  4292 loss:  1.77153e-09 Weight:  [ 1.00000417] bias:  [ -2.92675886e-05]\n",
      "step:  4293 loss:  1.76098e-09 Weight:  [ 1.00000417] bias:  [ -2.91554134e-05]\n",
      "step:  4294 loss:  1.73604e-09 Weight:  [ 1.00000405] bias:  [ -2.90446678e-05]\n",
      "step:  4295 loss:  1.74291e-09 Weight:  [ 1.00000405] bias:  [ -2.89284380e-05]\n",
      "step:  4296 loss:  1.71788e-09 Weight:  [ 1.00000405] bias:  [ -2.88136398e-05]\n",
      "step:  4297 loss:  1.70246e-09 Weight:  [ 1.00000405] bias:  [ -2.87012263e-05]\n",
      "step:  4298 loss:  1.68072e-09 Weight:  [ 1.00000405] bias:  [ -2.85901224e-05]\n",
      "step:  4299 loss:  1.66995e-09 Weight:  [ 1.00000405] bias:  [ -2.84794969e-05]\n",
      "step:  4300 loss:  1.64559e-09 Weight:  [ 1.00000405] bias:  [ -2.83703012e-05]\n",
      "step:  4301 loss:  1.63129e-09 Weight:  [ 1.00000393] bias:  [ -2.82634901e-05]\n",
      "step:  4302 loss:  1.61869e-09 Weight:  [ 1.00000393] bias:  [ -2.81464272e-05]\n",
      "step:  4303 loss:  1.5943e-09 Weight:  [ 1.00000393] bias:  [ -2.80307941e-05]\n",
      "step:  4304 loss:  1.62466e-09 Weight:  [ 1.00000393] bias:  [ -2.79213600e-05]\n",
      "step:  4305 loss:  1.60061e-09 Weight:  [ 1.00000393] bias:  [ -2.78133557e-05]\n",
      "step:  4306 loss:  1.58891e-09 Weight:  [ 1.00000381] bias:  [ -2.77076178e-05]\n",
      "step:  4307 loss:  1.57536e-09 Weight:  [ 1.00000381] bias:  [ -2.75911498e-05]\n",
      "step:  4308 loss:  1.56492e-09 Weight:  [ 1.00000381] bias:  [ -2.74751583e-05]\n",
      "step:  4309 loss:  1.541e-09 Weight:  [ 1.00000381] bias:  [ -2.73605983e-05]\n",
      "step:  4310 loss:  1.53037e-09 Weight:  [ 1.00000381] bias:  [ -2.72481848e-05]\n",
      "step:  4311 loss:  1.50677e-09 Weight:  [ 1.00000381] bias:  [ -2.71372010e-05]\n",
      "step:  4312 loss:  1.53745e-09 Weight:  [ 1.00000381] bias:  [ -2.70324163e-05]\n",
      "step:  4313 loss:  1.51695e-09 Weight:  [ 1.00000381] bias:  [ -2.69289430e-05]\n",
      "step:  4314 loss:  1.50271e-09 Weight:  [ 1.0000037] bias:  [ -2.68278527e-05]\n",
      "step:  4315 loss:  1.50187e-09 Weight:  [ 1.0000037] bias:  [ -2.67150808e-05]\n",
      "step:  4316 loss:  1.47845e-09 Weight:  [ 1.0000037] bias:  [ -2.66037387e-05]\n",
      "step:  4317 loss:  1.46839e-09 Weight:  [ 1.0000037] bias:  [ -2.64928749e-05]\n",
      "step:  4318 loss:  1.44531e-09 Weight:  [ 1.0000037] bias:  [ -2.63834409e-05]\n",
      "step:  4319 loss:  1.43307e-09 Weight:  [ 1.0000037] bias:  [ -2.62762715e-05]\n",
      "step:  4320 loss:  1.41032e-09 Weight:  [ 1.0000037] bias:  [ -2.61705336e-05]\n",
      "step:  4321 loss:  1.44587e-09 Weight:  [ 1.00000358] bias:  [ -2.60707548e-05]\n",
      "step:  4322 loss:  1.4211e-09 Weight:  [ 1.00000358] bias:  [ -2.59616791e-05]\n",
      "step:  4323 loss:  1.40847e-09 Weight:  [ 1.00000358] bias:  [ -2.58548680e-05]\n",
      "step:  4324 loss:  1.38593e-09 Weight:  [ 1.00000358] bias:  [ -2.57494867e-05]\n",
      "step:  4325 loss:  1.37618e-09 Weight:  [ 1.00000358] bias:  [ -2.56445819e-05]\n",
      "step:  4326 loss:  1.36649e-09 Weight:  [ 1.00000358] bias:  [ -2.55401537e-05]\n",
      "step:  4327 loss:  1.34435e-09 Weight:  [ 1.00000358] bias:  [ -2.54371571e-05]\n",
      "step:  4328 loss:  1.33283e-09 Weight:  [ 1.00000358] bias:  [ -2.53364251e-05]\n",
      "step:  4329 loss:  1.31535e-09 Weight:  [ 1.00000358] bias:  [ -2.52368845e-05]\n",
      "step:  4330 loss:  1.34677e-09 Weight:  [ 1.00000346] bias:  [ -2.51435431e-05]\n",
      "step:  4331 loss:  1.30349e-09 Weight:  [ 1.00000346] bias:  [ -2.50398316e-05]\n",
      "step:  4332 loss:  1.294e-09 Weight:  [ 1.00000346] bias:  [ -2.49365967e-05]\n",
      "step:  4333 loss:  1.28001e-09 Weight:  [ 1.00000346] bias:  [ -2.48357464e-05]\n",
      "step:  4334 loss:  1.26102e-09 Weight:  [ 1.00000346] bias:  [ -2.47362059e-05]\n",
      "step:  4335 loss:  1.25593e-09 Weight:  [ 1.00000346] bias:  [ -2.46369054e-05]\n",
      "step:  4336 loss:  1.23469e-09 Weight:  [ 1.00000346] bias:  [ -2.45390347e-05]\n",
      "step:  4337 loss:  1.22431e-09 Weight:  [ 1.00000346] bias:  [ -2.44434286e-05]\n",
      "step:  4338 loss:  1.2151e-09 Weight:  [ 1.00000346] bias:  [ -2.43482991e-05]\n",
      "step:  4339 loss:  1.19424e-09 Weight:  [ 1.00000346] bias:  [ -2.42546012e-05]\n",
      "step:  4340 loss:  1.22673e-09 Weight:  [ 1.00000334] bias:  [ -2.41669823e-05]\n",
      "step:  4341 loss:  1.21581e-09 Weight:  [ 1.00000334] bias:  [ -2.40683967e-05]\n",
      "step:  4342 loss:  1.20463e-09 Weight:  [ 1.00000334] bias:  [ -2.39720757e-05]\n",
      "step:  4343 loss:  1.19556e-09 Weight:  [ 1.00000334] bias:  [ -2.38762314e-05]\n",
      "step:  4344 loss:  1.17729e-09 Weight:  [ 1.00000334] bias:  [ -2.37816985e-05]\n",
      "step:  4345 loss:  1.16832e-09 Weight:  [ 1.00000334] bias:  [ -2.36876422e-05]\n",
      "step:  4346 loss:  1.15198e-09 Weight:  [ 1.00000334] bias:  [ -2.35947773e-05]\n",
      "step:  4347 loss:  1.14187e-09 Weight:  [ 1.00000334] bias:  [ -2.35041789e-05]\n",
      "step:  4348 loss:  1.13303e-09 Weight:  [ 1.00000334] bias:  [ -2.34140571e-05]\n",
      "step:  4349 loss:  1.11539e-09 Weight:  [ 1.00000334] bias:  [ -2.33252467e-05]\n",
      "step:  4350 loss:  1.14972e-09 Weight:  [ 1.00000322] bias:  [ -2.32423954e-05]\n",
      "step:  4351 loss:  1.1246e-09 Weight:  [ 1.00000322] bias:  [ -2.31467893e-05]\n",
      "step:  4352 loss:  1.11585e-09 Weight:  [ 1.00000322] bias:  [ -2.30516598e-05]\n",
      "step:  4353 loss:  1.10497e-09 Weight:  [ 1.00000322] bias:  [ -2.29587949e-05]\n",
      "step:  4354 loss:  1.089e-09 Weight:  [ 1.00000322] bias:  [ -2.28671233e-05]\n",
      "step:  4355 loss:  1.08271e-09 Weight:  [ 1.00000322] bias:  [ -2.27758082e-05]\n",
      "step:  4356 loss:  1.07413e-09 Weight:  [ 1.00000322] bias:  [ -2.26849716e-05]\n",
      "step:  4357 loss:  1.05689e-09 Weight:  [ 1.00000322] bias:  [ -2.25954445e-05]\n",
      "step:  4358 loss:  1.04862e-09 Weight:  [ 1.00000322] bias:  [ -2.25080639e-05]\n",
      "step:  4359 loss:  1.03167e-09 Weight:  [ 1.00000322] bias:  [ -2.24219948e-05]\n",
      "step:  4360 loss:  1.02325e-09 Weight:  [ 1.00000322] bias:  [ -2.23364023e-05]\n",
      "step:  4361 loss:  1.05446e-09 Weight:  [ 1.0000031] bias:  [ -2.22568888e-05]\n",
      "step:  4362 loss:  1.01575e-09 Weight:  [ 1.0000031] bias:  [ -2.21668852e-05]\n",
      "step:  4363 loss:  1.00601e-09 Weight:  [ 1.0000031] bias:  [ -2.20791480e-05]\n",
      "step:  4364 loss:  9.97687e-10 Weight:  [ 1.0000031] bias:  [ -2.19918875e-05]\n",
      "step:  4365 loss:  9.80979e-10 Weight:  [ 1.0000031] bias:  [ -2.19059384e-05]\n",
      "step:  4366 loss:  9.7873e-10 Weight:  [ 1.0000031] bias:  [ -2.18201076e-05]\n",
      "step:  4367 loss:  9.70516e-10 Weight:  [ 1.0000031] bias:  [ -2.17347533e-05]\n",
      "step:  4368 loss:  9.5412e-10 Weight:  [ 1.0000031] bias:  [ -2.16507106e-05]\n",
      "step:  4369 loss:  9.46958e-10 Weight:  [ 1.0000031] bias:  [ -2.15688142e-05]\n",
      "step:  4370 loss:  9.30839e-10 Weight:  [ 1.0000031] bias:  [ -2.14882293e-05]\n",
      "step:  4371 loss:  9.24988e-10 Weight:  [ 1.0000031] bias:  [ -2.14080010e-05]\n",
      "step:  4372 loss:  9.56081e-10 Weight:  [ 1.00000298] bias:  [ -2.13337335e-05]\n",
      "step:  4373 loss:  9.33781e-10 Weight:  [ 1.00000298] bias:  [ -2.12500490e-05]\n",
      "step:  4374 loss:  9.27944e-10 Weight:  [ 1.00000298] bias:  [ -2.11667211e-05]\n",
      "step:  4375 loss:  9.20412e-10 Weight:  [ 1.00000298] bias:  [ -2.10855396e-05]\n",
      "step:  4376 loss:  9.04507e-10 Weight:  [ 1.00000298] bias:  [ -2.10056696e-05]\n",
      "step:  4377 loss:  8.96605e-10 Weight:  [ 1.00000298] bias:  [ -2.09262762e-05]\n",
      "step:  4378 loss:  8.94463e-10 Weight:  [ 1.00000298] bias:  [ -2.08470028e-05]\n",
      "step:  4379 loss:  8.7887e-10 Weight:  [ 1.00000298] bias:  [ -2.07690391e-05]\n",
      "step:  4380 loss:  8.71083e-10 Weight:  [ 1.00000298] bias:  [ -2.06915538e-05]\n",
      "step:  4381 loss:  8.66695e-10 Weight:  [ 1.00000286] bias:  [ -2.06160948e-05]\n",
      "step:  4382 loss:  8.57682e-10 Weight:  [ 1.00000286] bias:  [ -2.05297874e-05]\n",
      "step:  4383 loss:  8.49923e-10 Weight:  [ 1.00000286] bias:  [ -2.04439566e-05]\n",
      "step:  4384 loss:  8.74326e-10 Weight:  [ 1.00000286] bias:  [ -2.03642048e-05]\n",
      "step:  4385 loss:  8.62343e-10 Weight:  [ 1.00000286] bias:  [ -2.02855263e-05]\n",
      "step:  4386 loss:  8.54698e-10 Weight:  [ 1.00000286] bias:  [ -2.02073243e-05]\n",
      "step:  4387 loss:  8.49443e-10 Weight:  [ 1.00000286] bias:  [ -2.01311486e-05]\n",
      "step:  4388 loss:  8.34262e-10 Weight:  [ 1.00000286] bias:  [ -2.00562845e-05]\n",
      "step:  4389 loss:  8.26731e-10 Weight:  [ 1.00000286] bias:  [ -1.99818987e-05]\n",
      "step:  4390 loss:  8.24688e-10 Weight:  [ 1.00000286] bias:  [ -1.99076312e-05]\n",
      "step:  4391 loss:  8.0982e-10 Weight:  [ 1.00000274] bias:  [ -1.98346752e-05]\n",
      "step:  4392 loss:  8.11614e-10 Weight:  [ 1.00000274] bias:  [ -1.97499176e-05]\n",
      "step:  4393 loss:  8.03457e-10 Weight:  [ 1.00000274] bias:  [ -1.96673063e-05]\n",
      "step:  4394 loss:  7.88376e-10 Weight:  [ 1.00000274] bias:  [ -1.95860048e-05]\n",
      "step:  4395 loss:  7.82993e-10 Weight:  [ 1.00000274] bias:  [ -1.95050616e-05]\n",
      "step:  4396 loss:  8.08999e-10 Weight:  [ 1.00000274] bias:  [ -1.94300792e-05]\n",
      "step:  4397 loss:  7.9423e-10 Weight:  [ 1.00000274] bias:  [ -1.93564083e-05]\n",
      "step:  4398 loss:  7.88933e-10 Weight:  [ 1.00000274] bias:  [ -1.92830939e-05]\n",
      "step:  4399 loss:  7.83771e-10 Weight:  [ 1.00000262] bias:  [ -1.92118059e-05]\n",
      "step:  4400 loss:  7.7452e-10 Weight:  [ 1.00000262] bias:  [ -1.91302661e-05]\n",
      "step:  4401 loss:  7.59687e-10 Weight:  [ 1.00000262] bias:  [ -1.90500377e-05]\n",
      "step:  4402 loss:  7.57723e-10 Weight:  [ 1.00000262] bias:  [ -1.89699294e-05]\n",
      "step:  4403 loss:  7.50532e-10 Weight:  [ 1.00000262] bias:  [ -1.88902977e-05]\n",
      "step:  4404 loss:  7.36012e-10 Weight:  [ 1.00000262] bias:  [ -1.88119775e-05]\n",
      "step:  4405 loss:  7.30438e-10 Weight:  [ 1.00000262] bias:  [ -1.87356836e-05]\n",
      "step:  4406 loss:  7.23361e-10 Weight:  [ 1.00000262] bias:  [ -1.86598663e-05]\n",
      "step:  4407 loss:  7.12351e-10 Weight:  [ 1.00000262] bias:  [ -1.85851222e-05]\n",
      "step:  4408 loss:  7.37259e-10 Weight:  [ 1.0000025] bias:  [ -1.85164572e-05]\n",
      "step:  4409 loss:  7.15477e-10 Weight:  [ 1.0000025] bias:  [ -1.84364671e-05]\n",
      "step:  4410 loss:  7.0246e-10 Weight:  [ 1.0000025] bias:  [ -1.83576703e-05]\n",
      "step:  4411 loss:  6.93731e-10 Weight:  [ 1.0000025] bias:  [ -1.82811382e-05]\n",
      "step:  4412 loss:  6.88676e-10 Weight:  [ 1.0000025] bias:  [ -1.82049625e-05]\n",
      "step:  4413 loss:  6.75943e-10 Weight:  [ 1.0000025] bias:  [ -1.81299802e-05]\n",
      "step:  4414 loss:  6.70937e-10 Weight:  [ 1.0000025] bias:  [ -1.80553543e-05]\n",
      "step:  4415 loss:  6.65967e-10 Weight:  [ 1.0000025] bias:  [ -1.79810868e-05]\n",
      "step:  4416 loss:  6.55362e-10 Weight:  [ 1.0000025] bias:  [ -1.79078925e-05]\n",
      "step:  4417 loss:  6.48598e-10 Weight:  [ 1.0000025] bias:  [ -1.78351747e-05]\n",
      "step:  4418 loss:  6.44039e-10 Weight:  [ 1.0000025] bias:  [ -1.77644833e-05]\n",
      "step:  4419 loss:  6.39162e-10 Weight:  [ 1.0000025] bias:  [ -1.76941503e-05]\n",
      "step:  4420 loss:  6.25793e-10 Weight:  [ 1.0000025] bias:  [ -1.76251287e-05]\n",
      "step:  4421 loss:  6.52179e-10 Weight:  [ 1.00000238] bias:  [ -1.75619480e-05]\n",
      "step:  4422 loss:  6.44281e-10 Weight:  [ 1.00000238] bias:  [ -1.74850575e-05]\n",
      "step:  4423 loss:  6.31918e-10 Weight:  [ 1.00000238] bias:  [ -1.74093602e-05]\n",
      "step:  4424 loss:  6.27097e-10 Weight:  [ 1.00000238] bias:  [ -1.73340195e-05]\n",
      "step:  4425 loss:  6.18673e-10 Weight:  [ 1.00000238] bias:  [ -1.72609434e-05]\n",
      "step:  4426 loss:  6.08367e-10 Weight:  [ 1.00000238] bias:  [ -1.71889405e-05]\n",
      "step:  4427 loss:  6.01858e-10 Weight:  [ 1.00000238] bias:  [ -1.71174142e-05]\n",
      "step:  4428 loss:  6.001e-10 Weight:  [ 1.00000238] bias:  [ -1.70460080e-05]\n",
      "step:  4429 loss:  5.87107e-10 Weight:  [ 1.00000238] bias:  [ -1.69759132e-05]\n",
      "step:  4430 loss:  5.8245e-10 Weight:  [ 1.00000238] bias:  [ -1.69061750e-05]\n",
      "step:  4431 loss:  5.77984e-10 Weight:  [ 1.00000238] bias:  [ -1.68384649e-05]\n",
      "step:  4432 loss:  5.73369e-10 Weight:  [ 1.00000238] bias:  [ -1.67711114e-05]\n",
      "step:  4433 loss:  5.61858e-10 Weight:  [ 1.00000238] bias:  [ -1.67049511e-05]\n",
      "step:  4434 loss:  5.57293e-10 Weight:  [ 1.00000238] bias:  [ -1.66391474e-05]\n",
      "step:  4435 loss:  5.81966e-10 Weight:  [ 1.00000226] bias:  [ -1.65791844e-05]\n",
      "step:  4436 loss:  5.82887e-10 Weight:  [ 1.00000226] bias:  [ -1.65074198e-05]\n",
      "step:  4437 loss:  5.70029e-10 Weight:  [ 1.00000226] bias:  [ -1.64369667e-05]\n",
      "step:  4438 loss:  5.64697e-10 Weight:  [ 1.00000226] bias:  [ -1.63685399e-05]\n",
      "step:  4439 loss:  5.60181e-10 Weight:  [ 1.00000226] bias:  [ -1.63004715e-05]\n",
      "step:  4440 loss:  5.48756e-10 Weight:  [ 1.00000226] bias:  [ -1.62335946e-05]\n",
      "step:  4441 loss:  5.4429e-10 Weight:  [ 1.00000226] bias:  [ -1.61670760e-05]\n",
      "step:  4442 loss:  5.42631e-10 Weight:  [ 1.00000226] bias:  [ -1.61006756e-05]\n",
      "step:  4443 loss:  5.38208e-10 Weight:  [ 1.00000226] bias:  [ -1.60346335e-05]\n",
      "step:  4444 loss:  5.28718e-10 Weight:  [ 1.00000226] bias:  [ -1.59696647e-05]\n",
      "step:  4445 loss:  5.21609e-10 Weight:  [ 1.00000226] bias:  [ -1.59069605e-05]\n",
      "step:  4446 loss:  5.17264e-10 Weight:  [ 1.00000226] bias:  [ -1.58446146e-05]\n",
      "step:  4447 loss:  5.08024e-10 Weight:  [ 1.00000226] bias:  [ -1.57833401e-05]\n",
      "step:  4448 loss:  5.03722e-10 Weight:  [ 1.00000226] bias:  [ -1.57224240e-05]\n",
      "step:  4449 loss:  5.28487e-10 Weight:  [ 1.00000215] bias:  [ -1.56673486e-05]\n",
      "step:  4450 loss:  5.08283e-10 Weight:  [ 1.00000215] bias:  [ -1.56009482e-05]\n",
      "step:  4451 loss:  4.98943e-10 Weight:  [ 1.00000215] bias:  [ -1.55356211e-05]\n",
      "step:  4452 loss:  4.94655e-10 Weight:  [ 1.00000215] bias:  [ -1.54706522e-05]\n",
      "step:  4453 loss:  4.90331e-10 Weight:  [ 1.00000215] bias:  [ -1.54077097e-05]\n",
      "step:  4454 loss:  4.84505e-10 Weight:  [ 1.00000215] bias:  [ -1.53452438e-05]\n",
      "step:  4455 loss:  4.75428e-10 Weight:  [ 1.00000215] bias:  [ -1.52838511e-05]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step:  4456 loss:  4.71232e-10 Weight:  [ 1.00000215] bias:  [ -1.52228158e-05]\n",
      "step:  4457 loss:  4.69672e-10 Weight:  [ 1.00000215] bias:  [ -1.51618997e-05]\n",
      "step:  4458 loss:  4.65519e-10 Weight:  [ 1.00000215] bias:  [ -1.51013410e-05]\n",
      "step:  4459 loss:  4.56698e-10 Weight:  [ 1.00000215] bias:  [ -1.50418555e-05]\n",
      "step:  4460 loss:  4.52587e-10 Weight:  [ 1.00000215] bias:  [ -1.49827274e-05]\n",
      "step:  4461 loss:  4.49237e-10 Weight:  [ 1.00000215] bias:  [ -1.49256266e-05]\n",
      "step:  4462 loss:  4.45169e-10 Weight:  [ 1.00000215] bias:  [ -1.48688832e-05]\n",
      "step:  4463 loss:  4.36604e-10 Weight:  [ 1.00000215] bias:  [ -1.48132121e-05]\n",
      "step:  4464 loss:  4.34088e-10 Weight:  [ 1.00000215] bias:  [ -1.47577803e-05]\n",
      "step:  4465 loss:  4.57135e-10 Weight:  [ 1.00000203] bias:  [ -1.47081892e-05]\n",
      "step:  4466 loss:  4.48825e-10 Weight:  [ 1.00000203] bias:  [ -1.46482271e-05]\n",
      "step:  4467 loss:  4.40153e-10 Weight:  [ 1.00000203] bias:  [ -1.45893373e-05]\n",
      "step:  4468 loss:  4.36135e-10 Weight:  [ 1.00000203] bias:  [ -1.45308059e-05]\n",
      "step:  4469 loss:  4.32365e-10 Weight:  [ 1.00000203] bias:  [ -1.44743008e-05]\n",
      "step:  4470 loss:  4.2839e-10 Weight:  [ 1.00000203] bias:  [ -1.44181531e-05]\n",
      "step:  4471 loss:  4.19973e-10 Weight:  [ 1.00000203] bias:  [ -1.43630787e-05]\n",
      "step:  4472 loss:  4.16041e-10 Weight:  [ 1.00000203] bias:  [ -1.43083616e-05]\n",
      "step:  4473 loss:  4.14573e-10 Weight:  [ 1.00000203] bias:  [ -1.42537638e-05]\n",
      "step:  4474 loss:  4.13113e-10 Weight:  [ 1.00000203] bias:  [ -1.41992850e-05]\n",
      "step:  4475 loss:  4.0923e-10 Weight:  [ 1.00000203] bias:  [ -1.41451637e-05]\n",
      "step:  4476 loss:  4.01077e-10 Weight:  [ 1.00000203] bias:  [ -1.40921156e-05]\n",
      "step:  4477 loss:  3.97236e-10 Weight:  [ 1.00000203] bias:  [ -1.40394250e-05]\n",
      "step:  4478 loss:  3.95872e-10 Weight:  [ 1.00000203] bias:  [ -1.39886415e-05]\n",
      "step:  4479 loss:  3.92067e-10 Weight:  [ 1.00000203] bias:  [ -1.39382164e-05]\n",
      "step:  4480 loss:  3.84162e-10 Weight:  [ 1.00000203] bias:  [ -1.38888636e-05]\n",
      "step:  4481 loss:  3.82745e-10 Weight:  [ 1.00000203] bias:  [ -1.38396299e-05]\n",
      "step:  4482 loss:  3.78989e-10 Weight:  [ 1.00000203] bias:  [ -1.37907537e-05]\n",
      "step:  4483 loss:  4.02142e-10 Weight:  [ 1.00000191] bias:  [ -1.37477191e-05]\n",
      "step:  4484 loss:  3.9957e-10 Weight:  [ 1.00000191] bias:  [ -1.36927638e-05]\n",
      "step:  4485 loss:  3.91452e-10 Weight:  [ 1.00000191] bias:  [ -1.36388808e-05]\n",
      "step:  4486 loss:  3.87704e-10 Weight:  [ 1.00000191] bias:  [ -1.35853561e-05]\n",
      "step:  4487 loss:  3.84034e-10 Weight:  [ 1.00000191] bias:  [ -1.35338578e-05]\n",
      "step:  4488 loss:  3.82645e-10 Weight:  [ 1.00000191] bias:  [ -1.34824786e-05]\n",
      "step:  4489 loss:  3.78947e-10 Weight:  [ 1.00000191] bias:  [ -1.34314569e-05]\n",
      "step:  4490 loss:  3.71092e-10 Weight:  [ 1.00000191] bias:  [ -1.33815083e-05]\n",
      "step:  4491 loss:  3.68804e-10 Weight:  [ 1.00000191] bias:  [ -1.33317981e-05]\n",
      "step:  4492 loss:  3.67436e-10 Weight:  [ 1.00000191] bias:  [ -1.32822070e-05]\n",
      "step:  4493 loss:  3.63816e-10 Weight:  [ 1.00000191] bias:  [ -1.32329733e-05]\n",
      "step:  4494 loss:  3.62462e-10 Weight:  [ 1.00000191] bias:  [ -1.31838588e-05]\n",
      "step:  4495 loss:  3.54863e-10 Weight:  [ 1.00000191] bias:  [ -1.31358174e-05]\n",
      "step:  4496 loss:  3.51292e-10 Weight:  [ 1.00000191] bias:  [ -1.30881335e-05]\n",
      "step:  4497 loss:  3.49928e-10 Weight:  [ 1.00000191] bias:  [ -1.30423568e-05]\n",
      "step:  4498 loss:  3.46393e-10 Weight:  [ 1.00000191] bias:  [ -1.29969385e-05]\n",
      "step:  4499 loss:  3.45068e-10 Weight:  [ 1.00000191] bias:  [ -1.29516393e-05]\n",
      "step:  4500 loss:  3.37725e-10 Weight:  [ 1.00000191] bias:  [ -1.29074124e-05]\n",
      "step:  4501 loss:  3.3555e-10 Weight:  [ 1.00000179] bias:  [ -1.28634238e-05]\n",
      "step:  4502 loss:  3.57161e-10 Weight:  [ 1.00000179] bias:  [ -1.28131178e-05]\n",
      "step:  4503 loss:  3.53641e-10 Weight:  [ 1.00000179] bias:  [ -1.27631693e-05]\n",
      "step:  4504 loss:  3.5233e-10 Weight:  [ 1.00000179] bias:  [ -1.27133399e-05]\n",
      "step:  4505 loss:  3.44773e-10 Weight:  [ 1.00000179] bias:  [ -1.26645837e-05]\n",
      "step:  4506 loss:  3.41302e-10 Weight:  [ 1.00000179] bias:  [ -1.26161849e-05]\n",
      "step:  4507 loss:  3.39028e-10 Weight:  [ 1.00000179] bias:  [ -1.25696934e-05]\n",
      "step:  4508 loss:  3.35593e-10 Weight:  [ 1.00000179] bias:  [ -1.25235592e-05]\n",
      "step:  4509 loss:  3.3431e-10 Weight:  [ 1.00000179] bias:  [ -1.24775443e-05]\n",
      "step:  4510 loss:  3.2701e-10 Weight:  [ 1.00000179] bias:  [ -1.24326025e-05]\n",
      "step:  4511 loss:  3.24892e-10 Weight:  [ 1.00000179] bias:  [ -1.23878990e-05]\n",
      "step:  4512 loss:  3.23624e-10 Weight:  [ 1.00000179] bias:  [ -1.23433147e-05]\n",
      "step:  4513 loss:  3.22363e-10 Weight:  [ 1.00000179] bias:  [ -1.22988495e-05]\n",
      "step:  4514 loss:  3.19019e-10 Weight:  [ 1.00000179] bias:  [ -1.22547417e-05]\n",
      "step:  4515 loss:  3.13221e-10 Weight:  [ 1.00000179] bias:  [ -1.22115880e-05]\n",
      "step:  4516 loss:  3.09914e-10 Weight:  [ 1.00000179] bias:  [ -1.21687917e-05]\n",
      "step:  4517 loss:  3.08674e-10 Weight:  [ 1.00000179] bias:  [ -1.21261146e-05]\n",
      "step:  4518 loss:  3.06077e-10 Weight:  [ 1.00000167] bias:  [ -1.20854638e-05]\n",
      "step:  4519 loss:  3.01458e-10 Weight:  [ 1.00000167] bias:  [ -1.20343229e-05]\n",
      "step:  4520 loss:  2.94307e-10 Weight:  [ 1.00000167] bias:  [ -1.19842553e-05]\n",
      "step:  4521 loss:  2.93074e-10 Weight:  [ 1.00000167] bias:  [ -1.19343067e-05]\n",
      "step:  4522 loss:  2.89788e-10 Weight:  [ 1.00000167] bias:  [ -1.18847156e-05]\n",
      "step:  4523 loss:  3.07669e-10 Weight:  [ 1.00000167] bias:  [ -1.18409662e-05]\n",
      "step:  4524 loss:  3.05636e-10 Weight:  [ 1.00000167] bias:  [ -1.17974550e-05]\n",
      "step:  4525 loss:  2.98741e-10 Weight:  [ 1.00000167] bias:  [ -1.17550162e-05]\n",
      "step:  4526 loss:  2.97536e-10 Weight:  [ 1.00000167] bias:  [ -1.17126965e-05]\n",
      "step:  4527 loss:  2.95532e-10 Weight:  [ 1.00000167] bias:  [ -1.16706160e-05]\n",
      "step:  4528 loss:  2.92516e-10 Weight:  [ 1.00000155] bias:  [ -1.16305619e-05]\n",
      "step:  4529 loss:  2.80526e-10 Weight:  [ 1.00000155] bias:  [ -1.15789444e-05]\n",
      "step:  4530 loss:  2.77296e-10 Weight:  [ 1.00000155] bias:  [ -1.15276844e-05]\n",
      "step:  4531 loss:  2.70308e-10 Weight:  [ 1.00000155] bias:  [ -1.14774975e-05]\n",
      "step:  4532 loss:  2.68304e-10 Weight:  [ 1.00000155] bias:  [ -1.14275490e-05]\n",
      "step:  4533 loss:  2.67121e-10 Weight:  [ 1.00000155] bias:  [ -1.13777196e-05]\n",
      "step:  4534 loss:  2.6397e-10 Weight:  [ 1.00000155] bias:  [ -1.13282476e-05]\n",
      "step:  4535 loss:  2.62801e-10 Weight:  [ 1.00000155] bias:  [ -1.12788948e-05]\n",
      "step:  4536 loss:  2.56069e-10 Weight:  [ 1.00000155] bias:  [ -1.12306152e-05]\n",
      "step:  4537 loss:  2.52967e-10 Weight:  [ 1.00000155] bias:  [ -1.11826930e-05]\n",
      "step:  4538 loss:  2.51148e-10 Weight:  [ 1.00000155] bias:  [ -1.11366780e-05]\n",
      "step:  4539 loss:  2.48082e-10 Weight:  [ 1.00000155] bias:  [ -1.10910205e-05]\n",
      "step:  4540 loss:  2.46942e-10 Weight:  [ 1.00000155] bias:  [ -1.10454821e-05]\n",
      "step:  4541 loss:  2.40465e-10 Weight:  [ 1.00000155] bias:  [ -1.10010169e-05]\n",
      "step:  4542 loss:  2.38575e-10 Weight:  [ 1.00000155] bias:  [ -1.09567900e-05]\n",
      "step:  4543 loss:  2.5473e-10 Weight:  [ 1.00000155] bias:  [ -1.09184048e-05]\n",
      "step:  4544 loss:  2.5361e-10 Weight:  [ 1.00000155] bias:  [ -1.08801387e-05]\n",
      "step:  4545 loss:  2.51749e-10 Weight:  [ 1.00000155] bias:  [ -1.08421109e-05]\n",
      "step:  4546 loss:  2.45521e-10 Weight:  [ 1.00000155] bias:  [ -1.08051563e-05]\n",
      "step:  4547 loss:  2.44416e-10 Weight:  [ 1.00000155] bias:  [ -1.07683209e-05]\n",
      "step:  4548 loss:  2.42583e-10 Weight:  [ 1.00000155] bias:  [ -1.07317237e-05]\n",
      "step:  4549 loss:  2.41485e-10 Weight:  [ 1.00000155] bias:  [ -1.06952457e-05]\n",
      "step:  4550 loss:  2.39485e-10 Weight:  [ 1.00000143] bias:  [ -1.06607940e-05]\n",
      "step:  4551 loss:  2.39414e-10 Weight:  [ 1.00000143] bias:  [ -1.06144216e-05]\n",
      "step:  4552 loss:  2.38316e-10 Weight:  [ 1.00000143] bias:  [ -1.05681684e-05]\n",
      "step:  4553 loss:  2.31882e-10 Weight:  [ 1.00000143] bias:  [ -1.05229883e-05]\n",
      "step:  4554 loss:  2.30049e-10 Weight:  [ 1.00000143] bias:  [ -1.04780465e-05]\n",
      "step:  4555 loss:  2.28965e-10 Weight:  [ 1.00000143] bias:  [ -1.04332239e-05]\n",
      "step:  4556 loss:  2.27889e-10 Weight:  [ 1.00000143] bias:  [ -1.03885204e-05]\n",
      "step:  4557 loss:  2.25015e-10 Weight:  [ 1.00000143] bias:  [ -1.03441744e-05]\n",
      "step:  4558 loss:  2.19899e-10 Weight:  [ 1.00000143] bias:  [ -1.03007824e-05]\n",
      "step:  4559 loss:  2.1706e-10 Weight:  [ 1.00000143] bias:  [ -1.02577478e-05]\n",
      "step:  4560 loss:  2.16005e-10 Weight:  [ 1.00000143] bias:  [ -1.02148324e-05]\n",
      "step:  4561 loss:  2.13138e-10 Weight:  [ 1.00000143] bias:  [ -1.01739433e-05]\n",
      "step:  4562 loss:  2.1139e-10 Weight:  [ 1.00000143] bias:  [ -1.01332926e-05]\n",
      "step:  4563 loss:  2.10349e-10 Weight:  [ 1.00000143] bias:  [ -1.00927618e-05]\n",
      "step:  4564 loss:  2.04427e-10 Weight:  [ 1.00000143] bias:  [ -1.00533034e-05]\n",
      "step:  4565 loss:  2.02707e-10 Weight:  [ 1.00000143] bias:  [ -1.00140833e-05]\n",
      "step:  4566 loss:  2.0168e-10 Weight:  [ 1.00000143] bias:  [ -9.97498228e-06]\n",
      "step:  4567 loss:  2.17941e-10 Weight:  [ 1.00000131] bias:  [ -9.94172296e-06]\n",
      "step:  4568 loss:  2.14431e-10 Weight:  [ 1.00000131] bias:  [ -9.89463570e-06]\n",
      "step:  4569 loss:  2.13404e-10 Weight:  [ 1.00000131] bias:  [ -9.84766757e-06]\n",
      "step:  4570 loss:  2.07269e-10 Weight:  [ 1.00000131] bias:  [ -9.80177174e-06]\n",
      "step:  4571 loss:  2.04537e-10 Weight:  [ 1.00000131] bias:  [ -9.75623334e-06]\n",
      "step:  4572 loss:  2.01808e-10 Weight:  [ 1.00000131] bias:  [ -9.71260306e-06]\n",
      "step:  4573 loss:  1.99112e-10 Weight:  [ 1.00000131] bias:  [ -9.66933021e-06]\n",
      "step:  4574 loss:  1.98114e-10 Weight:  [ 1.00000131] bias:  [ -9.62617651e-06]\n",
      "step:  4575 loss:  1.93225e-10 Weight:  [ 1.00000131] bias:  [ -9.58397686e-06]\n",
      "step:  4576 loss:  1.90571e-10 Weight:  [ 1.00000131] bias:  [ -9.54213465e-06]\n",
      "step:  4577 loss:  1.89587e-10 Weight:  [ 1.00000131] bias:  [ -9.50041158e-06]\n",
      "step:  4578 loss:  1.8861e-10 Weight:  [ 1.00000131] bias:  [ -9.45880765e-06]\n",
      "step:  4579 loss:  1.86976e-10 Weight:  [ 1.00000131] bias:  [ -9.41744202e-06]\n",
      "step:  4580 loss:  1.81345e-10 Weight:  [ 1.00000131] bias:  [ -9.37714958e-06]\n",
      "step:  4581 loss:  1.80382e-10 Weight:  [ 1.00000131] bias:  [ -9.33697629e-06]\n",
      "step:  4582 loss:  1.78776e-10 Weight:  [ 1.00000131] bias:  [ -9.29704129e-06]\n",
      "step:  4583 loss:  1.76001e-10 Weight:  [ 1.00000131] bias:  [ -9.25913264e-06]\n",
      "step:  4584 loss:  1.75053e-10 Weight:  [ 1.00000131] bias:  [ -9.22134313e-06]\n",
      "step:  4585 loss:  1.73475e-10 Weight:  [ 1.00000131] bias:  [ -9.18379192e-06]\n",
      "step:  4586 loss:  1.72534e-10 Weight:  [ 1.00000131] bias:  [ -9.14635984e-06]\n",
      "step:  4587 loss:  1.67166e-10 Weight:  [ 1.00000131] bias:  [ -9.11000097e-06]\n",
      "step:  4588 loss:  1.65617e-10 Weight:  [ 1.00000131] bias:  [ -9.07388039e-06]\n",
      "step:  4589 loss:  1.6469e-10 Weight:  [ 1.00000131] bias:  [ -9.03787895e-06]\n",
      "step:  4590 loss:  1.80151e-10 Weight:  [ 1.00000131] bias:  [ -9.00760006e-06]\n",
      "step:  4591 loss:  1.79231e-10 Weight:  [ 1.00000131] bias:  [ -8.97744030e-06]\n",
      "step:  4592 loss:  1.7771e-10 Weight:  [ 1.00000131] bias:  [ -8.94751884e-06]\n",
      "step:  4593 loss:  1.76797e-10 Weight:  [ 1.00000131] bias:  [ -8.91771651e-06]\n",
      "step:  4594 loss:  1.72591e-10 Weight:  [ 1.00000119] bias:  [ -8.88886825e-06]\n",
      "step:  4595 loss:  1.62647e-10 Weight:  [ 1.00000119] bias:  [ -8.84845667e-06]\n",
      "step:  4596 loss:  1.60185e-10 Weight:  [ 1.00000119] bias:  [ -8.80840253e-06]\n",
      "step:  4597 loss:  1.58366e-10 Weight:  [ 1.00000119] bias:  [ -8.77025559e-06]\n",
      "step:  4598 loss:  1.5746e-10 Weight:  [ 1.00000119] bias:  [ -8.73222780e-06]\n",
      "step:  4599 loss:  1.55939e-10 Weight:  [ 1.00000119] bias:  [ -8.69443829e-06]\n",
      "step:  4600 loss:  1.5072e-10 Weight:  [ 1.00000119] bias:  [ -8.65772199e-06]\n",
      "step:  4601 loss:  1.49829e-10 Weight:  [ 1.00000119] bias:  [ -8.62112483e-06]\n",
      "step:  4602 loss:  1.48336e-10 Weight:  [ 1.00000119] bias:  [ -8.58476596e-06]\n",
      "step:  4603 loss:  1.47452e-10 Weight:  [ 1.00000119] bias:  [ -8.54852624e-06]\n",
      "step:  4604 loss:  1.46574e-10 Weight:  [ 1.00000119] bias:  [ -8.51240566e-06]\n",
      "step:  4605 loss:  1.45111e-10 Weight:  [ 1.00000119] bias:  [ -8.47652336e-06]\n",
      "step:  4606 loss:  1.4424e-10 Weight:  [ 1.00000119] bias:  [ -8.44076021e-06]\n",
      "step:  4607 loss:  1.40147e-10 Weight:  [ 1.00000119] bias:  [ -8.40595112e-06]\n",
      "step:  4608 loss:  1.39284e-10 Weight:  [ 1.00000119] bias:  [ -8.37126117e-06]\n",
      "step:  4609 loss:  1.36993e-10 Weight:  [ 1.00000119] bias:  [ -8.33692866e-06]\n",
      "step:  4610 loss:  1.36083e-10 Weight:  [ 1.00000119] bias:  [ -8.30450335e-06]\n",
      "step:  4611 loss:  1.35234e-10 Weight:  [ 1.00000119] bias:  [ -8.27219719e-06]\n",
      "step:  4612 loss:  1.33827e-10 Weight:  [ 1.00000119] bias:  [ -8.24013023e-06]\n",
      "step:  4613 loss:  1.32985e-10 Weight:  [ 1.00000119] bias:  [ -8.20818241e-06]\n",
      "step:  4614 loss:  1.2912e-10 Weight:  [ 1.00000119] bias:  [ -8.17718774e-06]\n",
      "step:  4615 loss:  1.28285e-10 Weight:  [ 1.00000119] bias:  [ -8.14631221e-06]\n",
      "step:  4616 loss:  1.26906e-10 Weight:  [ 1.00000119] bias:  [ -8.11567497e-06]\n",
      "step:  4617 loss:  1.26079e-10 Weight:  [ 1.00000119] bias:  [ -8.08515779e-06]\n",
      "step:  4618 loss:  1.39721e-10 Weight:  [ 1.00000107] bias:  [ -8.06036223e-06]\n",
      "step:  4619 loss:  1.36456e-10 Weight:  [ 1.00000107] bias:  [ -8.02495742e-06]\n",
      "step:  4620 loss:  1.35049e-10 Weight:  [ 1.00000107] bias:  [ -7.98979090e-06]\n",
      "step:  4621 loss:  1.34222e-10 Weight:  [ 1.00000107] bias:  [ -7.95474352e-06]\n",
      "step:  4622 loss:  1.29422e-10 Weight:  [ 1.00000107] bias:  [ -7.92076844e-06]\n",
      "step:  4623 loss:  1.28043e-10 Weight:  [ 1.00000107] bias:  [ -7.88703255e-06]\n",
      "step:  4624 loss:  1.2723e-10 Weight:  [ 1.00000107] bias:  [ -7.85341581e-06]\n",
      "step:  4625 loss:  1.25866e-10 Weight:  [ 1.00000107] bias:  [ -7.82170628e-06]\n",
      "step:  4626 loss:  1.25059e-10 Weight:  [ 1.00000107] bias:  [ -7.79011589e-06]\n",
      "step:  4627 loss:  1.23709e-10 Weight:  [ 1.00000107] bias:  [ -7.75876379e-06]\n",
      "step:  4628 loss:  1.2291e-10 Weight:  [ 1.00000107] bias:  [ -7.72753083e-06]\n",
      "step:  4629 loss:  1.19158e-10 Weight:  [ 1.00000107] bias:  [ -7.69725193e-06]\n",
      "step:  4630 loss:  1.18366e-10 Weight:  [ 1.00000107] bias:  [ -7.66709218e-06]\n",
      "step:  4631 loss:  1.17044e-10 Weight:  [ 1.00000107] bias:  [ -7.63717071e-06]\n",
      "step:  4632 loss:  1.16259e-10 Weight:  [ 1.00000107] bias:  [ -7.60736839e-06]\n",
      "step:  4633 loss:  1.16259e-10 Weight:  [ 1.00000107] bias:  [ -7.57756607e-06]\n",
      "step:  4634 loss:  1.15481e-10 Weight:  [ 1.00000107] bias:  [ -7.54788289e-06]\n",
      "step:  4635 loss:  1.14188e-10 Weight:  [ 1.00000107] bias:  [ -7.51843800e-06]\n",
      "step:  4636 loss:  1.13417e-10 Weight:  [ 1.00000107] bias:  [ -7.48911270e-06]\n",
      "step:  4637 loss:  1.09893e-10 Weight:  [ 1.00000107] bias:  [ -7.46074102e-06]\n",
      "step:  4638 loss:  1.09129e-10 Weight:  [ 1.00000107] bias:  [ -7.43248847e-06]\n",
      "step:  4639 loss:  1.07864e-10 Weight:  [ 1.00000107] bias:  [ -7.40447422e-06]\n",
      "step:  4640 loss:  1.07107e-10 Weight:  [ 1.00000107] bias:  [ -7.37657911e-06]\n",
      "step:  4641 loss:  1.06652e-10 Weight:  [ 1.00000107] bias:  [ -7.35059166e-06]\n",
      "step:  4642 loss:  1.05903e-10 Weight:  [ 1.00000107] bias:  [ -7.32472336e-06]\n",
      "step:  4643 loss:  1.04666e-10 Weight:  [ 1.00000107] bias:  [ -7.29909334e-06]\n",
      "step:  4644 loss:  1.03924e-10 Weight:  [ 1.00000107] bias:  [ -7.27358247e-06]\n",
      "step:  4645 loss:  1.03924e-10 Weight:  [ 1.00000107] bias:  [ -7.24807160e-06]\n",
      "step:  4646 loss:  1.00627e-10 Weight:  [ 1.00000107] bias:  [ -7.22351433e-06]\n",
      "step:  4647 loss:  9.98917e-11 Weight:  [ 1.00000107] bias:  [ -7.19907621e-06]\n",
      "step:  4648 loss:  9.86837e-11 Weight:  [ 1.00000107] bias:  [ -7.17487683e-06]\n",
      "step:  4649 loss:  9.79554e-11 Weight:  [ 1.00000107] bias:  [ -7.15079659e-06]\n",
      "step:  4650 loss:  1.11598e-10 Weight:  [ 1.00000095] bias:  [ -7.13243844e-06]\n",
      "step:  4651 loss:  1.10504e-10 Weight:  [ 1.00000095] bias:  [ -7.10192080e-06]\n",
      "step:  4652 loss:  1.09768e-10 Weight:  [ 1.00000095] bias:  [ -7.07152230e-06]\n",
      "step:  4653 loss:  1.08532e-10 Weight:  [ 1.00000095] bias:  [ -7.04136255e-06]\n",
      "step:  4654 loss:  1.07804e-10 Weight:  [ 1.00000095] bias:  [ -7.01132194e-06]\n",
      "step:  4655 loss:  1.04279e-10 Weight:  [ 1.00000095] bias:  [ -6.98223494e-06]\n",
      "step:  4656 loss:  1.03558e-10 Weight:  [ 1.00000095] bias:  [ -6.95326708e-06]\n",
      "step:  4657 loss:  1.0235e-10 Weight:  [ 1.00000095] bias:  [ -6.92453750e-06]\n",
      "step:  4658 loss:  1.01636e-10 Weight:  [ 1.00000095] bias:  [ -6.89592707e-06]\n",
      "step:  4659 loss:  1.00272e-10 Weight:  [ 1.00000095] bias:  [ -6.86922431e-06]\n",
      "step:  4660 loss:  9.95648e-11 Weight:  [ 1.00000095] bias:  [ -6.84264069e-06]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step:  4661 loss:  9.83853e-11 Weight:  [ 1.00000095] bias:  [ -6.81629535e-06]\n",
      "step:  4662 loss:  9.76854e-11 Weight:  [ 1.00000095] bias:  [ -6.79006916e-06]\n",
      "step:  4663 loss:  9.43885e-11 Weight:  [ 1.00000095] bias:  [ -6.76479658e-06]\n",
      "step:  4664 loss:  9.36957e-11 Weight:  [ 1.00000095] bias:  [ -6.73964360e-06]\n",
      "step:  4665 loss:  9.36957e-11 Weight:  [ 1.00000095] bias:  [ -6.71449061e-06]\n",
      "step:  4666 loss:  9.25446e-11 Weight:  [ 1.00000095] bias:  [ -6.68957591e-06]\n",
      "step:  4667 loss:  9.1859e-11 Weight:  [ 1.00000095] bias:  [ -6.66478036e-06]\n",
      "step:  4668 loss:  9.1859e-11 Weight:  [ 1.00000095] bias:  [ -6.63998480e-06]\n",
      "step:  4669 loss:  9.11804e-11 Weight:  [ 1.00000095] bias:  [ -6.61530839e-06]\n",
      "step:  4670 loss:  9.00577e-11 Weight:  [ 1.00000095] bias:  [ -6.59087027e-06]\n",
      "step:  4671 loss:  9.00577e-11 Weight:  [ 1.00000095] bias:  [ -6.56643215e-06]\n",
      "step:  4672 loss:  8.93863e-11 Weight:  [ 1.00000095] bias:  [ -6.54211362e-06]\n",
      "step:  4673 loss:  8.63167e-11 Weight:  [ 1.00000095] bias:  [ -6.51874871e-06]\n",
      "step:  4674 loss:  8.56524e-11 Weight:  [ 1.00000095] bias:  [ -6.49550293e-06]\n",
      "step:  4675 loss:  8.45581e-11 Weight:  [ 1.00000095] bias:  [ -6.47249544e-06]\n",
      "step:  4676 loss:  8.45581e-11 Weight:  [ 1.00000095] bias:  [ -6.44948796e-06]\n",
      "step:  4677 loss:  8.39009e-11 Weight:  [ 1.00000095] bias:  [ -6.42659961e-06]\n",
      "step:  4678 loss:  8.34461e-11 Weight:  [ 1.00000095] bias:  [ -6.40561893e-06]\n",
      "step:  4679 loss:  8.2796e-11 Weight:  [ 1.00000095] bias:  [ -6.38475740e-06]\n",
      "step:  4680 loss:  8.2796e-11 Weight:  [ 1.00000095] bias:  [ -6.36389586e-06]\n",
      "step:  4681 loss:  8.17302e-11 Weight:  [ 1.00000095] bias:  [ -6.34327262e-06]\n",
      "step:  4682 loss:  8.10871e-11 Weight:  [ 1.00000095] bias:  [ -6.32276851e-06]\n",
      "step:  4683 loss:  8.10871e-11 Weight:  [ 1.00000095] bias:  [ -6.30226441e-06]\n",
      "step:  4684 loss:  7.8245e-11 Weight:  [ 1.00000095] bias:  [ -6.28271391e-06]\n",
      "step:  4685 loss:  7.7609e-11 Weight:  [ 1.00000095] bias:  [ -6.26328301e-06]\n",
      "step:  4686 loss:  7.7609e-11 Weight:  [ 1.00000095] bias:  [ -6.24385211e-06]\n",
      "step:  4687 loss:  7.65716e-11 Weight:  [ 1.00000095] bias:  [ -6.22465950e-06]\n",
      "step:  4688 loss:  7.59428e-11 Weight:  [ 1.00000095] bias:  [ -6.20558603e-06]\n",
      "step:  4689 loss:  7.59428e-11 Weight:  [ 1.00000095] bias:  [ -6.18651256e-06]\n",
      "step:  4690 loss:  8.95852e-11 Weight:  [ 1.00000083] bias:  [ -6.17316118e-06]\n",
      "step:  4691 loss:  8.70415e-11 Weight:  [ 1.00000083] bias:  [ -6.14765031e-06]\n",
      "step:  4692 loss:  8.64055e-11 Weight:  [ 1.00000083] bias:  [ -6.12225858e-06]\n",
      "step:  4693 loss:  8.53397e-11 Weight:  [ 1.00000083] bias:  [ -6.09710560e-06]\n",
      "step:  4694 loss:  8.47109e-11 Weight:  [ 1.00000083] bias:  [ -6.07207176e-06]\n",
      "step:  4695 loss:  8.16414e-11 Weight:  [ 1.00000083] bias:  [ -6.04799152e-06]\n",
      "step:  4696 loss:  8.10196e-11 Weight:  [ 1.00000083] bias:  [ -6.02403043e-06]\n",
      "step:  4697 loss:  8.10196e-11 Weight:  [ 1.00000083] bias:  [ -6.00006933e-06]\n",
      "step:  4698 loss:  7.99822e-11 Weight:  [ 1.00000083] bias:  [ -5.97634653e-06]\n",
      "step:  4699 loss:  7.93676e-11 Weight:  [ 1.00000083] bias:  [ -5.95274287e-06]\n",
      "step:  4700 loss:  7.80034e-11 Weight:  [ 1.00000083] bias:  [ -5.93104687e-06]\n",
      "step:  4701 loss:  7.80034e-11 Weight:  [ 1.00000083] bias:  [ -5.90935088e-06]\n",
      "step:  4702 loss:  7.73959e-11 Weight:  [ 1.00000083] bias:  [ -5.88777402e-06]\n",
      "step:  4703 loss:  7.63869e-11 Weight:  [ 1.00000083] bias:  [ -5.86643546e-06]\n",
      "step:  4704 loss:  7.57865e-11 Weight:  [ 1.00000083] bias:  [ -5.84521604e-06]\n",
      "step:  4705 loss:  7.57865e-11 Weight:  [ 1.00000083] bias:  [ -5.82399662e-06]\n",
      "step:  4706 loss:  7.29443e-11 Weight:  [ 1.00000083] bias:  [ -5.80373126e-06]\n",
      "step:  4707 loss:  7.2351e-11 Weight:  [ 1.00000083] bias:  [ -5.78358504e-06]\n",
      "step:  4708 loss:  7.2351e-11 Weight:  [ 1.00000083] bias:  [ -5.76343882e-06]\n",
      "step:  4709 loss:  7.13705e-11 Weight:  [ 1.00000083] bias:  [ -5.74353089e-06]\n",
      "step:  4710 loss:  7.07843e-11 Weight:  [ 1.00000083] bias:  [ -5.72374211e-06]\n",
      "step:  4711 loss:  7.07843e-11 Weight:  [ 1.00000083] bias:  [ -5.70395332e-06]\n",
      "step:  4712 loss:  7.07843e-11 Weight:  [ 1.00000083] bias:  [ -5.68416453e-06]\n",
      "step:  4713 loss:  7.02052e-11 Weight:  [ 1.00000083] bias:  [ -5.66449489e-06]\n",
      "step:  4714 loss:  7.02052e-11 Weight:  [ 1.00000083] bias:  [ -5.64482525e-06]\n",
      "step:  4715 loss:  6.9253e-11 Weight:  [ 1.00000083] bias:  [ -5.62539435e-06]\n",
      "step:  4716 loss:  6.86811e-11 Weight:  [ 1.00000083] bias:  [ -5.60608260e-06]\n",
      "step:  4717 loss:  6.86811e-11 Weight:  [ 1.00000083] bias:  [ -5.58677084e-06]\n",
      "step:  4718 loss:  6.60663e-11 Weight:  [ 1.00000083] bias:  [ -5.56841269e-06]\n",
      "step:  4719 loss:  6.55014e-11 Weight:  [ 1.00000083] bias:  [ -5.55017368e-06]\n",
      "step:  4720 loss:  6.55014e-11 Weight:  [ 1.00000083] bias:  [ -5.53193468e-06]\n",
      "step:  4721 loss:  6.45777e-11 Weight:  [ 1.00000083] bias:  [ -5.51393396e-06]\n",
      "step:  4722 loss:  6.45777e-11 Weight:  [ 1.00000083] bias:  [ -5.49593324e-06]\n",
      "step:  4723 loss:  6.40199e-11 Weight:  [ 1.00000083] bias:  [ -5.47805166e-06]\n",
      "step:  4724 loss:  6.35652e-11 Weight:  [ 1.00000083] bias:  [ -5.46207775e-06]\n",
      "step:  4725 loss:  6.35652e-11 Weight:  [ 1.00000083] bias:  [ -5.44610384e-06]\n",
      "step:  4726 loss:  6.30145e-11 Weight:  [ 1.00000083] bias:  [ -5.43024908e-06]\n",
      "step:  4727 loss:  6.30145e-11 Weight:  [ 1.00000083] bias:  [ -5.41439431e-06]\n",
      "step:  4728 loss:  6.21192e-11 Weight:  [ 1.00000083] bias:  [ -5.39877783e-06]\n",
      "step:  4729 loss:  6.21192e-11 Weight:  [ 1.00000083] bias:  [ -5.38316135e-06]\n",
      "step:  4730 loss:  6.15756e-11 Weight:  [ 1.00000083] bias:  [ -5.36766402e-06]\n",
      "step:  4731 loss:  6.15756e-11 Weight:  [ 1.00000083] bias:  [ -5.35216668e-06]\n",
      "step:  4732 loss:  5.91882e-11 Weight:  [ 1.00000083] bias:  [ -5.33762295e-06]\n",
      "step:  4733 loss:  5.91882e-11 Weight:  [ 1.00000083] bias:  [ -5.32307922e-06]\n",
      "step:  4734 loss:  5.86518e-11 Weight:  [ 1.00000083] bias:  [ -5.30865509e-06]\n",
      "step:  4735 loss:  5.86518e-11 Weight:  [ 1.00000083] bias:  [ -5.29423096e-06]\n",
      "step:  4736 loss:  5.77849e-11 Weight:  [ 1.00000083] bias:  [ -5.28004512e-06]\n",
      "step:  4737 loss:  5.77849e-11 Weight:  [ 1.00000083] bias:  [ -5.26585927e-06]\n",
      "step:  4738 loss:  5.72555e-11 Weight:  [ 1.00000083] bias:  [ -5.25179257e-06]\n",
      "step:  4739 loss:  5.72555e-11 Weight:  [ 1.00000083] bias:  [ -5.23772587e-06]\n",
      "step:  4740 loss:  7.0898e-11 Weight:  [ 1.00000072] bias:  [ -5.22938126e-06]\n",
      "step:  4741 loss:  6.42331e-11 Weight:  [ 1.00000072] bias:  [ -5.21030779e-06]\n",
      "step:  4742 loss:  6.36966e-11 Weight:  [ 1.00000072] bias:  [ -5.19135347e-06]\n",
      "step:  4743 loss:  6.36966e-11 Weight:  [ 1.00000072] bias:  [ -5.17239914e-06]\n",
      "step:  4744 loss:  6.28013e-11 Weight:  [ 1.00000072] bias:  [ -5.15368311e-06]\n",
      "step:  4745 loss:  6.2272e-11 Weight:  [ 1.00000072] bias:  [ -5.13508667e-06]\n",
      "step:  4746 loss:  6.2272e-11 Weight:  [ 1.00000072] bias:  [ -5.11649023e-06]\n",
      "step:  4747 loss:  5.97709e-11 Weight:  [ 1.00000072] bias:  [ -5.09884740e-06]\n",
      "step:  4748 loss:  5.97709e-11 Weight:  [ 1.00000072] bias:  [ -5.08120456e-06]\n",
      "step:  4749 loss:  5.92486e-11 Weight:  [ 1.00000072] bias:  [ -5.06368087e-06]\n",
      "step:  4750 loss:  5.83817e-11 Weight:  [ 1.00000072] bias:  [ -5.04639547e-06]\n",
      "step:  4751 loss:  5.83817e-11 Weight:  [ 1.00000072] bias:  [ -5.02911007e-06]\n",
      "step:  4752 loss:  5.78666e-11 Weight:  [ 1.00000072] bias:  [ -5.01194381e-06]\n",
      "step:  4753 loss:  5.78666e-11 Weight:  [ 1.00000072] bias:  [ -4.99477756e-06]\n",
      "step:  4754 loss:  5.69571e-11 Weight:  [ 1.00000072] bias:  [ -4.97951896e-06]\n",
      "step:  4755 loss:  5.69571e-11 Weight:  [ 1.00000072] bias:  [ -4.96426037e-06]\n",
      "step:  4756 loss:  5.64491e-11 Weight:  [ 1.00000072] bias:  [ -4.94912092e-06]\n",
      "step:  4757 loss:  5.64491e-11 Weight:  [ 1.00000072] bias:  [ -4.93398147e-06]\n",
      "step:  4758 loss:  5.56106e-11 Weight:  [ 1.00000072] bias:  [ -4.91908031e-06]\n",
      "step:  4759 loss:  5.56106e-11 Weight:  [ 1.00000072] bias:  [ -4.90417915e-06]\n",
      "step:  4760 loss:  5.51097e-11 Weight:  [ 1.00000072] bias:  [ -4.88939713e-06]\n",
      "step:  4761 loss:  5.51097e-11 Weight:  [ 1.00000072] bias:  [ -4.87461512e-06]\n",
      "step:  4762 loss:  5.2836e-11 Weight:  [ 1.00000072] bias:  [ -4.86078670e-06]\n",
      "step:  4763 loss:  5.2836e-11 Weight:  [ 1.00000072] bias:  [ -4.84695829e-06]\n",
      "step:  4764 loss:  5.23421e-11 Weight:  [ 1.00000072] bias:  [ -4.83324902e-06]\n",
      "step:  4765 loss:  5.23421e-11 Weight:  [ 1.00000072] bias:  [ -4.81953975e-06]\n",
      "step:  4766 loss:  5.15321e-11 Weight:  [ 1.00000072] bias:  [ -4.80606923e-06]\n",
      "step:  4767 loss:  5.15321e-11 Weight:  [ 1.00000072] bias:  [ -4.79259870e-06]\n",
      "step:  4768 loss:  5.10454e-11 Weight:  [ 1.00000072] bias:  [ -4.77924732e-06]\n",
      "step:  4769 loss:  5.10454e-11 Weight:  [ 1.00000072] bias:  [ -4.76589594e-06]\n",
      "step:  4770 loss:  5.10454e-11 Weight:  [ 1.00000072] bias:  [ -4.75254456e-06]\n",
      "step:  4771 loss:  5.10454e-11 Weight:  [ 1.00000072] bias:  [ -4.73919317e-06]\n",
      "step:  4772 loss:  5.10454e-11 Weight:  [ 1.00000072] bias:  [ -4.72584179e-06]\n",
      "step:  4773 loss:  5.05658e-11 Weight:  [ 1.00000072] bias:  [ -4.71260955e-06]\n",
      "step:  4774 loss:  5.05658e-11 Weight:  [ 1.00000072] bias:  [ -4.69937731e-06]\n",
      "step:  4775 loss:  4.97842e-11 Weight:  [ 1.00000072] bias:  [ -4.68638336e-06]\n",
      "step:  4776 loss:  4.97842e-11 Weight:  [ 1.00000072] bias:  [ -4.67338941e-06]\n",
      "step:  4777 loss:  4.93117e-11 Weight:  [ 1.00000072] bias:  [ -4.66051461e-06]\n",
      "step:  4778 loss:  4.93117e-11 Weight:  [ 1.00000072] bias:  [ -4.64763980e-06]\n",
      "step:  4779 loss:  4.72653e-11 Weight:  [ 1.00000072] bias:  [ -4.63571905e-06]\n",
      "step:  4780 loss:  4.72653e-11 Weight:  [ 1.00000072] bias:  [ -4.62379830e-06]\n",
      "step:  4781 loss:  4.72653e-11 Weight:  [ 1.00000072] bias:  [ -4.61187756e-06]\n",
      "step:  4782 loss:  4.67999e-11 Weight:  [ 1.00000072] bias:  [ -4.60007595e-06]\n",
      "step:  4783 loss:  4.67999e-11 Weight:  [ 1.00000072] bias:  [ -4.58827435e-06]\n",
      "step:  4784 loss:  4.60467e-11 Weight:  [ 1.00000072] bias:  [ -4.57671104e-06]\n",
      "step:  4785 loss:  4.60467e-11 Weight:  [ 1.00000072] bias:  [ -4.56514772e-06]\n",
      "step:  4786 loss:  4.60467e-11 Weight:  [ 1.00000072] bias:  [ -4.55358440e-06]\n",
      "step:  4787 loss:  4.55884e-11 Weight:  [ 1.00000072] bias:  [ -4.54214023e-06]\n",
      "step:  4788 loss:  4.55884e-11 Weight:  [ 1.00000072] bias:  [ -4.53069606e-06]\n",
      "step:  4789 loss:  4.55884e-11 Weight:  [ 1.00000072] bias:  [ -4.51925189e-06]\n",
      "step:  4790 loss:  4.55884e-11 Weight:  [ 1.0000006] bias:  [ -4.50971493e-06]\n",
      "step:  4791 loss:  4.01457e-11 Weight:  [ 1.0000006] bias:  [ -4.48849551e-06]\n",
      "step:  4792 loss:  3.96803e-11 Weight:  [ 1.0000006] bias:  [ -4.46739568e-06]\n",
      "step:  4793 loss:  3.88987e-11 Weight:  [ 1.0000006] bias:  [ -4.44653415e-06]\n",
      "step:  4794 loss:  3.88987e-11 Weight:  [ 1.0000006] bias:  [ -4.42567261e-06]\n",
      "step:  4795 loss:  3.84404e-11 Weight:  [ 1.0000006] bias:  [ -4.40493022e-06]\n",
      "step:  4796 loss:  3.62803e-11 Weight:  [ 1.0000006] bias:  [ -4.38514144e-06]\n",
      "step:  4797 loss:  3.62803e-11 Weight:  [ 1.0000006] bias:  [ -4.36535265e-06]\n",
      "step:  4798 loss:  3.58291e-11 Weight:  [ 1.0000006] bias:  [ -4.34568301e-06]\n",
      "step:  4799 loss:  3.50759e-11 Weight:  [ 1.0000006] bias:  [ -4.32625211e-06]\n",
      "step:  4800 loss:  3.50759e-11 Weight:  [ 1.0000006] bias:  [ -4.30682121e-06]\n",
      "step:  4801 loss:  3.46319e-11 Weight:  [ 1.0000006] bias:  [ -4.28750946e-06]\n",
      "step:  4802 loss:  4.09983e-11 Weight:  [ 1.0000006] bias:  [ -4.27391979e-06]\n",
      "step:  4803 loss:  4.09983e-11 Weight:  [ 1.0000006] bias:  [ -4.26033012e-06]\n",
      "step:  4804 loss:  4.05613e-11 Weight:  [ 1.0000006] bias:  [ -4.24685959e-06]\n",
      "step:  4805 loss:  4.05613e-11 Weight:  [ 1.0000006] bias:  [ -4.23338906e-06]\n",
      "step:  4806 loss:  4.05613e-11 Weight:  [ 1.0000006] bias:  [ -4.21991854e-06]\n",
      "step:  4807 loss:  3.98366e-11 Weight:  [ 1.0000006] bias:  [ -4.20668630e-06]\n",
      "step:  4808 loss:  3.98366e-11 Weight:  [ 1.0000006] bias:  [ -4.19345406e-06]\n",
      "step:  4809 loss:  3.94067e-11 Weight:  [ 1.0000006] bias:  [ -4.18034097e-06]\n",
      "step:  4810 loss:  3.94067e-11 Weight:  [ 1.0000006] bias:  [ -4.16722787e-06]\n",
      "step:  4811 loss:  3.7474e-11 Weight:  [ 1.0000006] bias:  [ -4.15506838e-06]\n",
      "step:  4812 loss:  3.7474e-11 Weight:  [ 1.0000006] bias:  [ -4.14290889e-06]\n",
      "step:  4813 loss:  3.7474e-11 Weight:  [ 1.0000006] bias:  [ -4.13074940e-06]\n",
      "step:  4814 loss:  3.70513e-11 Weight:  [ 1.0000006] bias:  [ -4.11870906e-06]\n",
      "step:  4815 loss:  3.70513e-11 Weight:  [ 1.0000006] bias:  [ -4.10666871e-06]\n",
      "step:  4816 loss:  3.63549e-11 Weight:  [ 1.0000006] bias:  [ -4.09486711e-06]\n",
      "step:  4817 loss:  3.63549e-11 Weight:  [ 1.0000006] bias:  [ -4.08306551e-06]\n",
      "step:  4818 loss:  3.63549e-11 Weight:  [ 1.0000006] bias:  [ -4.07126390e-06]\n",
      "step:  4819 loss:  3.59393e-11 Weight:  [ 1.0000006] bias:  [ -4.05958144e-06]\n",
      "step:  4820 loss:  3.59393e-11 Weight:  [ 1.0000006] bias:  [ -4.04789898e-06]\n",
      "step:  4821 loss:  3.59393e-11 Weight:  [ 1.0000006] bias:  [ -4.03812373e-06]\n",
      "step:  4822 loss:  3.59393e-11 Weight:  [ 1.0000006] bias:  [ -4.02834849e-06]\n",
      "step:  4823 loss:  3.59393e-11 Weight:  [ 1.0000006] bias:  [ -4.01857324e-06]\n",
      "step:  4824 loss:  3.55307e-11 Weight:  [ 1.0000006] bias:  [ -4.00891713e-06]\n",
      "step:  4825 loss:  3.55307e-11 Weight:  [ 1.0000006] bias:  [ -3.99926103e-06]\n",
      "step:  4826 loss:  3.55307e-11 Weight:  [ 1.0000006] bias:  [ -3.98960492e-06]\n",
      "step:  4827 loss:  3.48628e-11 Weight:  [ 1.0000006] bias:  [ -3.98018756e-06]\n",
      "step:  4828 loss:  3.48628e-11 Weight:  [ 1.0000006] bias:  [ -3.97077019e-06]\n",
      "step:  4829 loss:  3.48628e-11 Weight:  [ 1.0000006] bias:  [ -3.96135283e-06]\n",
      "step:  4830 loss:  3.44613e-11 Weight:  [ 1.0000006] bias:  [ -3.95205461e-06]\n",
      "step:  4831 loss:  3.44613e-11 Weight:  [ 1.0000006] bias:  [ -3.94275639e-06]\n",
      "step:  4832 loss:  3.44613e-11 Weight:  [ 1.0000006] bias:  [ -3.93345817e-06]\n",
      "step:  4833 loss:  3.2756e-11 Weight:  [ 1.0000006] bias:  [ -3.92511356e-06]\n",
      "step:  4834 loss:  3.2756e-11 Weight:  [ 1.0000006] bias:  [ -3.91676895e-06]\n",
      "step:  4835 loss:  3.2756e-11 Weight:  [ 1.0000006] bias:  [ -3.90842433e-06]\n",
      "step:  4836 loss:  3.2756e-11 Weight:  [ 1.0000006] bias:  [ -3.90007972e-06]\n",
      "step:  4837 loss:  3.23617e-11 Weight:  [ 1.0000006] bias:  [ -3.89185425e-06]\n",
      "step:  4838 loss:  3.23617e-11 Weight:  [ 1.0000006] bias:  [ -3.88362878e-06]\n",
      "step:  4839 loss:  3.23617e-11 Weight:  [ 1.0000006] bias:  [ -3.87540331e-06]\n",
      "step:  4840 loss:  3.23617e-11 Weight:  [ 1.0000006] bias:  [ -3.86717784e-06]\n",
      "step:  4841 loss:  3.17222e-11 Weight:  [ 1.0000006] bias:  [ -3.85919066e-06]\n",
      "step:  4842 loss:  3.17222e-11 Weight:  [ 1.0000006] bias:  [ -3.85120347e-06]\n",
      "step:  4843 loss:  3.17222e-11 Weight:  [ 1.0000006] bias:  [ -3.84321629e-06]\n",
      "step:  4844 loss:  3.13349e-11 Weight:  [ 1.0000006] bias:  [ -3.83534825e-06]\n",
      "step:  4845 loss:  3.13349e-11 Weight:  [ 1.0000006] bias:  [ -3.82748021e-06]\n",
      "step:  4846 loss:  3.13349e-11 Weight:  [ 1.0000006] bias:  [ -3.81961217e-06]\n",
      "step:  4847 loss:  3.13349e-11 Weight:  [ 1.0000006] bias:  [ -3.81174436e-06]\n",
      "step:  4848 loss:  3.13349e-11 Weight:  [ 1.0000006] bias:  [ -3.80387655e-06]\n",
      "step:  4849 loss:  3.13349e-11 Weight:  [ 1.0000006] bias:  [ -3.79600874e-06]\n",
      "step:  4850 loss:  3.13349e-11 Weight:  [ 1.0000006] bias:  [ -3.78814093e-06]\n",
      "step:  4851 loss:  3.13349e-11 Weight:  [ 1.0000006] bias:  [ -3.78027312e-06]\n",
      "step:  4852 loss:  3.09548e-11 Weight:  [ 1.0000006] bias:  [ -3.77252445e-06]\n",
      "step:  4853 loss:  3.09548e-11 Weight:  [ 1.0000006] bias:  [ -3.76477578e-06]\n",
      "step:  4854 loss:  3.09548e-11 Weight:  [ 1.0000006] bias:  [ -3.75702712e-06]\n",
      "step:  4855 loss:  3.09548e-11 Weight:  [ 1.0000006] bias:  [ -3.74927845e-06]\n",
      "step:  4856 loss:  3.03437e-11 Weight:  [ 1.0000006] bias:  [ -3.74176830e-06]\n",
      "step:  4857 loss:  3.03437e-11 Weight:  [ 1.0000006] bias:  [ -3.73425814e-06]\n",
      "step:  4858 loss:  3.03437e-11 Weight:  [ 1.0000006] bias:  [ -3.72674799e-06]\n",
      "step:  4859 loss:  3.03437e-11 Weight:  [ 1.0000006] bias:  [ -3.71923784e-06]\n",
      "step:  4860 loss:  2.99707e-11 Weight:  [ 1.0000006] bias:  [ -3.71184683e-06]\n",
      "step:  4861 loss:  2.99707e-11 Weight:  [ 1.0000006] bias:  [ -3.70445582e-06]\n",
      "step:  4862 loss:  2.99707e-11 Weight:  [ 1.0000006] bias:  [ -3.69706481e-06]\n",
      "step:  4863 loss:  2.99707e-11 Weight:  [ 1.0000006] bias:  [ -3.68967380e-06]\n",
      "step:  4864 loss:  2.84928e-11 Weight:  [ 1.0000006] bias:  [ -3.68323640e-06]\n",
      "step:  4865 loss:  2.84928e-11 Weight:  [ 1.0000006] bias:  [ -3.67679900e-06]\n",
      "step:  4866 loss:  2.84928e-11 Weight:  [ 1.0000006] bias:  [ -3.67036159e-06]\n",
      "step:  4867 loss:  2.84928e-11 Weight:  [ 1.0000006] bias:  [ -3.66392419e-06]\n",
      "step:  4868 loss:  2.81268e-11 Weight:  [ 1.0000006] bias:  [ -3.65760616e-06]\n",
      "step:  4869 loss:  2.81268e-11 Weight:  [ 1.0000006] bias:  [ -3.65128813e-06]\n",
      "step:  4870 loss:  2.81268e-11 Weight:  [ 1.0000006] bias:  [ -3.64497009e-06]\n",
      "step:  4871 loss:  2.81268e-11 Weight:  [ 1.0000006] bias:  [ -3.63865206e-06]\n",
      "step:  4872 loss:  2.81268e-11 Weight:  [ 1.0000006] bias:  [ -3.63233403e-06]\n",
      "step:  4873 loss:  2.75442e-11 Weight:  [ 1.0000006] bias:  [ -3.62625428e-06]\n",
      "step:  4874 loss:  2.75442e-11 Weight:  [ 1.0000006] bias:  [ -3.62017454e-06]\n",
      "step:  4875 loss:  2.75442e-11 Weight:  [ 1.0000006] bias:  [ -3.61409479e-06]\n",
      "step:  4876 loss:  2.75442e-11 Weight:  [ 1.0000006] bias:  [ -3.60801505e-06]\n",
      "step:  4877 loss:  2.75442e-11 Weight:  [ 1.0000006] bias:  [ -3.60193530e-06]\n",
      "step:  4878 loss:  2.71854e-11 Weight:  [ 1.0000006] bias:  [ -3.59597493e-06]\n",
      "step:  4879 loss:  2.71854e-11 Weight:  [ 1.0000006] bias:  [ -3.59001456e-06]\n",
      "step:  4880 loss:  2.71854e-11 Weight:  [ 1.0000006] bias:  [ -3.58405418e-06]\n",
      "step:  4881 loss:  2.71854e-11 Weight:  [ 1.0000006] bias:  [ -3.57809381e-06]\n",
      "step:  4882 loss:  2.71854e-11 Weight:  [ 1.0000006] bias:  [ -3.57213344e-06]\n",
      "step:  4883 loss:  2.80949e-11 Weight:  [ 1.00000048] bias:  [ -3.56808027e-06]\n",
      "step:  4884 loss:  2.50111e-11 Weight:  [ 1.00000048] bias:  [ -3.54996041e-06]\n",
      "step:  4885 loss:  2.50111e-11 Weight:  [ 1.00000048] bias:  [ -3.53184055e-06]\n",
      "step:  4886 loss:  2.46452e-11 Weight:  [ 1.00000048] bias:  [ -3.51384006e-06]\n",
      "step:  4887 loss:  2.40341e-11 Weight:  [ 1.00000048] bias:  [ -3.49607785e-06]\n",
      "step:  4888 loss:  2.40341e-11 Weight:  [ 1.00000048] bias:  [ -3.47831565e-06]\n",
      "step:  4889 loss:  2.36753e-11 Weight:  [ 1.00000048] bias:  [ -3.46067259e-06]\n",
      "step:  4890 loss:  2.36753e-11 Weight:  [ 1.00000048] bias:  [ -3.44302953e-06]\n",
      "step:  4891 loss:  2.197e-11 Weight:  [ 1.00000048] bias:  [ -3.42634030e-06]\n",
      "step:  4892 loss:  2.16183e-11 Weight:  [ 1.00000048] bias:  [ -3.40977022e-06]\n",
      "step:  4893 loss:  2.16183e-11 Weight:  [ 1.00000048] bias:  [ -3.39320013e-06]\n",
      "step:  4894 loss:  2.10356e-11 Weight:  [ 1.00000048] bias:  [ -3.37686856e-06]\n",
      "step:  4895 loss:  2.10356e-11 Weight:  [ 1.00000048] bias:  [ -3.36053699e-06]\n",
      "step:  4896 loss:  2.0691e-11 Weight:  [ 1.00000048] bias:  [ -3.34432457e-06]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step:  4897 loss:  2.0691e-11 Weight:  [ 1.00000048] bias:  [ -3.32811214e-06]\n",
      "step:  4898 loss:  2.52385e-11 Weight:  [ 1.00000048] bias:  [ -3.31762180e-06]\n",
      "step:  4899 loss:  2.52385e-11 Weight:  [ 1.00000048] bias:  [ -3.30713146e-06]\n",
      "step:  4900 loss:  2.4901e-11 Weight:  [ 1.00000048] bias:  [ -3.29676027e-06]\n",
      "step:  4901 loss:  2.4901e-11 Weight:  [ 1.00000048] bias:  [ -3.28638907e-06]\n",
      "step:  4902 loss:  2.4901e-11 Weight:  [ 1.00000048] bias:  [ -3.27601788e-06]\n",
      "step:  4903 loss:  2.43467e-11 Weight:  [ 1.00000048] bias:  [ -3.26588520e-06]\n",
      "step:  4904 loss:  2.43467e-11 Weight:  [ 1.00000048] bias:  [ -3.25575252e-06]\n",
      "step:  4905 loss:  2.43467e-11 Weight:  [ 1.00000048] bias:  [ -3.24561984e-06]\n",
      "step:  4906 loss:  2.40163e-11 Weight:  [ 1.00000048] bias:  [ -3.23560630e-06]\n",
      "step:  4907 loss:  2.40163e-11 Weight:  [ 1.00000048] bias:  [ -3.22559276e-06]\n",
      "step:  4908 loss:  2.40163e-11 Weight:  [ 1.00000048] bias:  [ -3.21557923e-06]\n",
      "step:  4909 loss:  2.25384e-11 Weight:  [ 1.00000048] bias:  [ -3.20651930e-06]\n",
      "step:  4910 loss:  2.25384e-11 Weight:  [ 1.00000048] bias:  [ -3.19745936e-06]\n",
      "step:  4911 loss:  2.25384e-11 Weight:  [ 1.00000048] bias:  [ -3.18839943e-06]\n",
      "step:  4912 loss:  2.22151e-11 Weight:  [ 1.00000048] bias:  [ -3.17945864e-06]\n",
      "step:  4913 loss:  2.22151e-11 Weight:  [ 1.00000048] bias:  [ -3.17051786e-06]\n",
      "step:  4914 loss:  2.22151e-11 Weight:  [ 1.00000048] bias:  [ -3.16157707e-06]\n",
      "step:  4915 loss:  2.22151e-11 Weight:  [ 1.00000048] bias:  [ -3.15263628e-06]\n",
      "step:  4916 loss:  2.16893e-11 Weight:  [ 1.00000048] bias:  [ -3.14393401e-06]\n",
      "step:  4917 loss:  2.16893e-11 Weight:  [ 1.00000048] bias:  [ -3.13523174e-06]\n",
      "step:  4918 loss:  2.16893e-11 Weight:  [ 1.00000048] bias:  [ -3.12652946e-06]\n",
      "step:  4919 loss:  2.13731e-11 Weight:  [ 1.00000048] bias:  [ -3.11794633e-06]\n",
      "step:  4920 loss:  2.13731e-11 Weight:  [ 1.00000048] bias:  [ -3.10936321e-06]\n",
      "step:  4921 loss:  2.13731e-11 Weight:  [ 1.00000048] bias:  [ -3.10078008e-06]\n",
      "step:  4922 loss:  2.13731e-11 Weight:  [ 1.00000048] bias:  [ -3.09219695e-06]\n",
      "step:  4923 loss:  2.13731e-11 Weight:  [ 1.00000048] bias:  [ -3.08552126e-06]\n",
      "step:  4924 loss:  2.13731e-11 Weight:  [ 1.00000048] bias:  [ -3.07884557e-06]\n",
      "step:  4925 loss:  2.13731e-11 Weight:  [ 1.00000048] bias:  [ -3.07216987e-06]\n",
      "step:  4926 loss:  2.13731e-11 Weight:  [ 1.00000048] bias:  [ -3.06549418e-06]\n",
      "step:  4927 loss:  2.1064e-11 Weight:  [ 1.00000048] bias:  [ -3.05893764e-06]\n",
      "step:  4928 loss:  2.1064e-11 Weight:  [ 1.00000048] bias:  [ -3.05238109e-06]\n",
      "step:  4929 loss:  2.1064e-11 Weight:  [ 1.00000048] bias:  [ -3.04582454e-06]\n",
      "step:  4930 loss:  2.1064e-11 Weight:  [ 1.00000048] bias:  [ -3.03926799e-06]\n",
      "step:  4931 loss:  2.05667e-11 Weight:  [ 1.00000048] bias:  [ -3.03294996e-06]\n",
      "step:  4932 loss:  2.05667e-11 Weight:  [ 1.00000048] bias:  [ -3.02663193e-06]\n",
      "step:  4933 loss:  2.05667e-11 Weight:  [ 1.00000048] bias:  [ -3.02031390e-06]\n",
      "step:  4934 loss:  2.05667e-11 Weight:  [ 1.00000048] bias:  [ -3.01399587e-06]\n",
      "step:  4935 loss:  2.05667e-11 Weight:  [ 1.00000048] bias:  [ -3.00767783e-06]\n",
      "step:  4936 loss:  2.02647e-11 Weight:  [ 1.00000048] bias:  [ -3.00147894e-06]\n",
      "step:  4937 loss:  2.02647e-11 Weight:  [ 1.00000048] bias:  [ -2.99528006e-06]\n",
      "step:  4938 loss:  2.02647e-11 Weight:  [ 1.00000048] bias:  [ -2.98908117e-06]\n",
      "step:  4939 loss:  2.02647e-11 Weight:  [ 1.00000048] bias:  [ -2.98288228e-06]\n",
      "step:  4940 loss:  2.02647e-11 Weight:  [ 1.00000048] bias:  [ -2.97668339e-06]\n",
      "step:  4941 loss:  1.90141e-11 Weight:  [ 1.00000048] bias:  [ -2.97143811e-06]\n",
      "step:  4942 loss:  1.90141e-11 Weight:  [ 1.00000048] bias:  [ -2.96619282e-06]\n",
      "step:  4943 loss:  1.90141e-11 Weight:  [ 1.00000048] bias:  [ -2.96094754e-06]\n",
      "step:  4944 loss:  1.90141e-11 Weight:  [ 1.00000048] bias:  [ -2.95570226e-06]\n",
      "step:  4945 loss:  1.90141e-11 Weight:  [ 1.00000048] bias:  [ -2.95045697e-06]\n",
      "step:  4946 loss:  1.90141e-11 Weight:  [ 1.00000048] bias:  [ -2.94521169e-06]\n",
      "step:  4947 loss:  1.87192e-11 Weight:  [ 1.00000048] bias:  [ -2.94008578e-06]\n",
      "step:  4948 loss:  1.87192e-11 Weight:  [ 1.00000048] bias:  [ -2.93495987e-06]\n",
      "step:  4949 loss:  1.87192e-11 Weight:  [ 1.00000048] bias:  [ -2.92983395e-06]\n",
      "step:  4950 loss:  1.87192e-11 Weight:  [ 1.00000048] bias:  [ -2.92470804e-06]\n",
      "step:  4951 loss:  1.87192e-11 Weight:  [ 1.00000048] bias:  [ -2.91958213e-06]\n",
      "step:  4952 loss:  1.82503e-11 Weight:  [ 1.00000048] bias:  [ -2.91469451e-06]\n",
      "step:  4953 loss:  1.82503e-11 Weight:  [ 1.00000048] bias:  [ -2.90980688e-06]\n",
      "step:  4954 loss:  1.82503e-11 Weight:  [ 1.00000048] bias:  [ -2.90491926e-06]\n",
      "step:  4955 loss:  1.82503e-11 Weight:  [ 1.00000048] bias:  [ -2.90003163e-06]\n",
      "step:  4956 loss:  1.82503e-11 Weight:  [ 1.00000048] bias:  [ -2.89514401e-06]\n",
      "step:  4957 loss:  1.82503e-11 Weight:  [ 1.00000048] bias:  [ -2.89025638e-06]\n",
      "step:  4958 loss:  1.79625e-11 Weight:  [ 1.00000048] bias:  [ -2.88548790e-06]\n",
      "step:  4959 loss:  1.79625e-11 Weight:  [ 1.00000048] bias:  [ -2.88071942e-06]\n",
      "step:  4960 loss:  1.79625e-11 Weight:  [ 1.00000048] bias:  [ -2.87595094e-06]\n",
      "step:  4961 loss:  1.79625e-11 Weight:  [ 1.00000048] bias:  [ -2.87118246e-06]\n",
      "step:  4962 loss:  1.79625e-11 Weight:  [ 1.00000048] bias:  [ -2.86641398e-06]\n",
      "step:  4963 loss:  1.79625e-11 Weight:  [ 1.00000048] bias:  [ -2.86164550e-06]\n",
      "step:  4964 loss:  1.79625e-11 Weight:  [ 1.00000048] bias:  [ -2.85687702e-06]\n",
      "step:  4965 loss:  1.79625e-11 Weight:  [ 1.00000048] bias:  [ -2.85210854e-06]\n",
      "step:  4966 loss:  1.79625e-11 Weight:  [ 1.00000048] bias:  [ -2.84734006e-06]\n",
      "step:  4967 loss:  1.79625e-11 Weight:  [ 1.00000048] bias:  [ -2.84257158e-06]\n",
      "step:  4968 loss:  1.79625e-11 Weight:  [ 1.00000048] bias:  [ -2.83780309e-06]\n",
      "step:  4969 loss:  1.79625e-11 Weight:  [ 1.00000048] bias:  [ -2.83303461e-06]\n",
      "step:  4970 loss:  1.79625e-11 Weight:  [ 1.00000048] bias:  [ -2.82826613e-06]\n",
      "step:  4971 loss:  1.76819e-11 Weight:  [ 1.00000048] bias:  [ -2.82361702e-06]\n",
      "step:  4972 loss:  1.76819e-11 Weight:  [ 1.00000048] bias:  [ -2.81896791e-06]\n",
      "step:  4973 loss:  1.76819e-11 Weight:  [ 1.00000048] bias:  [ -2.81431880e-06]\n",
      "step:  4974 loss:  1.76819e-11 Weight:  [ 1.00000048] bias:  [ -2.80966970e-06]\n",
      "step:  4975 loss:  1.76819e-11 Weight:  [ 1.00000048] bias:  [ -2.80502059e-06]\n",
      "step:  4976 loss:  1.76819e-11 Weight:  [ 1.00000048] bias:  [ -2.80037148e-06]\n",
      "step:  4977 loss:  1.72413e-11 Weight:  [ 1.00000048] bias:  [ -2.79596065e-06]\n",
      "step:  4978 loss:  1.72413e-11 Weight:  [ 1.00000048] bias:  [ -2.79154983e-06]\n",
      "step:  4979 loss:  1.72413e-11 Weight:  [ 1.00000048] bias:  [ -2.78713901e-06]\n",
      "step:  4980 loss:  1.72413e-11 Weight:  [ 1.00000048] bias:  [ -2.78272819e-06]\n",
      "step:  4981 loss:  1.72413e-11 Weight:  [ 1.00000048] bias:  [ -2.77831737e-06]\n",
      "step:  4982 loss:  1.72413e-11 Weight:  [ 1.00000048] bias:  [ -2.77390654e-06]\n",
      "step:  4983 loss:  1.72413e-11 Weight:  [ 1.00000048] bias:  [ -2.76949572e-06]\n",
      "step:  4984 loss:  1.69678e-11 Weight:  [ 1.00000048] bias:  [ -2.76520427e-06]\n",
      "step:  4985 loss:  1.69678e-11 Weight:  [ 1.00000048] bias:  [ -2.76091282e-06]\n",
      "step:  4986 loss:  1.69678e-11 Weight:  [ 1.00000048] bias:  [ -2.75662137e-06]\n",
      "step:  4987 loss:  1.69678e-11 Weight:  [ 1.00000048] bias:  [ -2.75232992e-06]\n",
      "step:  4988 loss:  1.69678e-11 Weight:  [ 1.00000048] bias:  [ -2.74803847e-06]\n",
      "step:  4989 loss:  1.69678e-11 Weight:  [ 1.00000048] bias:  [ -2.74374702e-06]\n",
      "step:  4990 loss:  1.69678e-11 Weight:  [ 1.00000048] bias:  [ -2.73945557e-06]\n",
      "step:  4991 loss:  1.59446e-11 Weight:  [ 1.00000048] bias:  [ -2.73611772e-06]\n",
      "step:  4992 loss:  1.59446e-11 Weight:  [ 1.00000048] bias:  [ -2.73277988e-06]\n",
      "step:  4993 loss:  1.59446e-11 Weight:  [ 1.00000048] bias:  [ -2.72944203e-06]\n",
      "step:  4994 loss:  1.59446e-11 Weight:  [ 1.00000048] bias:  [ -2.72610419e-06]\n",
      "step:  4995 loss:  1.59446e-11 Weight:  [ 1.00000048] bias:  [ -2.72276634e-06]\n",
      "step:  4996 loss:  1.59446e-11 Weight:  [ 1.00000048] bias:  [ -2.71942849e-06]\n",
      "step:  4997 loss:  1.59446e-11 Weight:  [ 1.00000048] bias:  [ -2.71609065e-06]\n",
      "step:  4998 loss:  1.59446e-11 Weight:  [ 1.00000048] bias:  [ -2.71275280e-06]\n",
      "step:  4999 loss:  1.59446e-11 Weight:  [ 1.00000048] bias:  [ -2.70941496e-06]\n",
      "W: [ 1.00000048] b: [ -2.70941496e-06] loss : 1.56781e-11\n"
     ]
    }
   ],
   "source": [
    "for step in range(5000):\n",
    "    _, W_value, b_value, loss_value = sess.run([train, W, b, loss], {x: x_train, y: y_groundtruth})\n",
    "\n",
    "    if step % 1 == 0:\n",
    "        print(\"step: \", step, \"loss: \", loss_value, \"Weight: \", W_value, \"bias: \", b_value)\n",
    "\n",
    "# evaluate training accuracy\n",
    "curr_W, curr_b, curr_loss = sess.run([W, b, loss], {x: x_train, y: y_groundtruth})\n",
    "print(\"W: %s b: %s loss : %s\"%(curr_W, curr_b, curr_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: 5.0, y: [ 4.99999952]\n"
     ]
    }
   ],
   "source": [
    "x_test = 5.\n",
    "y_predict = sess.run(y_hat, {x: x_test})\n",
    "print(\"x: %s, y: %s\"%(x_test, y_predict)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
